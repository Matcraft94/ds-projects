{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from common.pinn_base import BasePINN, PINNConfig\n",
    "from common.trainer import PINNTrainer, TrainerConfig\n",
    "from common.visualizer import PINNVisualizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionPINN(BasePINN):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(input_dim=2, output_dim=1, config=config)\n",
    "        \n",
    "    def compute_pde_residual(self, points):\n",
    "        \"\"\"Compute PDE residual for the heat equation\"\"\"\n",
    "        # Asegurarnos de que points requiere gradiente\n",
    "        if not points.requires_grad:\n",
    "            points.requires_grad_(True)\n",
    "            \n",
    "        # Forward pass\n",
    "        u = self.forward(points)\n",
    "        \n",
    "        # Primera derivada respecto al tiempo (componente 1)\n",
    "        du_dt = torch.autograd.grad(\n",
    "            u.sum(), points, \n",
    "            create_graph=True, \n",
    "            retain_graph=True\n",
    "        )[0][:, 1:2]\n",
    "        \n",
    "        # Primera derivada respecto al espacio (componente 0)\n",
    "        du_dx = torch.autograd.grad(\n",
    "            u.sum(), points, \n",
    "            create_graph=True, \n",
    "            retain_graph=True\n",
    "        )[0][:, 0:1]\n",
    "        \n",
    "        # Segunda derivada respecto al espacio\n",
    "        d2u_dx2 = torch.autograd.grad(\n",
    "            du_dx.sum(), points, \n",
    "            create_graph=True, \n",
    "            retain_graph=True\n",
    "        )[0][:, 0:1]\n",
    "        \n",
    "        return du_dt - d2u_dx2\n",
    "\n",
    "    def pde_residual_wrapper(self, points):\n",
    "        return self.compute_pde_residual(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(nx: int = 100, nt: int = 100) -> dict:\n",
    "    \"\"\"Generate training data for the diffusion equation\"\"\"\n",
    "    # Domain points\n",
    "    x = torch.linspace(0, 1, nx)\n",
    "    t = torch.linspace(0, 1, nt)\n",
    "    \n",
    "    # Initial condition points\n",
    "    x_init = torch.linspace(0, 1, nx).reshape(-1, 1)\n",
    "    t_init = torch.zeros_like(x_init)\n",
    "    u_init = torch.sin(np.pi * x_init)\n",
    "    \n",
    "    # Boundary condition points\n",
    "    t_bound = torch.linspace(0, 1, nt).reshape(-1, 1)\n",
    "    x_bound_left = torch.zeros_like(t_bound)\n",
    "    x_bound_right = torch.ones_like(t_bound)\n",
    "    u_bound = torch.zeros_like(t_bound)\n",
    "    \n",
    "    # Collocation points for residual\n",
    "    x_resid = torch.rand(1000, 1)\n",
    "    t_resid = torch.rand(1000, 1)\n",
    "    \n",
    "    # Combine boundary points\n",
    "    x_bound = torch.cat([x_bound_left, x_bound_right])\n",
    "    t_bound = torch.cat([t_bound, t_bound])\n",
    "    u_bound = torch.cat([u_bound, u_bound])\n",
    "    \n",
    "    return {\n",
    "        'x': x,\n",
    "        't': t,\n",
    "        'x_init': x_init,\n",
    "        't_init': t_init,\n",
    "        'u_init': u_init,\n",
    "        'x_bound': x_bound,\n",
    "        't_bound': t_bound,\n",
    "        'u_bound': u_bound,\n",
    "        'x_resid': x_resid,\n",
    "        't_resid': t_resid\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_solution(x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute exact solution for comparison\"\"\"\n",
    "    k = 0.1  # Diffusion coefficient\n",
    "    return torch.sin(np.pi * x) * torch.exp(-k * (np.pi**2) * t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "pinn_config = PINNConfig(\n",
    "    hidden_layers=4,\n",
    "    neurons_per_layer=50,\n",
    "    activation=torch.nn.Tanh(),\n",
    "    w_initial=1.0,\n",
    "    w_boundary=1.0,\n",
    "    w_residual=1.0\n",
    ")\n",
    "\n",
    "trainer_config = TrainerConfig(\n",
    "    adam_iterations=5000,\n",
    "    initial_lr=1e-3,\n",
    "    resampling_enabled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "data = generate_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and trainer\n",
    "model = DiffusionPINN(pinn_config)\n",
    "trainer = PINNTrainer(model, trainer_config)\n",
    "visualizer = PINNVisualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training points\n",
    "x_init_t = torch.cat([data['x_init'], data['t_init']], dim=1)\n",
    "x_bound_t = torch.cat([data['x_bound'], data['t_bound']], dim=1)\n",
    "x_resid_t = torch.cat([data['x_resid'], data['t_resid']], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain bounds for resampling\n",
    "domain_bounds = torch.tensor([[0., 1.], [0., 1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mated\\anaconda3\\envs\\torch_win39\\lib\\site-packages\\torch\\functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3638.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# Generate test points for full solution visualization\n",
    "x_test = data['x'].reshape(-1, 1)\n",
    "t_test = data['t'].reshape(-1, 1)\n",
    "X, T = torch.meshgrid(x_test.squeeze(), t_test.squeeze())\n",
    "test_points = torch.stack([X.flatten(), T.flatten()], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute exact solution\n",
    "u_exact = exact_solution(X, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 0.539633\n",
      "Step 100: Loss = 0.124239\n",
      "Step 200: Loss = 0.043412\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_init_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mu_init\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_bound_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mu_bound\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_resid_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdomain_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpde_residual_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mu_exact\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mj:\\ds-projects\\neural-pdes-solver\\pt-pinn\\common\\trainer.py:216\u001b[0m, in \u001b[0;36mPINNTrainer.train\u001b[1;34m(self, x_initial, initial_condition, x_boundary, boundary_condition, x_residual, domain_bounds, pde_operator, x_test, y_test, x_supervised, y_supervised)\u001b[0m\n\u001b[0;32m    213\u001b[0m x_residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresample_residual_points(x_residual, domain_bounds, step)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# Training step\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m loss_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_initial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_condition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_boundary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboundary_condition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_residual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpde_operator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_adam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_supervised\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_supervised\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# Update history\u001b[39;00m\n",
      "File \u001b[1;32mj:\\ds-projects\\neural-pdes-solver\\pt-pinn\\common\\trainer.py:174\u001b[0m, in \u001b[0;36mPINNTrainer.train_step\u001b[1;34m(self, x_initial, initial_condition, x_boundary, boundary_condition, x_residual, pde_operator, optimizer, x_supervised, y_supervised)\u001b[0m\n\u001b[0;32m    171\u001b[0m     loss_components[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupervised\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m supervised_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_components\n",
      "File \u001b[1;32mc:\\Users\\mated\\anaconda3\\envs\\torch_win39\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mated\\anaconda3\\envs\\torch_win39\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mated\\anaconda3\\envs\\torch_win39\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print(\"Starting training...\")\n",
    "history = trainer.train(\n",
    "    x_init_t, data['u_init'],\n",
    "    x_bound_t, data['u_bound'],\n",
    "    x_resid_t, domain_bounds,\n",
    "    model.pde_residual_wrapper,\n",
    "    test_points, \n",
    "    u_exact.flatten().reshape(-1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    u_pred = model(test_points).reshape(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute error\n",
    "error = torch.abs(u_pred - u_exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "print(\"\\nGenerating visualizations...\")\n",
    "\n",
    "# Plot training history\n",
    "visualizer.plot_training_history(history, \"training_history.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot solution comparison\n",
    "visualizer.plot_1d_solution(\n",
    "    data['t'], data['x'],\n",
    "    u_pred, u_exact,\n",
    "    \"Diffusion Equation Solution\",\n",
    "    \"solution_comparison.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error heatmap\n",
    "visualizer.plot_error_heatmap(\n",
    "    error, data['x'], data['t'],\n",
    "    \"Absolute Error\",\n",
    "    \"error_heatmap.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot solutions at specific times\n",
    "times = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "visualizer.plot_multiple_times(\n",
    "    data['x'], data['t'], u_pred,\n",
    "    times,\n",
    "    \"Solution at Different Times\",\n",
    "    \"solution_times.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExperiment completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_win39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
