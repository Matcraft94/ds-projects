{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict, List\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from common.pinn_base import BasePINN, PINNConfig\n",
    "from common.trainer import PINNTrainer, TrainerConfig\n",
    "from common.visualizer import PINNVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Heat3DPDE:\n",
    "    \"\"\"3D nonlinear heat equation from section 4.1\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def exact_solution(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"u(x,y,z,t) = 2 + sin(5πt + πxyz)\"\"\"\n",
    "        return 2 + torch.sin(5*np.pi*x[..., 3:4] + np.pi*x[..., 0:1]*x[..., 1:2]*x[..., 2:3])\n",
    "        \n",
    "    def source_term(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute Q(x,y,z,t) source term\"\"\"\n",
    "        xyz = x[..., 0:1]*x[..., 1:2]*x[..., 2:3]\n",
    "        t = x[..., 3:4]\n",
    "        \n",
    "        term1 = 5*np.pi*torch.cos(5*np.pi*t + np.pi*xyz)\n",
    "        \n",
    "        term2 = np.pi**2 * (x[..., 0:1]**2 * x[..., 1:2]**2 + \n",
    "                           x[..., 0:1]**2 * x[..., 2:3]**2 + \n",
    "                           x[..., 1:2]**2 * x[..., 2:3]**2)\n",
    "        \n",
    "        term3 = torch.cos(10*np.pi*t + 2*np.pi*xyz) - 2*torch.sin(5*np.pi*t + np.pi*xyz)\n",
    "        \n",
    "        return term1 - term2*term3\n",
    "        \n",
    "    def pde_operator(self, x: torch.Tensor, model: BasePINN) -> torch.Tensor:\n",
    "        \"\"\"Implements the PDE: u_t - ∇·(u∇u) = Q\"\"\"\n",
    "        # Enable gradient computation\n",
    "        x.requires_grad_(True)\n",
    "        \n",
    "        # Get model prediction\n",
    "        u = model(x)\n",
    "        \n",
    "        # Compute derivatives\n",
    "        du_dx = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True)[0]\n",
    "        u_t = du_dx[..., 3:4]\n",
    "        \n",
    "        # Compute second derivatives for Laplacian\n",
    "        u_x = du_dx[..., 0:1]\n",
    "        u_y = du_dx[..., 1:2]\n",
    "        u_z = du_dx[..., 2:3]\n",
    "        \n",
    "        # Compute diffusion term ∇·(u∇u)\n",
    "        d2u_dx2 = torch.autograd.grad(u * u_x, x, torch.ones_like(u_x), create_graph=True)[0][..., 0:1]\n",
    "        d2u_dy2 = torch.autograd.grad(u * u_y, x, torch.ones_like(u_y), create_graph=True)[0][..., 1:2]\n",
    "        d2u_dz2 = torch.autograd.grad(u * u_z, x, torch.ones_like(u_z), create_graph=True)[0][..., 2:3]\n",
    "        \n",
    "        diffusion = d2u_dx2 + d2u_dy2 + d2u_dz2\n",
    "        \n",
    "        # Return residual\n",
    "        return u_t - diffusion - self.source_term(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(num_residual_points: int, \n",
    "                  resampling_ratio: float,\n",
    "                  device: torch.device) -> Dict[str, float]:\n",
    "    \"\"\"Run a single experiment with given parameters\"\"\"\n",
    "    \n",
    "    # Problem setup\n",
    "    pde = Heat3DPDE()\n",
    "    \n",
    "    # Generate training points\n",
    "    x_initial = torch.rand(100, 4, device=device)  # N_i = 100\n",
    "    x_initial[:, 3] = 0  # Set t=0 for initial condition\n",
    "    \n",
    "    x_boundary = torch.rand(300, 4, device=device)  # N_b = 300\n",
    "    # Set boundary points at x,y,z = 0,1\n",
    "    x_boundary[:100, 0] = torch.randint(0, 2, (100,), device=device)\n",
    "    x_boundary[100:200, 1] = torch.randint(0, 2, (100,), device=device)\n",
    "    x_boundary[200:, 2] = torch.randint(0, 2, (100,), device=device)\n",
    "    \n",
    "    x_residual = torch.rand(num_residual_points, 4, device=device)\n",
    "    \n",
    "    # Get conditions from exact solution\n",
    "    initial_condition = pde.exact_solution(x_initial)\n",
    "    boundary_condition = pde.exact_solution(x_boundary)\n",
    "    \n",
    "    # Create test points\n",
    "    x_test = torch.rand(10000, 4, device=device)\n",
    "    y_test = pde.exact_solution(x_test)\n",
    "    \n",
    "    # Model configuration\n",
    "    pinn_config = PINNConfig(\n",
    "        hidden_layers=6,\n",
    "        neurons_per_layer=50,\n",
    "        activation=torch.nn.Tanh()\n",
    "    )\n",
    "    \n",
    "    trainer_config = TrainerConfig(\n",
    "        resampling_enabled=True if resampling_ratio > 0 else False,\n",
    "        resampling_ratio=resampling_ratio,\n",
    "        resampling_interval=200,\n",
    "        resampling_termination=4000,\n",
    "        adam_iterations=5000\n",
    "    )\n",
    "    \n",
    "    # Create and train model\n",
    "    model = BasePINN(input_dim=4, output_dim=1, config=pinn_config).to(device)\n",
    "    trainer = PINNTrainer(model, trainer_config)\n",
    "    \n",
    "    # Training domain bounds for resampling\n",
    "    domain_bounds = torch.tensor([[0, 1], [0, 1], [0, 1], [0, 1]], device=device)\n",
    "    \n",
    "    # Train model\n",
    "    history = trainer.train(\n",
    "        x_initial, initial_condition,\n",
    "        x_boundary, boundary_condition,\n",
    "        x_residual, domain_bounds, \n",
    "        lambda x: pde.pde_operator(x, model),\n",
    "        x_test, y_test\n",
    "    )\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_experiments() -> None:\n",
    "    \"\"\"Run all experiments and create plots\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Experiment parameters\n",
    "    # residual_points = [50, 100, 500, 1000, 2000]\n",
    "    residual_points = [10, 20, 30, 40, 50]\n",
    "    resampling_ratios = [0.0, 0.3, 0.6, 0.9]  # 0.0 = standard PINN\n",
    "    num_repeats = 5\n",
    "    \n",
    "    # Store results\n",
    "    results = {eta: [] for eta in resampling_ratios}\n",
    "    \n",
    "    # Run experiments\n",
    "    for n_points in residual_points:\n",
    "        print(f\"\\nRunning experiments with {n_points} residual points\")\n",
    "        \n",
    "        for eta in resampling_ratios:\n",
    "            print(f\"  Resampling ratio η = {eta}\")\n",
    "            \n",
    "            errors = []\n",
    "            for i in range(num_repeats):\n",
    "                print(f\"    Repeat {i+1}/{num_repeats}\")\n",
    "                history = run_experiment(n_points, eta, device)\n",
    "                errors.append(history['l2_relative'][-1])\n",
    "                \n",
    "            mean_error = np.mean(errors)\n",
    "            results[eta].append(mean_error)\n",
    "            \n",
    "    # Create visualization\n",
    "    visualizer = PINNVisualizer()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for eta, errors in results.items():\n",
    "        if eta == 0.0:\n",
    "            label = 'Standard PINN'\n",
    "        else:\n",
    "            label = f'η = {eta}'\n",
    "        plt.plot(residual_points, errors, 'o-', label=label)\n",
    "    \n",
    "    plt.xlabel('Num. of Residual points')\n",
    "    plt.ylabel('Relative L2 error')\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.title('Effect of Resampling Strategy on 3D Heat Equation')\n",
    "    plt.savefig('heat_3d_resampling.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Running experiments with 10 residual points\n",
      "  Resampling ratio η = 0.0\n",
      "    Repeat 1/5\n",
      "Starting Adam optimization phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mated\\anaconda3\\envs\\torch_win39\\lib\\site-packages\\torch\\autograd\\graph.py:823: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:180.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss = 139.644119\n",
      "Step 100: Loss = 5.227610\n",
      "Step 200: Loss = 3.289800\n",
      "Step 300: Loss = 2.725805\n",
      "Step 400: Loss = 2.332481\n",
      "Step 500: Loss = 1.925111\n",
      "Step 600: Loss = 1.629492\n",
      "Step 700: Loss = 1.918779\n",
      "Step 800: Loss = 0.907755\n",
      "Step 900: Loss = 0.799729\n",
      "Step 1000: Loss = 0.763501\n",
      "Step 1100: Loss = 0.734997\n",
      "Step 1200: Loss = 0.710823\n",
      "Step 1300: Loss = 0.690092\n",
      "Step 1400: Loss = 0.672155\n",
      "Step 1500: Loss = 0.656491\n",
      "Step 1600: Loss = 0.642682\n",
      "Step 1700: Loss = 0.630534\n",
      "Step 1800: Loss = 0.619453\n",
      "Step 1900: Loss = 0.609539\n",
      "Step 2000: Loss = 0.600585\n",
      "Step 2100: Loss = 0.592447\n",
      "Step 2200: Loss = 0.619903\n",
      "Step 2300: Loss = 0.578591\n",
      "Step 2400: Loss = 0.572619\n",
      "Step 2500: Loss = 0.567208\n",
      "Step 2600: Loss = 0.562299\n",
      "Step 2700: Loss = 0.559350\n",
      "Step 2800: Loss = 0.554037\n",
      "Step 2900: Loss = 0.550312\n",
      "Step 3000: Loss = 0.555537\n",
      "Step 3100: Loss = 0.544049\n",
      "Step 3200: Loss = 0.541019\n",
      "Step 3300: Loss = 0.543022\n",
      "Step 3400: Loss = 0.535655\n",
      "Step 3500: Loss = 0.533008\n",
      "Step 3600: Loss = 0.531023\n",
      "Step 3700: Loss = 0.528615\n",
      "Step 3800: Loss = 0.526278\n",
      "Step 3900: Loss = 0.523993\n",
      "Step 4000: Loss = 0.521805\n",
      "Step 4100: Loss = 0.519832\n",
      "Step 4200: Loss = 0.517721\n",
      "Step 4300: Loss = 0.527492\n",
      "Step 4400: Loss = 0.513946\n",
      "Step 4500: Loss = 0.512041\n",
      "Step 4600: Loss = 0.510225\n",
      "Step 4700: Loss = 0.508521\n",
      "Step 4800: Loss = 0.506768\n",
      "Step 4900: Loss = 0.505852\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 2/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 192.972153\n",
      "Step 100: Loss = 6.224082\n",
      "Step 200: Loss = 3.681535\n",
      "Step 300: Loss = 2.987069\n",
      "Step 400: Loss = 2.666478\n",
      "Step 500: Loss = 2.420246\n",
      "Step 600: Loss = 2.258500\n",
      "Step 700: Loss = 2.042441\n",
      "Step 800: Loss = 1.817352\n",
      "Step 900: Loss = 1.769585\n",
      "Step 1000: Loss = 1.758212\n",
      "Step 1100: Loss = 2.724804\n",
      "Step 1200: Loss = 1.499236\n",
      "Step 1300: Loss = 1.454832\n",
      "Step 1400: Loss = 1.530743\n",
      "Step 1500: Loss = 1.376147\n",
      "Step 1600: Loss = 1.338783\n",
      "Step 1700: Loss = 2.509013\n",
      "Step 1800: Loss = 1.268367\n",
      "Step 1900: Loss = 1.534836\n",
      "Step 2000: Loss = 1.203900\n",
      "Step 2100: Loss = 1.208966\n",
      "Step 2200: Loss = 1.147351\n",
      "Step 2300: Loss = 1.157099\n",
      "Step 2400: Loss = 1.107356\n",
      "Step 2500: Loss = 1.152830\n",
      "Step 2600: Loss = 1.100312\n",
      "Step 2700: Loss = 1.069227\n",
      "Step 2800: Loss = 1.087813\n",
      "Step 2900: Loss = 1.036826\n",
      "Step 3000: Loss = 1.038195\n",
      "Step 3100: Loss = 1.013326\n",
      "Step 3200: Loss = 1.015358\n",
      "Step 3300: Loss = 0.986219\n",
      "Step 3400: Loss = 0.991776\n",
      "Step 3500: Loss = 0.972825\n",
      "Step 3600: Loss = 1.054843\n",
      "Step 3700: Loss = 0.951864\n",
      "Step 3800: Loss = 0.939700\n",
      "Step 3900: Loss = 0.936566\n",
      "Step 4000: Loss = 0.923790\n",
      "Step 4100: Loss = 0.931286\n",
      "Step 4200: Loss = 0.906013\n",
      "Step 4300: Loss = 0.902779\n",
      "Step 4400: Loss = 0.892639\n",
      "Step 4500: Loss = 0.891460\n",
      "Step 4600: Loss = 0.878874\n",
      "Step 4700: Loss = 0.872674\n",
      "Step 4800: Loss = 2.135418\n",
      "Step 4900: Loss = 0.863707\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 3/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 138.088715\n",
      "Step 100: Loss = 2.148992\n",
      "Step 200: Loss = 1.319247\n",
      "Step 300: Loss = 2.031203\n",
      "Step 400: Loss = 1.029401\n",
      "Step 500: Loss = 0.939512\n",
      "Step 600: Loss = 0.857592\n",
      "Step 700: Loss = 0.780608\n",
      "Step 800: Loss = 0.737718\n",
      "Step 900: Loss = 0.800459\n",
      "Step 1000: Loss = 0.685235\n",
      "Step 1100: Loss = 0.672438\n",
      "Step 1200: Loss = 0.672987\n",
      "Step 1300: Loss = 0.660375\n",
      "Step 1400: Loss = 0.653615\n",
      "Step 1500: Loss = 0.667587\n",
      "Step 1600: Loss = 0.643678\n",
      "Step 1700: Loss = 0.851025\n",
      "Step 1800: Loss = 0.636664\n",
      "Step 1900: Loss = 0.640700\n",
      "Step 2000: Loss = 0.629882\n",
      "Step 2100: Loss = 0.628860\n",
      "Step 2200: Loss = 0.624940\n",
      "Step 2300: Loss = 0.623464\n",
      "Step 2400: Loss = 0.620076\n",
      "Step 2500: Loss = 0.618135\n",
      "Step 2600: Loss = 0.615768\n",
      "Step 2700: Loss = 0.616378\n",
      "Step 2800: Loss = 0.611652\n",
      "Step 2900: Loss = 0.609942\n",
      "Step 3000: Loss = 0.607753\n",
      "Step 3100: Loss = 0.606062\n",
      "Step 3200: Loss = 0.606091\n",
      "Step 3300: Loss = 0.602207\n",
      "Step 3400: Loss = 0.600647\n",
      "Step 3500: Loss = 0.598769\n",
      "Step 3600: Loss = 0.597537\n",
      "Step 3700: Loss = 0.595225\n",
      "Step 3800: Loss = 0.594178\n",
      "Step 3900: Loss = 0.591807\n",
      "Step 4000: Loss = 0.590118\n",
      "Step 4100: Loss = 0.588346\n",
      "Step 4200: Loss = 0.586628\n",
      "Step 4300: Loss = 0.584827\n",
      "Step 4400: Loss = 0.583330\n",
      "Step 4500: Loss = 0.581584\n",
      "Step 4600: Loss = 0.579832\n",
      "Step 4700: Loss = 0.578835\n",
      "Step 4800: Loss = 0.576370\n",
      "Step 4900: Loss = 0.574853\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 4/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 212.596283\n",
      "Step 100: Loss = 30.988312\n",
      "Step 200: Loss = 1.515226\n",
      "Step 300: Loss = 1.144746\n",
      "Step 400: Loss = 1.060050\n",
      "Step 500: Loss = 1.012539\n",
      "Step 600: Loss = 0.972136\n",
      "Step 700: Loss = 0.934689\n",
      "Step 800: Loss = 0.903858\n",
      "Step 900: Loss = 0.876302\n",
      "Step 1000: Loss = 0.851561\n",
      "Step 1100: Loss = 0.829242\n",
      "Step 1200: Loss = 0.809004\n",
      "Step 1300: Loss = 0.790549\n",
      "Step 1400: Loss = 0.773615\n",
      "Step 1500: Loss = 0.757988\n",
      "Step 1600: Loss = 0.743494\n",
      "Step 1700: Loss = 0.729997\n",
      "Step 1800: Loss = 0.717387\n",
      "Step 1900: Loss = 0.705575\n",
      "Step 2000: Loss = 0.694486\n",
      "Step 2100: Loss = 0.684057\n",
      "Step 2200: Loss = 0.674235\n",
      "Step 2300: Loss = 0.664971\n",
      "Step 2400: Loss = 0.656223\n",
      "Step 2500: Loss = 0.647955\n",
      "Step 2600: Loss = 0.640134\n",
      "Step 2700: Loss = 0.632730\n",
      "Step 2800: Loss = 0.625715\n",
      "Step 2900: Loss = 0.619065\n",
      "Step 3000: Loss = 0.612758\n",
      "Step 3100: Loss = 0.606771\n",
      "Step 3200: Loss = 0.601087\n",
      "Step 3300: Loss = 0.595688\n",
      "Step 3400: Loss = 0.590558\n",
      "Step 3500: Loss = 0.585681\n",
      "Step 3600: Loss = 0.581045\n",
      "Step 3700: Loss = 0.576638\n",
      "Step 3800: Loss = 0.572447\n",
      "Step 3900: Loss = 0.568462\n",
      "Step 4000: Loss = 0.564673\n",
      "Step 4100: Loss = 0.561072\n",
      "Step 4200: Loss = 0.557659\n",
      "Step 4300: Loss = 0.554516\n",
      "Step 4400: Loss = 0.551521\n",
      "Step 4500: Loss = 0.548869\n",
      "Step 4600: Loss = 0.546089\n",
      "Step 4700: Loss = 0.543969\n",
      "Step 4800: Loss = 0.565581\n",
      "Step 4900: Loss = 0.539090\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 5/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 166.782288\n",
      "Step 100: Loss = 1.235481\n",
      "Step 200: Loss = 0.846780\n",
      "Step 300: Loss = 0.731045\n",
      "Step 400: Loss = 0.750130\n",
      "Step 500: Loss = 0.651214\n",
      "Step 600: Loss = 0.621509\n",
      "Step 700: Loss = 0.636234\n",
      "Step 800: Loss = 0.611507\n",
      "Step 900: Loss = 0.600910\n",
      "Step 1000: Loss = 0.594317\n",
      "Step 1100: Loss = 0.589051\n",
      "Step 1200: Loss = 0.590276\n",
      "Step 1300: Loss = 0.582585\n",
      "Step 1400: Loss = 0.578273\n",
      "Step 1500: Loss = 0.574472\n",
      "Step 1600: Loss = 0.575220\n",
      "Step 1700: Loss = 0.569398\n",
      "Step 1800: Loss = 0.566229\n",
      "Step 1900: Loss = 0.563368\n",
      "Step 2000: Loss = 0.565917\n",
      "Step 2100: Loss = 0.559744\n",
      "Step 2200: Loss = 0.557385\n",
      "Step 2300: Loss = 0.555265\n",
      "Step 2400: Loss = 0.553330\n",
      "Step 2500: Loss = 0.552403\n",
      "Step 2600: Loss = 0.550449\n",
      "Step 2700: Loss = 0.548897\n",
      "Step 2800: Loss = 0.548734\n",
      "Step 2900: Loss = 0.546860\n",
      "Step 3000: Loss = 0.545621\n",
      "Step 3100: Loss = 0.544483\n",
      "Step 3200: Loss = 0.543440\n",
      "Step 3300: Loss = 0.542813\n",
      "Step 3400: Loss = 0.541852\n",
      "Step 3500: Loss = 0.540999\n",
      "Step 3600: Loss = 0.566349\n",
      "Step 3700: Loss = 0.539752\n",
      "Step 3800: Loss = 0.539047\n",
      "Step 3900: Loss = 0.538390\n",
      "Step 4000: Loss = 0.538661\n",
      "Step 4100: Loss = 0.537287\n",
      "Step 4200: Loss = 0.540895\n",
      "Step 4300: Loss = 0.536891\n",
      "Step 4400: Loss = 0.535828\n",
      "Step 4500: Loss = 0.536446\n",
      "Step 4600: Loss = 0.567109\n",
      "Step 4700: Loss = 0.534456\n",
      "Step 4800: Loss = 0.534023\n",
      "Step 4900: Loss = 0.534156\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "  Resampling ratio η = 0.3\n",
      "    Repeat 1/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 398.348541\n",
      "Step 100: Loss = 24.424805\n",
      "Step 200: Loss = 151.935104\n",
      "Step 300: Loss = 18.181391\n",
      "Step 400: Loss = 170.439240\n",
      "Step 500: Loss = 9.332326\n",
      "Step 600: Loss = 54.593288\n",
      "Step 700: Loss = 8.413127\n",
      "Step 800: Loss = 52.571548\n",
      "Step 900: Loss = 6.383642\n",
      "Step 1000: Loss = 82.223190\n",
      "Step 1100: Loss = 5.858703\n",
      "Step 1200: Loss = 266.144775\n",
      "Step 1300: Loss = 15.700026\n",
      "Step 1400: Loss = 41.828766\n",
      "Step 1500: Loss = 6.866036\n",
      "Step 1600: Loss = 83.379227\n",
      "Step 1700: Loss = 5.424555\n",
      "Step 1800: Loss = 114.814819\n",
      "Step 1900: Loss = 4.347165\n",
      "Step 2000: Loss = 44.281384\n",
      "Step 2100: Loss = 4.105634\n",
      "Step 2200: Loss = 366.098145\n",
      "Step 2300: Loss = 6.043944\n",
      "Step 2400: Loss = 62.046772\n",
      "Step 2500: Loss = 3.519588\n",
      "Step 2600: Loss = 372.426880\n",
      "Step 2700: Loss = 1.566980\n",
      "Step 2800: Loss = 283.464050\n",
      "Step 2900: Loss = 29.340734\n",
      "Step 3000: Loss = 47.870762\n",
      "Step 3100: Loss = 2.281966\n",
      "Step 3200: Loss = 222.987686\n",
      "Step 3300: Loss = 2.218671\n",
      "Step 3400: Loss = 249.271805\n",
      "Step 3500: Loss = 2.769609\n",
      "Step 3600: Loss = 256.043945\n",
      "Step 3700: Loss = 5.635460\n",
      "Step 3800: Loss = 88.250404\n",
      "Step 3900: Loss = 1.031792\n",
      "Step 4000: Loss = 0.899755\n",
      "Step 4100: Loss = 0.841169\n",
      "Step 4200: Loss = 0.802285\n",
      "Step 4300: Loss = 0.774513\n",
      "Step 4400: Loss = 0.753684\n",
      "Step 4500: Loss = 0.737356\n",
      "Step 4600: Loss = 0.724063\n",
      "Step 4700: Loss = 0.712916\n",
      "Step 4800: Loss = 0.703356\n",
      "Step 4900: Loss = 0.695020\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 2/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 206.156769\n",
      "Step 100: Loss = 8.111753\n",
      "Step 200: Loss = 167.688675\n",
      "Step 300: Loss = 5.249324\n",
      "Step 400: Loss = 381.392334\n",
      "Step 500: Loss = 1.725824\n",
      "Step 600: Loss = 69.461189\n",
      "Step 700: Loss = 0.951458\n",
      "Step 800: Loss = 4.538823\n",
      "Step 900: Loss = 0.748680\n",
      "Step 1000: Loss = 132.278717\n",
      "Step 1100: Loss = 1.231089\n",
      "Step 1200: Loss = 59.062798\n",
      "Step 1300: Loss = 0.925637\n",
      "Step 1400: Loss = 44.158989\n",
      "Step 1500: Loss = 0.750965\n",
      "Step 1600: Loss = 40.584618\n",
      "Step 1700: Loss = 0.662307\n",
      "Step 1800: Loss = 131.866714\n",
      "Step 1900: Loss = 2.924085\n",
      "Step 2000: Loss = 493.117371\n",
      "Step 2100: Loss = 1.253972\n",
      "Step 2200: Loss = 73.063477\n",
      "Step 2300: Loss = 1.399387\n",
      "Step 2400: Loss = 525.114807\n",
      "Step 2500: Loss = 1.733582\n",
      "Step 2600: Loss = 8.842777\n",
      "Step 2700: Loss = 1.066119\n",
      "Step 2800: Loss = 130.888565\n",
      "Step 2900: Loss = 2.051148\n",
      "Step 3000: Loss = 235.111832\n",
      "Step 3100: Loss = 0.791897\n",
      "Step 3200: Loss = 36.688969\n",
      "Step 3300: Loss = 0.913651\n",
      "Step 3400: Loss = 19.203335\n",
      "Step 3500: Loss = 0.948675\n",
      "Step 3600: Loss = 233.976105\n",
      "Step 3700: Loss = 2.488208\n",
      "Step 3800: Loss = 42.105309\n",
      "Step 3900: Loss = 0.945933\n",
      "Step 4000: Loss = 0.869147\n",
      "Step 4100: Loss = 0.838512\n",
      "Step 4200: Loss = 0.822193\n",
      "Step 4300: Loss = 0.810613\n",
      "Step 4400: Loss = 0.800805\n",
      "Step 4500: Loss = 0.791824\n",
      "Step 4600: Loss = 0.783349\n",
      "Step 4700: Loss = 0.775257\n",
      "Step 4800: Loss = 0.767490\n",
      "Step 4900: Loss = 0.760014\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 3/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 199.318695\n",
      "Step 100: Loss = 3.698153\n",
      "Step 200: Loss = 33.119728\n",
      "Step 300: Loss = 3.284086\n",
      "Step 400: Loss = 192.544037\n",
      "Step 500: Loss = 8.520451\n",
      "Step 600: Loss = 559.959900\n",
      "Step 700: Loss = 6.352423\n",
      "Step 800: Loss = 348.230835\n",
      "Step 900: Loss = 1.384854\n",
      "Step 1000: Loss = 94.362495\n",
      "Step 1100: Loss = 1.008414\n",
      "Step 1200: Loss = 58.490215\n",
      "Step 1300: Loss = 1.298618\n",
      "Step 1400: Loss = 184.653931\n",
      "Step 1500: Loss = 0.826386\n",
      "Step 1600: Loss = 12.588600\n",
      "Step 1700: Loss = 3.479840\n",
      "Step 1800: Loss = 772.017456\n",
      "Step 1900: Loss = 80.449219\n",
      "Step 2000: Loss = 113.449249\n",
      "Step 2100: Loss = 47.966846\n",
      "Step 2200: Loss = 95.158897\n",
      "Step 2300: Loss = 1.955648\n",
      "Step 2400: Loss = 135.548630\n",
      "Step 2500: Loss = 1.091819\n",
      "Step 2600: Loss = 282.879578\n",
      "Step 2700: Loss = 0.840974\n",
      "Step 2800: Loss = 47.142933\n",
      "Step 2900: Loss = 0.761218\n",
      "Step 3000: Loss = 180.115219\n",
      "Step 3100: Loss = 0.722812\n",
      "Step 3200: Loss = 251.928986\n",
      "Step 3300: Loss = 1.156684\n",
      "Step 3400: Loss = 3.925725\n",
      "Step 3500: Loss = 0.682367\n",
      "Step 3600: Loss = 1163.152344\n",
      "Step 3700: Loss = 1.901199\n",
      "Step 3800: Loss = 71.624809\n",
      "Step 3900: Loss = 2.392972\n",
      "Step 4000: Loss = 1.756400\n",
      "Step 4100: Loss = 1.313526\n",
      "Step 4200: Loss = 1.040975\n",
      "Step 4300: Loss = 0.885565\n",
      "Step 4400: Loss = 0.799694\n",
      "Step 4500: Loss = 0.750074\n",
      "Step 4600: Loss = 0.718186\n",
      "Step 4700: Loss = 0.695439\n",
      "Step 4800: Loss = 0.678107\n",
      "Step 4900: Loss = 0.664454\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 4/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 203.114868\n",
      "Step 100: Loss = 1.761948\n",
      "Step 200: Loss = 5.079058\n",
      "Step 300: Loss = 0.888554\n",
      "Step 400: Loss = 157.554230\n",
      "Step 500: Loss = 1.731741\n",
      "Step 600: Loss = 286.930023\n",
      "Step 700: Loss = 2.086251\n",
      "Step 800: Loss = 277.641663\n",
      "Step 900: Loss = 2.693841\n",
      "Step 1000: Loss = 54.591972\n",
      "Step 1100: Loss = 2.581665\n",
      "Step 1200: Loss = 193.136459\n",
      "Step 1300: Loss = 2.751015\n",
      "Step 1400: Loss = 88.166878\n",
      "Step 1500: Loss = 2.150644\n",
      "Step 1600: Loss = 13.586516\n",
      "Step 1700: Loss = 1.843782\n",
      "Step 1800: Loss = 178.394089\n",
      "Step 1900: Loss = 2.492293\n",
      "Step 2000: Loss = 57.690575\n",
      "Step 2100: Loss = 1.433925\n",
      "Step 2200: Loss = 68.962273\n",
      "Step 2300: Loss = 0.932742\n",
      "Step 2400: Loss = 156.178085\n",
      "Step 2500: Loss = 1.025993\n",
      "Step 2600: Loss = 42.683926\n",
      "Step 2700: Loss = 0.860066\n",
      "Step 2800: Loss = 26.112253\n",
      "Step 2900: Loss = 0.813925\n",
      "Step 3000: Loss = 31.021730\n",
      "Step 3100: Loss = 0.776784\n",
      "Step 3200: Loss = 73.492790\n",
      "Step 3300: Loss = 0.760130\n",
      "Step 3400: Loss = 95.047150\n",
      "Step 3500: Loss = 0.817168\n",
      "Step 3600: Loss = 51.975632\n",
      "Step 3700: Loss = 1.012357\n",
      "Step 3800: Loss = 48.400616\n",
      "Step 3900: Loss = 0.673345\n",
      "Step 4000: Loss = 0.663806\n",
      "Step 4100: Loss = 0.658783\n",
      "Step 4200: Loss = 0.655161\n",
      "Step 4300: Loss = 0.652182\n",
      "Step 4400: Loss = 0.649583\n",
      "Step 4500: Loss = 0.647249\n",
      "Step 4600: Loss = 0.645114\n",
      "Step 4700: Loss = 0.643136\n",
      "Step 4800: Loss = 0.641282\n",
      "Step 4900: Loss = 0.639530\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 5/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 301.711914\n",
      "Step 100: Loss = 15.971253\n",
      "Step 200: Loss = 77.064873\n",
      "Step 300: Loss = 16.306007\n",
      "Step 400: Loss = 254.648331\n",
      "Step 500: Loss = 6.306432\n",
      "Step 600: Loss = 66.625221\n",
      "Step 700: Loss = 3.671817\n",
      "Step 800: Loss = 217.709946\n",
      "Step 900: Loss = 13.503181\n",
      "Step 1000: Loss = 55.753738\n",
      "Step 1100: Loss = 4.075729\n",
      "Step 1200: Loss = 54.145805\n",
      "Step 1300: Loss = 2.384699\n",
      "Step 1400: Loss = 442.477539\n",
      "Step 1500: Loss = 38.576252\n",
      "Step 1600: Loss = 462.205505\n",
      "Step 1700: Loss = 5.193091\n",
      "Step 1800: Loss = 134.663956\n",
      "Step 1900: Loss = 22.687416\n",
      "Step 2000: Loss = 187.300934\n",
      "Step 2100: Loss = 2.950545\n",
      "Step 2200: Loss = 363.334167\n",
      "Step 2300: Loss = 2.904913\n",
      "Step 2400: Loss = 490.850403\n",
      "Step 2500: Loss = 2.968104\n",
      "Step 2600: Loss = 41.628330\n",
      "Step 2700: Loss = 2.641551\n",
      "Step 2800: Loss = 65.790092\n",
      "Step 2900: Loss = 2.195113\n",
      "Step 3000: Loss = 240.964523\n",
      "Step 3100: Loss = 2.475364\n",
      "Step 3200: Loss = 143.883621\n",
      "Step 3300: Loss = 2.530786\n",
      "Step 3400: Loss = 164.936981\n",
      "Step 3500: Loss = 4.986384\n",
      "Step 3600: Loss = 59.313690\n",
      "Step 3700: Loss = 12.934171\n",
      "Step 3800: Loss = 269.691254\n",
      "Step 3900: Loss = 3.992500\n",
      "Step 4000: Loss = 2.818513\n",
      "Step 4100: Loss = 2.609735\n",
      "Step 4200: Loss = 2.454591\n",
      "Step 4300: Loss = 2.333963\n",
      "Step 4400: Loss = 2.237159\n",
      "Step 4500: Loss = 2.157346\n",
      "Step 4600: Loss = 2.090385\n",
      "Step 4700: Loss = 2.033725\n",
      "Step 4800: Loss = 1.985639\n",
      "Step 4900: Loss = 1.944809\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "  Resampling ratio η = 0.6\n",
      "    Repeat 1/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 217.481689\n",
      "Step 100: Loss = 3.956997\n",
      "Step 200: Loss = 154.482346\n",
      "Step 300: Loss = 2.027234\n",
      "Step 400: Loss = 941.279724\n",
      "Step 500: Loss = 84.035614\n",
      "Step 600: Loss = 113.691055\n",
      "Step 700: Loss = 4.761518\n",
      "Step 800: Loss = 132.835556\n",
      "Step 900: Loss = 2.135079\n",
      "Step 1000: Loss = 260.373688\n",
      "Step 1100: Loss = 4.772302\n",
      "Step 1200: Loss = 205.264832\n",
      "Step 1300: Loss = 4.981411\n",
      "Step 1400: Loss = 67.534119\n",
      "Step 1500: Loss = 3.121796\n",
      "Step 1600: Loss = 194.698761\n",
      "Step 1700: Loss = 3.352491\n",
      "Step 1800: Loss = 77.432030\n",
      "Step 1900: Loss = 1.671095\n",
      "Step 2000: Loss = 178.020233\n",
      "Step 2100: Loss = 1.731739\n",
      "Step 2200: Loss = 138.637772\n",
      "Step 2300: Loss = 2.218788\n",
      "Step 2400: Loss = 259.170990\n",
      "Step 2500: Loss = 1.017495\n",
      "Step 2600: Loss = 61.830532\n",
      "Step 2700: Loss = 0.751258\n",
      "Step 2800: Loss = 149.416641\n",
      "Step 2900: Loss = 0.767302\n",
      "Step 3000: Loss = 185.997269\n",
      "Step 3100: Loss = 1.091647\n",
      "Step 3200: Loss = 245.955902\n",
      "Step 3300: Loss = 1.932145\n",
      "Step 3400: Loss = 275.613342\n",
      "Step 3500: Loss = 4.869482\n",
      "Step 3600: Loss = 144.297501\n",
      "Step 3700: Loss = 2.273794\n",
      "Step 3800: Loss = 467.946991\n",
      "Step 3900: Loss = 2.071497\n",
      "Step 4000: Loss = 1.458344\n",
      "Step 4100: Loss = 1.163460\n",
      "Step 4200: Loss = 0.981326\n",
      "Step 4300: Loss = 0.869339\n",
      "Step 4400: Loss = 0.800440\n",
      "Step 4500: Loss = 0.757473\n",
      "Step 4600: Loss = 0.729959\n",
      "Step 4700: Loss = 0.711665\n",
      "Step 4800: Loss = 0.698943\n",
      "Step 4900: Loss = 0.689668\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 2/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 267.873779\n",
      "Step 100: Loss = 45.207054\n",
      "Step 200: Loss = 323.026489\n",
      "Step 300: Loss = 5.658657\n",
      "Step 400: Loss = 120.085800\n",
      "Step 500: Loss = 7.553874\n",
      "Step 600: Loss = 737.728394\n",
      "Step 700: Loss = 19.799070\n",
      "Step 800: Loss = 97.850754\n",
      "Step 900: Loss = 16.510921\n",
      "Step 1000: Loss = 303.026764\n",
      "Step 1100: Loss = 6.569114\n",
      "Step 1200: Loss = 229.891907\n",
      "Step 1300: Loss = 1.257062\n",
      "Step 1400: Loss = 33.830704\n",
      "Step 1500: Loss = 0.730336\n",
      "Step 1600: Loss = 737.890625\n",
      "Step 1700: Loss = 3.866931\n",
      "Step 1800: Loss = 135.262924\n",
      "Step 1900: Loss = 0.763574\n",
      "Step 2000: Loss = 229.160385\n",
      "Step 2100: Loss = 0.869353\n",
      "Step 2200: Loss = 163.971237\n",
      "Step 2300: Loss = 0.794367\n",
      "Step 2400: Loss = 222.587082\n",
      "Step 2500: Loss = 1.017840\n",
      "Step 2600: Loss = 301.849304\n",
      "Step 2700: Loss = 0.897818\n",
      "Step 2800: Loss = 352.765717\n",
      "Step 2900: Loss = 0.913640\n",
      "Step 3000: Loss = 201.525223\n",
      "Step 3100: Loss = 0.846285\n",
      "Step 3200: Loss = 91.793083\n",
      "Step 3300: Loss = 1.555495\n",
      "Step 3400: Loss = 656.754883\n",
      "Step 3500: Loss = 2.747712\n",
      "Step 3600: Loss = 639.808228\n",
      "Step 3700: Loss = 4.017555\n",
      "Step 3800: Loss = 345.851624\n",
      "Step 3900: Loss = 1.738325\n",
      "Step 4000: Loss = 1.445102\n",
      "Step 4100: Loss = 1.286984\n",
      "Step 4200: Loss = 1.179597\n",
      "Step 4300: Loss = 1.100940\n",
      "Step 4400: Loss = 1.039598\n",
      "Step 4500: Loss = 0.989374\n",
      "Step 4600: Loss = 0.946757\n",
      "Step 4700: Loss = 0.909677\n",
      "Step 4800: Loss = 0.876873\n",
      "Step 4900: Loss = 0.847545\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 3/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 235.136337\n",
      "Step 100: Loss = 7.459836\n",
      "Step 200: Loss = 299.159180\n",
      "Step 300: Loss = 1.562763\n",
      "Step 400: Loss = 162.223328\n",
      "Step 500: Loss = 28.620195\n",
      "Step 600: Loss = 276.706085\n",
      "Step 700: Loss = 75.258957\n",
      "Step 800: Loss = 495.140076\n",
      "Step 900: Loss = 75.361290\n",
      "Step 1000: Loss = 973.918335\n",
      "Step 1100: Loss = 138.010742\n",
      "Step 1200: Loss = 206.989960\n",
      "Step 1300: Loss = 48.484589\n",
      "Step 1400: Loss = 244.869827\n",
      "Step 1500: Loss = 24.920776\n",
      "Step 1600: Loss = 730.672424\n",
      "Step 1700: Loss = 19.278038\n",
      "Step 1800: Loss = 294.447021\n",
      "Step 1900: Loss = 16.634945\n",
      "Step 2000: Loss = 368.750885\n",
      "Step 2100: Loss = 115.216461\n",
      "Step 2200: Loss = 285.508453\n",
      "Step 2300: Loss = 9.422347\n",
      "Step 2400: Loss = 179.802795\n",
      "Step 2500: Loss = 7.418400\n",
      "Step 2600: Loss = 320.111084\n",
      "Step 2700: Loss = 5.404765\n",
      "Step 2800: Loss = 211.130264\n",
      "Step 2900: Loss = 5.377668\n",
      "Step 3000: Loss = 689.451294\n",
      "Step 3100: Loss = 5.153832\n",
      "Step 3200: Loss = 544.565857\n",
      "Step 3300: Loss = 34.794842\n",
      "Step 3400: Loss = 93.485100\n",
      "Step 3500: Loss = 3.135824\n",
      "Step 3600: Loss = 153.982025\n",
      "Step 3700: Loss = 3.495474\n",
      "Step 3800: Loss = 256.934174\n",
      "Step 3900: Loss = 14.272147\n",
      "Step 4000: Loss = 5.576777\n",
      "Step 4100: Loss = 1.728954\n",
      "Step 4200: Loss = 1.556289\n",
      "Step 4300: Loss = 1.512994\n",
      "Step 4400: Loss = 1.474379\n",
      "Step 4500: Loss = 1.442230\n",
      "Step 4600: Loss = 1.414971\n",
      "Step 4700: Loss = 1.391219\n",
      "Step 4800: Loss = 1.370140\n",
      "Step 4900: Loss = 1.351199\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 4/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 192.284958\n",
      "Step 100: Loss = 10.942907\n",
      "Step 200: Loss = 193.421631\n",
      "Step 300: Loss = 4.418966\n",
      "Step 400: Loss = 25.781593\n",
      "Step 500: Loss = 2.188498\n",
      "Step 600: Loss = 154.788956\n",
      "Step 700: Loss = 4.378631\n",
      "Step 800: Loss = 166.816742\n",
      "Step 900: Loss = 45.691998\n",
      "Step 1000: Loss = 251.813690\n",
      "Step 1100: Loss = 1.509189\n",
      "Step 1200: Loss = 238.771408\n",
      "Step 1300: Loss = 0.909228\n",
      "Step 1400: Loss = 317.847534\n",
      "Step 1500: Loss = 1.020986\n",
      "Step 1600: Loss = 137.407883\n",
      "Step 1700: Loss = 0.983973\n",
      "Step 1800: Loss = 216.625748\n",
      "Step 1900: Loss = 1.251839\n",
      "Step 2000: Loss = 212.794830\n",
      "Step 2100: Loss = 27.229223\n",
      "Step 2200: Loss = 246.695496\n",
      "Step 2300: Loss = 3.408265\n",
      "Step 2400: Loss = 332.699097\n",
      "Step 2500: Loss = 1.356586\n",
      "Step 2600: Loss = 184.305954\n",
      "Step 2700: Loss = 0.640755\n",
      "Step 2800: Loss = 399.438904\n",
      "Step 2900: Loss = 9.026711\n",
      "Step 3000: Loss = 55.495785\n",
      "Step 3100: Loss = 0.992552\n",
      "Step 3200: Loss = 275.842651\n",
      "Step 3300: Loss = 1.036774\n",
      "Step 3400: Loss = 74.951347\n",
      "Step 3500: Loss = 0.851571\n",
      "Step 3600: Loss = 244.044434\n",
      "Step 3700: Loss = 0.852744\n",
      "Step 3800: Loss = 171.570923\n",
      "Step 3900: Loss = 0.867774\n",
      "Step 4000: Loss = 0.791286\n",
      "Step 4100: Loss = 0.749052\n",
      "Step 4200: Loss = 0.720120\n",
      "Step 4300: Loss = 0.698834\n",
      "Step 4400: Loss = 0.682616\n",
      "Step 4500: Loss = 0.669959\n",
      "Step 4600: Loss = 0.659862\n",
      "Step 4700: Loss = 0.651627\n",
      "Step 4800: Loss = 0.644758\n",
      "Step 4900: Loss = 0.638902\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 5/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 232.977737\n",
      "Step 100: Loss = 8.086001\n",
      "Step 200: Loss = 195.539642\n",
      "Step 300: Loss = 4.255718\n",
      "Step 400: Loss = 112.025200\n",
      "Step 500: Loss = 3.543400\n",
      "Step 600: Loss = 440.479156\n",
      "Step 700: Loss = 3.358537\n",
      "Step 800: Loss = 103.818741\n",
      "Step 900: Loss = 1.495267\n",
      "Step 1000: Loss = 165.655441\n",
      "Step 1100: Loss = 1.686931\n",
      "Step 1200: Loss = 97.344353\n",
      "Step 1300: Loss = 0.876180\n",
      "Step 1400: Loss = 259.208893\n",
      "Step 1500: Loss = 0.978104\n",
      "Step 1600: Loss = 1052.449707\n",
      "Step 1700: Loss = 2.146512\n",
      "Step 1800: Loss = 188.303177\n",
      "Step 1900: Loss = 1.339628\n",
      "Step 2000: Loss = 584.004944\n",
      "Step 2100: Loss = 2.483693\n",
      "Step 2200: Loss = 253.496475\n",
      "Step 2300: Loss = 1.726369\n",
      "Step 2400: Loss = 211.069366\n",
      "Step 2500: Loss = 1.260040\n",
      "Step 2600: Loss = 673.442017\n",
      "Step 2700: Loss = 1.996756\n",
      "Step 2800: Loss = 479.981354\n",
      "Step 2900: Loss = 1.744304\n",
      "Step 3000: Loss = 349.444061\n",
      "Step 3100: Loss = 2.605407\n",
      "Step 3200: Loss = 181.198441\n",
      "Step 3300: Loss = 0.952932\n",
      "Step 3400: Loss = 299.713287\n",
      "Step 3500: Loss = 1.035928\n",
      "Step 3600: Loss = 273.708252\n",
      "Step 3700: Loss = 0.992637\n",
      "Step 3800: Loss = 191.355896\n",
      "Step 3900: Loss = 1.043256\n",
      "Step 4000: Loss = 0.903603\n",
      "Step 4100: Loss = 0.861216\n",
      "Step 4200: Loss = 0.829359\n",
      "Step 4300: Loss = 0.805368\n",
      "Step 4400: Loss = 0.787201\n",
      "Step 4500: Loss = 0.773314\n",
      "Step 4600: Loss = 0.762557\n",
      "Step 4700: Loss = 0.754079\n",
      "Step 4800: Loss = 0.747256\n",
      "Step 4900: Loss = 0.741633\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "  Resampling ratio η = 0.9\n",
      "    Repeat 1/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 347.388275\n",
      "Step 100: Loss = 2.075561\n",
      "Step 200: Loss = 674.360107\n",
      "Step 300: Loss = 7.915110\n",
      "Step 400: Loss = 259.721588\n",
      "Step 500: Loss = 3.907648\n",
      "Step 600: Loss = 422.952789\n",
      "Step 700: Loss = 13.876390\n",
      "Step 800: Loss = 273.716217\n",
      "Step 900: Loss = 45.200493\n",
      "Step 1000: Loss = 602.606262\n",
      "Step 1100: Loss = 5.157468\n",
      "Step 1200: Loss = 339.911530\n",
      "Step 1300: Loss = 4.543228\n",
      "Step 1400: Loss = 611.379028\n",
      "Step 1500: Loss = 41.788860\n",
      "Step 1600: Loss = 823.469360\n",
      "Step 1700: Loss = 61.742481\n",
      "Step 1800: Loss = 464.977325\n",
      "Step 1900: Loss = 6.367519\n",
      "Step 2000: Loss = 224.062012\n",
      "Step 2100: Loss = 2.223558\n",
      "Step 2200: Loss = 767.709351\n",
      "Step 2300: Loss = 3.187348\n",
      "Step 2400: Loss = 214.350662\n",
      "Step 2500: Loss = 1.283626\n",
      "Step 2600: Loss = 706.463257\n",
      "Step 2700: Loss = 8.701120\n",
      "Step 2800: Loss = 543.364441\n",
      "Step 2900: Loss = 2.367787\n",
      "Step 3000: Loss = 876.391663\n",
      "Step 3100: Loss = 3.032125\n",
      "Step 3200: Loss = 387.768555\n",
      "Step 3300: Loss = 14.196014\n",
      "Step 3400: Loss = 349.461060\n",
      "Step 3500: Loss = 8.380068\n",
      "Step 3600: Loss = 168.630585\n",
      "Step 3700: Loss = 8.702027\n",
      "Step 3800: Loss = 256.590393\n",
      "Step 3900: Loss = 6.003105\n",
      "Step 4000: Loss = 3.065863\n",
      "Step 4100: Loss = 2.656543\n",
      "Step 4200: Loss = 2.302135\n",
      "Step 4300: Loss = 1.999284\n",
      "Step 4400: Loss = 1.744300\n",
      "Step 4500: Loss = 1.532831\n",
      "Step 4600: Loss = 1.359877\n",
      "Step 4700: Loss = 1.220070\n",
      "Step 4800: Loss = 1.108054\n",
      "Step 4900: Loss = 1.018829\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 2/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 189.783478\n",
      "Step 100: Loss = 1.060967\n",
      "Step 200: Loss = 367.435730\n",
      "Step 300: Loss = 14.339958\n",
      "Step 400: Loss = 318.396851\n",
      "Step 500: Loss = 7.784110\n",
      "Step 600: Loss = 526.139893\n",
      "Step 700: Loss = 7.552348\n",
      "Step 800: Loss = 250.425049\n",
      "Step 900: Loss = 72.063858\n",
      "Step 1000: Loss = 533.629761\n",
      "Step 1100: Loss = 66.080101\n",
      "Step 1200: Loss = 183.923630\n",
      "Step 1300: Loss = 1.163573\n",
      "Step 1400: Loss = 329.128815\n",
      "Step 1500: Loss = 13.261252\n",
      "Step 1600: Loss = 372.452515\n",
      "Step 1700: Loss = 11.081433\n",
      "Step 1800: Loss = 9966.926758\n",
      "Step 1900: Loss = 83.267326\n",
      "Step 2000: Loss = 547.489990\n",
      "Step 2100: Loss = 8.427489\n",
      "Step 2200: Loss = 504.028992\n",
      "Step 2300: Loss = 5.689895\n",
      "Step 2400: Loss = 330.327362\n",
      "Step 2500: Loss = 2.727077\n",
      "Step 2600: Loss = 3006.221680\n",
      "Step 2700: Loss = 54.555069\n",
      "Step 2800: Loss = 201.257126\n",
      "Step 2900: Loss = 32.262333\n",
      "Step 3000: Loss = 442.620148\n",
      "Step 3100: Loss = 35.846798\n",
      "Step 3200: Loss = 480.273010\n",
      "Step 3300: Loss = 30.294445\n",
      "Step 3400: Loss = 518.242126\n",
      "Step 3500: Loss = 4.955289\n",
      "Step 3600: Loss = 475.699738\n",
      "Step 3700: Loss = 24.250708\n",
      "Step 3800: Loss = 789.756287\n",
      "Step 3900: Loss = 66.363976\n",
      "Step 4000: Loss = 16.434832\n",
      "Step 4100: Loss = 3.785825\n",
      "Step 4200: Loss = 3.646925\n",
      "Step 4300: Loss = 3.518109\n",
      "Step 4400: Loss = 3.396875\n",
      "Step 4500: Loss = 3.284626\n",
      "Step 4600: Loss = 3.181673\n",
      "Step 4700: Loss = 3.087623\n",
      "Step 4800: Loss = 3.001704\n",
      "Step 4900: Loss = 2.922982\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 3/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 266.950104\n",
      "Step 100: Loss = 6.418666\n",
      "Step 200: Loss = 384.916321\n",
      "Step 300: Loss = 14.133173\n",
      "Step 400: Loss = 263.163147\n",
      "Step 500: Loss = 13.094733\n",
      "Step 600: Loss = 526.810791\n",
      "Step 700: Loss = 1.266283\n",
      "Step 800: Loss = 582.059937\n",
      "Step 900: Loss = 7.769711\n",
      "Step 1000: Loss = 176.930008\n",
      "Step 1100: Loss = 0.787805\n",
      "Step 1200: Loss = 268.759033\n",
      "Step 1300: Loss = 0.850960\n",
      "Step 1400: Loss = 496.508209\n",
      "Step 1500: Loss = 1.215956\n",
      "Step 1600: Loss = 468.708588\n",
      "Step 1700: Loss = 130.608566\n",
      "Step 1800: Loss = 440.027344\n",
      "Step 1900: Loss = 3.632043\n",
      "Step 2000: Loss = 435.497437\n",
      "Step 2100: Loss = 30.331646\n",
      "Step 2200: Loss = 876.220215\n",
      "Step 2300: Loss = 74.465286\n",
      "Step 2400: Loss = 696.749268\n",
      "Step 2500: Loss = 3.925541\n",
      "Step 2600: Loss = 350.301300\n",
      "Step 2700: Loss = 1.380939\n",
      "Step 2800: Loss = 551.388550\n",
      "Step 2900: Loss = 76.611015\n",
      "Step 3000: Loss = 593.313904\n",
      "Step 3100: Loss = 7.484043\n",
      "Step 3200: Loss = 460.824982\n",
      "Step 3300: Loss = 22.500286\n",
      "Step 3400: Loss = 499.862061\n",
      "Step 3500: Loss = 3.268285\n",
      "Step 3600: Loss = 310.260193\n",
      "Step 3700: Loss = 7.406157\n",
      "Step 3800: Loss = 292.709869\n",
      "Step 3900: Loss = 1.766884\n",
      "Step 4000: Loss = 0.721239\n",
      "Step 4100: Loss = 0.706150\n",
      "Step 4200: Loss = 0.693570\n",
      "Step 4300: Loss = 0.682803\n",
      "Step 4400: Loss = 0.673397\n",
      "Step 4500: Loss = 0.665053\n",
      "Step 4600: Loss = 0.657570\n",
      "Step 4700: Loss = 0.650806\n",
      "Step 4800: Loss = 0.644661\n",
      "Step 4900: Loss = 0.639058\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 4/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 112.777771\n",
      "Step 100: Loss = 2.151252\n",
      "Step 200: Loss = 492.634308\n",
      "Step 300: Loss = 4.266491\n",
      "Step 400: Loss = 683.463013\n",
      "Step 500: Loss = 48.314442\n",
      "Step 600: Loss = 328.467957\n",
      "Step 700: Loss = 1.234880\n",
      "Step 800: Loss = 320.883118\n",
      "Step 900: Loss = 13.956326\n",
      "Step 1000: Loss = 76.297424\n",
      "Step 1100: Loss = 0.720060\n",
      "Step 1200: Loss = 340.173431\n",
      "Step 1300: Loss = 7.555921\n",
      "Step 1400: Loss = 390.193848\n",
      "Step 1500: Loss = 0.948861\n",
      "Step 1600: Loss = 771.950317\n",
      "Step 1700: Loss = 16.312658\n",
      "Step 1800: Loss = 395.672241\n",
      "Step 1900: Loss = 0.838402\n",
      "Step 2000: Loss = 217.762848\n",
      "Step 2100: Loss = 12.179515\n",
      "Step 2200: Loss = 351.393646\n",
      "Step 2300: Loss = 81.132637\n",
      "Step 2400: Loss = 899.775330\n",
      "Step 2500: Loss = 80.290909\n",
      "Step 2600: Loss = 1231.069946\n",
      "Step 2700: Loss = 62.512115\n",
      "Step 2800: Loss = 196.622009\n",
      "Step 2900: Loss = 17.139637\n",
      "Step 3000: Loss = 517.582031\n",
      "Step 3100: Loss = 16.325994\n",
      "Step 3200: Loss = 288.778107\n",
      "Step 3300: Loss = 3.477811\n",
      "Step 3400: Loss = 605.847168\n",
      "Step 3500: Loss = 20.523102\n",
      "Step 3600: Loss = 201.637451\n",
      "Step 3700: Loss = 3.103110\n",
      "Step 3800: Loss = 608.285400\n",
      "Step 3900: Loss = 48.159519\n",
      "Step 4000: Loss = 42.666180\n",
      "Step 4100: Loss = 39.356445\n",
      "Step 4200: Loss = 16.612503\n",
      "Step 4300: Loss = 0.924039\n",
      "Step 4400: Loss = 0.686450\n",
      "Step 4500: Loss = 0.649049\n",
      "Step 4600: Loss = 0.640294\n",
      "Step 4700: Loss = 0.635113\n",
      "Step 4800: Loss = 0.630973\n",
      "Step 4900: Loss = 0.627463\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 5/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 306.517914\n",
      "Step 100: Loss = 1.515962\n",
      "Step 200: Loss = 462.156982\n",
      "Step 300: Loss = 23.534039\n",
      "Step 400: Loss = 155.664917\n",
      "Step 500: Loss = 15.938166\n",
      "Step 600: Loss = 795.780029\n",
      "Step 700: Loss = 27.546444\n",
      "Step 800: Loss = 1086.458984\n",
      "Step 900: Loss = 1.397184\n",
      "Step 1000: Loss = 211.672028\n",
      "Step 1100: Loss = 1.360735\n",
      "Step 1200: Loss = 559.760620\n",
      "Step 1300: Loss = 1.819184\n",
      "Step 1400: Loss = 426.292389\n",
      "Step 1500: Loss = 11.251516\n",
      "Step 1600: Loss = 1768.012817\n",
      "Step 1700: Loss = 77.946831\n",
      "Step 1800: Loss = 547.637390\n",
      "Step 1900: Loss = 17.259268\n",
      "Step 2000: Loss = 630.865234\n",
      "Step 2100: Loss = 22.133804\n",
      "Step 2200: Loss = 453.596680\n",
      "Step 2300: Loss = 57.304146\n",
      "Step 2400: Loss = 140.897751\n",
      "Step 2500: Loss = 36.961792\n",
      "Step 2600: Loss = 122.103859\n",
      "Step 2700: Loss = 1.307441\n",
      "Step 2800: Loss = 205.509338\n",
      "Step 2900: Loss = 2.673493\n",
      "Step 3000: Loss = 203.146179\n",
      "Step 3100: Loss = 52.753857\n",
      "Step 3200: Loss = 298.983307\n",
      "Step 3300: Loss = 2.025919\n",
      "Step 3400: Loss = 616.186218\n",
      "Step 3500: Loss = 2.755073\n",
      "Step 3600: Loss = 220.213699\n",
      "Step 3700: Loss = 10.317600\n",
      "Step 3800: Loss = 334.588165\n",
      "Step 3900: Loss = 12.502061\n",
      "Step 4000: Loss = 7.155469\n",
      "Step 4100: Loss = 1.196079\n",
      "Step 4200: Loss = 0.931169\n",
      "Step 4300: Loss = 0.877147\n",
      "Step 4400: Loss = 0.833967\n",
      "Step 4500: Loss = 0.799331\n",
      "Step 4600: Loss = 0.771390\n",
      "Step 4700: Loss = 0.748674\n",
      "Step 4800: Loss = 0.730035\n",
      "Step 4900: Loss = 0.714580\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "\n",
      "Running experiments with 20 residual points\n",
      "  Resampling ratio η = 0.0\n",
      "    Repeat 1/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 194.889786\n",
      "Step 100: Loss = 31.689718\n",
      "Step 200: Loss = 3.565395\n",
      "Step 300: Loss = 1.783680\n",
      "Step 400: Loss = 1.906765\n",
      "Step 500: Loss = 1.328210\n",
      "Step 600: Loss = 1.239069\n",
      "Step 700: Loss = 1.367257\n",
      "Step 800: Loss = 1.099811\n",
      "Step 900: Loss = 1.069262\n",
      "Step 1000: Loss = 0.995557\n",
      "Step 1100: Loss = 1.081316\n",
      "Step 1200: Loss = 0.916320\n",
      "Step 1300: Loss = 0.880634\n",
      "Step 1400: Loss = 0.858938\n",
      "Step 1500: Loss = 0.830949\n",
      "Step 1600: Loss = 0.808713\n",
      "Step 1700: Loss = 0.792152\n",
      "Step 1800: Loss = 0.773980\n",
      "Step 1900: Loss = 0.757811\n",
      "Step 2000: Loss = 0.748931\n",
      "Step 2100: Loss = 0.732461\n",
      "Step 2200: Loss = 0.719552\n",
      "Step 2300: Loss = 0.707541\n",
      "Step 2400: Loss = 0.696259\n",
      "Step 2500: Loss = 0.692198\n",
      "Step 2600: Loss = 0.677380\n",
      "Step 2700: Loss = 0.667868\n",
      "Step 2800: Loss = 0.658907\n",
      "Step 2900: Loss = 0.650448\n",
      "Step 3000: Loss = 0.642465\n",
      "Step 3100: Loss = 0.636362\n",
      "Step 3200: Loss = 0.628171\n",
      "Step 3300: Loss = 0.621614\n",
      "Step 3400: Loss = 0.703478\n",
      "Step 3500: Loss = 0.610160\n",
      "Step 3600: Loss = 0.604983\n",
      "Step 3700: Loss = 0.600198\n",
      "Step 3800: Loss = 0.595787\n",
      "Step 3900: Loss = 0.621716\n",
      "Step 4000: Loss = 0.588210\n",
      "Step 4100: Loss = 0.584884\n",
      "Step 4200: Loss = 0.581840\n",
      "Step 4300: Loss = 0.579057\n",
      "Step 4400: Loss = 0.576805\n",
      "Step 4500: Loss = 0.574261\n",
      "Step 4600: Loss = 0.572158\n",
      "Step 4700: Loss = 0.681021\n",
      "Step 4800: Loss = 0.568532\n",
      "Step 4900: Loss = 0.566925\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 2/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 143.518524\n",
      "Step 100: Loss = 30.881069\n",
      "Step 200: Loss = 2.081192\n",
      "Step 300: Loss = 3.546381\n",
      "Step 400: Loss = 1.168838\n",
      "Step 500: Loss = 6.142476\n",
      "Step 600: Loss = 0.974059\n",
      "Step 700: Loss = 0.919378\n",
      "Step 800: Loss = 0.878908\n",
      "Step 900: Loss = 1.473288\n",
      "Step 1000: Loss = 0.815604\n",
      "Step 1100: Loss = 0.790129\n",
      "Step 1200: Loss = 0.767314\n",
      "Step 1300: Loss = 0.798232\n",
      "Step 1400: Loss = 0.728214\n",
      "Step 1500: Loss = 0.711651\n",
      "Step 1600: Loss = 0.696489\n",
      "Step 1700: Loss = 0.682598\n",
      "Step 1800: Loss = 0.670501\n",
      "Step 1900: Loss = 0.658352\n",
      "Step 2000: Loss = 0.647573\n",
      "Step 2100: Loss = 0.761929\n",
      "Step 2200: Loss = 0.628835\n",
      "Step 2300: Loss = 0.620385\n",
      "Step 2400: Loss = 0.612471\n",
      "Step 2500: Loss = 0.605042\n",
      "Step 2600: Loss = 0.598541\n",
      "Step 2700: Loss = 0.592119\n",
      "Step 2800: Loss = 0.586121\n",
      "Step 2900: Loss = 0.580452\n",
      "Step 3000: Loss = 0.575573\n",
      "Step 3100: Loss = 0.570627\n",
      "Step 3200: Loss = 0.565966\n",
      "Step 3300: Loss = 0.561517\n",
      "Step 3400: Loss = 0.557868\n",
      "Step 3500: Loss = 0.553450\n",
      "Step 3600: Loss = 0.550909\n",
      "Step 3700: Loss = 0.546490\n",
      "Step 3800: Loss = 0.543118\n",
      "Step 3900: Loss = 0.539898\n",
      "Step 4000: Loss = 0.536988\n",
      "Step 4100: Loss = 0.534035\n",
      "Step 4200: Loss = 0.733818\n",
      "Step 4300: Loss = 0.528904\n",
      "Step 4400: Loss = 0.526408\n",
      "Step 4500: Loss = 0.523976\n",
      "Step 4600: Loss = 0.521689\n",
      "Step 4700: Loss = 0.519490\n",
      "Step 4800: Loss = 0.517293\n",
      "Step 4900: Loss = 0.516768\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 3/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 202.285095\n",
      "Step 100: Loss = 13.020448\n",
      "Step 200: Loss = 3.679602\n",
      "Step 300: Loss = 2.370495\n",
      "Step 400: Loss = 3.626533\n",
      "Step 500: Loss = 1.429762\n",
      "Step 600: Loss = 1.226949\n",
      "Step 700: Loss = 1.105050\n",
      "Step 800: Loss = 0.930480\n",
      "Step 900: Loss = 0.861438\n",
      "Step 1000: Loss = 0.817162\n",
      "Step 1100: Loss = 0.782979\n",
      "Step 1200: Loss = 0.817824\n",
      "Step 1300: Loss = 0.732100\n",
      "Step 1400: Loss = 0.712424\n",
      "Step 1500: Loss = 0.695572\n",
      "Step 1600: Loss = 0.681283\n",
      "Step 1700: Loss = 0.668757\n",
      "Step 1800: Loss = 0.657626\n",
      "Step 1900: Loss = 0.648529\n",
      "Step 2000: Loss = 0.639692\n",
      "Step 2100: Loss = 0.631628\n",
      "Step 2200: Loss = 0.892506\n",
      "Step 2300: Loss = 0.618702\n",
      "Step 2400: Loss = 0.612517\n",
      "Step 2500: Loss = 0.606700\n",
      "Step 2600: Loss = 0.601213\n",
      "Step 2700: Loss = 0.596653\n",
      "Step 2800: Loss = 0.591842\n",
      "Step 2900: Loss = 0.587354\n",
      "Step 3000: Loss = 0.583595\n",
      "Step 3100: Loss = 0.579597\n",
      "Step 3200: Loss = 0.577411\n",
      "Step 3300: Loss = 0.572435\n",
      "Step 3400: Loss = 0.569732\n",
      "Step 3500: Loss = 0.566096\n",
      "Step 3600: Loss = 0.568746\n",
      "Step 3700: Loss = 0.560336\n",
      "Step 3800: Loss = 0.580210\n",
      "Step 3900: Loss = 0.555358\n",
      "Step 4000: Loss = 0.552894\n",
      "Step 4100: Loss = 0.550515\n",
      "Step 4200: Loss = 0.548295\n",
      "Step 4300: Loss = 0.546191\n",
      "Step 4400: Loss = 0.544091\n",
      "Step 4500: Loss = 0.543554\n",
      "Step 4600: Loss = 0.540335\n",
      "Step 4700: Loss = 0.538477\n",
      "Step 4800: Loss = 0.536659\n",
      "Step 4900: Loss = 0.535031\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 4/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 201.467560\n",
      "Step 100: Loss = 25.193659\n",
      "Step 200: Loss = 1.761842\n",
      "Step 300: Loss = 1.015585\n",
      "Step 400: Loss = 0.989508\n",
      "Step 500: Loss = 0.776687\n",
      "Step 600: Loss = 0.744426\n",
      "Step 700: Loss = 0.694300\n",
      "Step 800: Loss = 0.692877\n",
      "Step 900: Loss = 0.667460\n",
      "Step 1000: Loss = 0.654547\n",
      "Step 1100: Loss = 0.704570\n",
      "Step 1200: Loss = 0.639936\n",
      "Step 1300: Loss = 0.631849\n",
      "Step 1400: Loss = 0.639162\n",
      "Step 1500: Loss = 0.621900\n",
      "Step 1600: Loss = 0.615773\n",
      "Step 1700: Loss = 0.617289\n",
      "Step 1800: Loss = 0.609659\n",
      "Step 1900: Loss = 0.604605\n",
      "Step 2000: Loss = 0.600089\n",
      "Step 2100: Loss = 0.595955\n",
      "Step 2200: Loss = 0.595287\n",
      "Step 2300: Loss = 0.590942\n",
      "Step 2400: Loss = 0.587517\n",
      "Step 2500: Loss = 0.584369\n",
      "Step 2600: Loss = 0.589127\n",
      "Step 2700: Loss = 0.579452\n",
      "Step 2800: Loss = 0.577139\n",
      "Step 2900: Loss = 0.575397\n",
      "Step 3000: Loss = 0.573018\n",
      "Step 3100: Loss = 0.570856\n",
      "Step 3200: Loss = 0.570491\n",
      "Step 3300: Loss = 0.567566\n",
      "Step 3400: Loss = 0.565737\n",
      "Step 3500: Loss = 0.564004\n",
      "Step 3600: Loss = 0.563071\n",
      "Step 3700: Loss = 0.561128\n",
      "Step 3800: Loss = 0.559583\n",
      "Step 3900: Loss = 0.558091\n",
      "Step 4000: Loss = 0.557078\n",
      "Step 4100: Loss = 0.555528\n",
      "Step 4200: Loss = 0.554392\n",
      "Step 4300: Loss = 0.553132\n",
      "Step 4400: Loss = 0.551948\n",
      "Step 4500: Loss = 0.589761\n",
      "Step 4600: Loss = 0.549785\n",
      "Step 4700: Loss = 0.548710\n",
      "Step 4800: Loss = 0.547658\n",
      "Step 4900: Loss = 0.546622\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 5/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 145.982544\n",
      "Step 100: Loss = 53.848713\n",
      "Step 200: Loss = 5.742415\n",
      "Step 300: Loss = 3.934330\n",
      "Step 400: Loss = 4.292020\n",
      "Step 500: Loss = 2.204348\n",
      "Step 600: Loss = 1.820694\n",
      "Step 700: Loss = 1.773410\n",
      "Step 800: Loss = 1.560610\n",
      "Step 900: Loss = 1.412888\n",
      "Step 1000: Loss = 1.312044\n",
      "Step 1100: Loss = 1.276700\n",
      "Step 1200: Loss = 1.299830\n",
      "Step 1300: Loss = 1.175491\n",
      "Step 1400: Loss = 1.213999\n",
      "Step 1500: Loss = 1.150849\n",
      "Step 1600: Loss = 4.348619\n",
      "Step 1700: Loss = 1.117262\n",
      "Step 1800: Loss = 1.122147\n",
      "Step 1900: Loss = 1.064889\n",
      "Step 2000: Loss = 1.054047\n",
      "Step 2100: Loss = 1.059217\n",
      "Step 2200: Loss = 1.023562\n",
      "Step 2300: Loss = 1.017571\n",
      "Step 2400: Loss = 1.629523\n",
      "Step 2500: Loss = 1.033812\n",
      "Step 2600: Loss = 1.005738\n",
      "Step 2700: Loss = 0.984925\n",
      "Step 2800: Loss = 0.966779\n",
      "Step 2900: Loss = 1.271609\n",
      "Step 3000: Loss = 0.957588\n",
      "Step 3100: Loss = 0.941482\n",
      "Step 3200: Loss = 0.927217\n",
      "Step 3300: Loss = 0.941936\n",
      "Step 3400: Loss = 0.911010\n",
      "Step 3500: Loss = 1.167023\n",
      "Step 3600: Loss = 0.902875\n",
      "Step 3700: Loss = 0.890900\n",
      "Step 3800: Loss = 0.880241\n",
      "Step 3900: Loss = 0.896441\n",
      "Step 4000: Loss = 0.874583\n",
      "Step 4100: Loss = 0.865138\n",
      "Step 4200: Loss = 0.856459\n",
      "Step 4300: Loss = 0.929747\n",
      "Step 4400: Loss = 0.846411\n",
      "Step 4500: Loss = 0.838671\n",
      "Step 4600: Loss = 0.982832\n",
      "Step 4700: Loss = 0.827873\n",
      "Step 4800: Loss = 0.821143\n",
      "Step 4900: Loss = 0.819357\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "  Resampling ratio η = 0.3\n",
      "    Repeat 1/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 327.153137\n",
      "Step 100: Loss = 104.162010\n",
      "Step 200: Loss = 149.124466\n",
      "Step 300: Loss = 2.461470\n",
      "Step 400: Loss = 48.931702\n",
      "Step 500: Loss = 3.090823\n",
      "Step 600: Loss = 138.452866\n",
      "Step 700: Loss = 5.157526\n",
      "Step 800: Loss = 187.200928\n",
      "Step 900: Loss = 13.852547\n",
      "Step 1000: Loss = 688.006897\n",
      "Step 1100: Loss = 4.735654\n",
      "Step 1200: Loss = 100.160919\n",
      "Step 1300: Loss = 3.641851\n",
      "Step 1400: Loss = 149.445038\n",
      "Step 1500: Loss = 4.074045\n",
      "Step 1600: Loss = 96.171669\n",
      "Step 1700: Loss = 4.175433\n",
      "Step 1800: Loss = 1406.959473\n",
      "Step 1900: Loss = 16.001511\n",
      "Step 2000: Loss = 255.232788\n",
      "Step 2100: Loss = 3.000413\n",
      "Step 2200: Loss = 84.459633\n",
      "Step 2300: Loss = 2.152667\n",
      "Step 2400: Loss = 134.614410\n",
      "Step 2500: Loss = 1.980415\n",
      "Step 2600: Loss = 99.668388\n",
      "Step 2700: Loss = 1.248398\n",
      "Step 2800: Loss = 159.176025\n",
      "Step 2900: Loss = 1.270947\n",
      "Step 3000: Loss = 130.306519\n",
      "Step 3100: Loss = 1.281407\n",
      "Step 3200: Loss = 108.857513\n",
      "Step 3300: Loss = 1.286973\n",
      "Step 3400: Loss = 217.886795\n",
      "Step 3500: Loss = 1.369255\n",
      "Step 3600: Loss = 76.852142\n",
      "Step 3700: Loss = 1.086595\n",
      "Step 3800: Loss = 61.165318\n",
      "Step 3900: Loss = 1.523369\n",
      "Step 4000: Loss = 1.104508\n",
      "Step 4100: Loss = 0.995635\n",
      "Step 4200: Loss = 0.937634\n",
      "Step 4300: Loss = 0.901304\n",
      "Step 4400: Loss = 0.875353\n",
      "Step 4500: Loss = 0.854728\n",
      "Step 4600: Loss = 0.837213\n",
      "Step 4700: Loss = 0.821822\n",
      "Step 4800: Loss = 0.808082\n",
      "Step 4900: Loss = 0.795722\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 2/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 276.757324\n",
      "Step 100: Loss = 31.562954\n",
      "Step 200: Loss = 759.531067\n",
      "Step 300: Loss = 24.147036\n",
      "Step 400: Loss = 45.555447\n",
      "Step 500: Loss = 5.239165\n",
      "Step 600: Loss = 486.760956\n",
      "Step 700: Loss = 5.192997\n",
      "Step 800: Loss = 98.956337\n",
      "Step 900: Loss = 3.588783\n",
      "Step 1000: Loss = 120.308243\n",
      "Step 1100: Loss = 3.735475\n",
      "Step 1200: Loss = 82.148926\n",
      "Step 1300: Loss = 2.873379\n",
      "Step 1400: Loss = 211.147690\n",
      "Step 1500: Loss = 3.117317\n",
      "Step 1600: Loss = 47.999737\n",
      "Step 1700: Loss = 2.331903\n",
      "Step 1800: Loss = 88.157486\n",
      "Step 1900: Loss = 2.166807\n",
      "Step 2000: Loss = 176.991455\n",
      "Step 2100: Loss = 6.025825\n",
      "Step 2200: Loss = 128.031372\n",
      "Step 2300: Loss = 2.438110\n",
      "Step 2400: Loss = 126.675400\n",
      "Step 2500: Loss = 2.056915\n",
      "Step 2600: Loss = 165.909546\n",
      "Step 2700: Loss = 11.920506\n",
      "Step 2800: Loss = 93.242195\n",
      "Step 2900: Loss = 1.600127\n",
      "Step 3000: Loss = 54.943077\n",
      "Step 3100: Loss = 1.989614\n",
      "Step 3200: Loss = 51.215073\n",
      "Step 3300: Loss = 0.959718\n",
      "Step 3400: Loss = 155.718857\n",
      "Step 3500: Loss = 0.835515\n",
      "Step 3600: Loss = 94.391670\n",
      "Step 3700: Loss = 0.669416\n",
      "Step 3800: Loss = 56.320148\n",
      "Step 3900: Loss = 0.732624\n",
      "Step 4000: Loss = 0.667624\n",
      "Step 4100: Loss = 0.657915\n",
      "Step 4200: Loss = 0.649946\n",
      "Step 4300: Loss = 0.643053\n",
      "Step 4400: Loss = 0.637004\n",
      "Step 4500: Loss = 0.631640\n",
      "Step 4600: Loss = 0.626839\n",
      "Step 4700: Loss = 0.622510\n",
      "Step 4800: Loss = 0.618579\n",
      "Step 4900: Loss = 0.614985\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 3/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 147.240799\n",
      "Step 100: Loss = 20.706081\n",
      "Step 200: Loss = 151.479294\n",
      "Step 300: Loss = 3.475577\n",
      "Step 400: Loss = 150.561569\n",
      "Step 500: Loss = 3.784257\n",
      "Step 600: Loss = 89.135094\n",
      "Step 700: Loss = 4.044529\n",
      "Step 800: Loss = 159.290405\n",
      "Step 900: Loss = 4.410139\n",
      "Step 1000: Loss = 121.197571\n",
      "Step 1100: Loss = 2.616274\n",
      "Step 1200: Loss = 53.106831\n",
      "Step 1300: Loss = 1.945078\n",
      "Step 1400: Loss = 234.980072\n",
      "Step 1500: Loss = 5.420218\n",
      "Step 1600: Loss = 135.086304\n",
      "Step 1700: Loss = 2.948559\n",
      "Step 1800: Loss = 49.657001\n",
      "Step 1900: Loss = 3.010546\n",
      "Step 2000: Loss = 178.812088\n",
      "Step 2100: Loss = 3.827254\n",
      "Step 2200: Loss = 950.006104\n",
      "Step 2300: Loss = 49.317276\n",
      "Step 2400: Loss = 101.958229\n",
      "Step 2500: Loss = 3.627606\n",
      "Step 2600: Loss = 41.861740\n",
      "Step 2700: Loss = 2.957352\n",
      "Step 2800: Loss = 253.260406\n",
      "Step 2900: Loss = 11.727038\n",
      "Step 3000: Loss = 227.073929\n",
      "Step 3100: Loss = 3.631851\n",
      "Step 3200: Loss = 114.135986\n",
      "Step 3300: Loss = 2.711777\n",
      "Step 3400: Loss = 49.496311\n",
      "Step 3500: Loss = 2.182430\n",
      "Step 3600: Loss = 171.288986\n",
      "Step 3700: Loss = 4.281411\n",
      "Step 3800: Loss = 142.564606\n",
      "Step 3900: Loss = 8.199946\n",
      "Step 4000: Loss = 2.623610\n",
      "Step 4100: Loss = 2.358476\n",
      "Step 4200: Loss = 2.227990\n",
      "Step 4300: Loss = 2.123939\n",
      "Step 4400: Loss = 2.034755\n",
      "Step 4500: Loss = 1.956581\n",
      "Step 4600: Loss = 1.887113\n",
      "Step 4700: Loss = 1.824694\n",
      "Step 4800: Loss = 1.768137\n",
      "Step 4900: Loss = 1.716623\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 4/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 235.106964\n",
      "Step 100: Loss = 59.615540\n",
      "Step 200: Loss = 412.850677\n",
      "Step 300: Loss = 13.741273\n",
      "Step 400: Loss = 58.975151\n",
      "Step 500: Loss = 8.773399\n",
      "Step 600: Loss = 718.530334\n",
      "Step 700: Loss = 45.563931\n",
      "Step 800: Loss = 47.183670\n",
      "Step 900: Loss = 11.821445\n",
      "Step 1000: Loss = 61.452957\n",
      "Step 1100: Loss = 2.193856\n",
      "Step 1200: Loss = 228.772705\n",
      "Step 1300: Loss = 4.488902\n",
      "Step 1400: Loss = 118.308380\n",
      "Step 1500: Loss = 1.879737\n",
      "Step 1600: Loss = 299.851532\n",
      "Step 1700: Loss = 1.710347\n",
      "Step 1800: Loss = 100.879097\n",
      "Step 1900: Loss = 5.342288\n",
      "Step 2000: Loss = 731.188477\n",
      "Step 2100: Loss = 9.728548\n",
      "Step 2200: Loss = 35.057137\n",
      "Step 2300: Loss = 5.119774\n",
      "Step 2400: Loss = 314.708496\n",
      "Step 2500: Loss = 4.133422\n",
      "Step 2600: Loss = 172.164551\n",
      "Step 2700: Loss = 17.635221\n",
      "Step 2800: Loss = 365.517792\n",
      "Step 2900: Loss = 2.201235\n",
      "Step 3000: Loss = 153.193375\n",
      "Step 3100: Loss = 2.921156\n",
      "Step 3200: Loss = 214.779816\n",
      "Step 3300: Loss = 1.823890\n",
      "Step 3400: Loss = 256.353943\n",
      "Step 3500: Loss = 1.872374\n",
      "Step 3600: Loss = 344.105835\n",
      "Step 3700: Loss = 1.324037\n",
      "Step 3800: Loss = 103.926163\n",
      "Step 3900: Loss = 3.305078\n",
      "Step 4000: Loss = 1.590545\n",
      "Step 4100: Loss = 1.190098\n",
      "Step 4200: Loss = 1.134728\n",
      "Step 4300: Loss = 1.107935\n",
      "Step 4400: Loss = 1.082567\n",
      "Step 4500: Loss = 1.057706\n",
      "Step 4600: Loss = 1.033431\n",
      "Step 4700: Loss = 1.009848\n",
      "Step 4800: Loss = 0.987055\n",
      "Step 4900: Loss = 0.965132\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 5/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 178.866226\n",
      "Step 100: Loss = 14.518955\n",
      "Step 200: Loss = 76.226273\n",
      "Step 300: Loss = 3.683235\n",
      "Step 400: Loss = 216.296692\n",
      "Step 500: Loss = 1.485864\n",
      "Step 600: Loss = 85.543045\n",
      "Step 700: Loss = 1.108835\n",
      "Step 800: Loss = 64.181511\n",
      "Step 900: Loss = 1.153284\n",
      "Step 1000: Loss = 460.383362\n",
      "Step 1100: Loss = 1.231385\n",
      "Step 1200: Loss = 49.956020\n",
      "Step 1300: Loss = 0.687234\n",
      "Step 1400: Loss = 140.100800\n",
      "Step 1500: Loss = 0.753289\n",
      "Step 1600: Loss = 141.604904\n",
      "Step 1700: Loss = 0.821327\n",
      "Step 1800: Loss = 356.671265\n",
      "Step 1900: Loss = 7.776811\n",
      "Step 2000: Loss = 54.707466\n",
      "Step 2100: Loss = 0.818777\n",
      "Step 2200: Loss = 64.915955\n",
      "Step 2300: Loss = 0.700327\n",
      "Step 2400: Loss = 80.640167\n",
      "Step 2500: Loss = 0.787580\n",
      "Step 2600: Loss = 104.445671\n",
      "Step 2700: Loss = 7.436219\n",
      "Step 2800: Loss = 202.655441\n",
      "Step 2900: Loss = 2.787491\n",
      "Step 3000: Loss = 266.207581\n",
      "Step 3100: Loss = 1.076827\n",
      "Step 3200: Loss = 204.518799\n",
      "Step 3300: Loss = 1.475694\n",
      "Step 3400: Loss = 217.533081\n",
      "Step 3500: Loss = 1.723694\n",
      "Step 3600: Loss = 104.484474\n",
      "Step 3700: Loss = 1.150399\n",
      "Step 3800: Loss = 132.076492\n",
      "Step 3900: Loss = 1.169716\n",
      "Step 4000: Loss = 1.092252\n",
      "Step 4100: Loss = 1.031377\n",
      "Step 4200: Loss = 0.980443\n",
      "Step 4300: Loss = 0.937638\n",
      "Step 4400: Loss = 0.901587\n",
      "Step 4500: Loss = 0.871177\n",
      "Step 4600: Loss = 0.845472\n",
      "Step 4700: Loss = 0.823679\n",
      "Step 4800: Loss = 0.805120\n",
      "Step 4900: Loss = 0.789222\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "  Resampling ratio η = 0.6\n",
      "    Repeat 1/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 376.905762\n",
      "Step 100: Loss = 9.519484\n",
      "Step 200: Loss = 266.687286\n",
      "Step 300: Loss = 5.362481\n",
      "Step 400: Loss = 160.712128\n",
      "Step 500: Loss = 1.725308\n",
      "Step 600: Loss = 305.284515\n",
      "Step 700: Loss = 29.155159\n",
      "Step 800: Loss = 542.074036\n",
      "Step 900: Loss = 7.434957\n",
      "Step 1000: Loss = 147.659683\n",
      "Step 1100: Loss = 1.637267\n",
      "Step 1200: Loss = 259.871307\n",
      "Step 1300: Loss = 1.870005\n",
      "Step 1400: Loss = 763.657776\n",
      "Step 1500: Loss = 5.033603\n",
      "Step 1600: Loss = 279.004333\n",
      "Step 1700: Loss = 6.836595\n",
      "Step 1800: Loss = 262.962402\n",
      "Step 1900: Loss = 1.090274\n",
      "Step 2000: Loss = 317.039978\n",
      "Step 2100: Loss = 1.804728\n",
      "Step 2200: Loss = 381.594971\n",
      "Step 2300: Loss = 1.515542\n",
      "Step 2400: Loss = 442.750488\n",
      "Step 2500: Loss = 7.975731\n",
      "Step 2600: Loss = 448.071777\n",
      "Step 2700: Loss = 65.413925\n",
      "Step 2800: Loss = 214.581085\n",
      "Step 2900: Loss = 7.738041\n",
      "Step 3000: Loss = 282.466339\n",
      "Step 3100: Loss = 1.556766\n",
      "Step 3200: Loss = 677.200378\n",
      "Step 3300: Loss = 6.943779\n",
      "Step 3400: Loss = 274.524506\n",
      "Step 3500: Loss = 0.858719\n",
      "Step 3600: Loss = 829.963196\n",
      "Step 3700: Loss = 20.287365\n",
      "Step 3800: Loss = 493.931671\n",
      "Step 3900: Loss = 1.618201\n",
      "Step 4000: Loss = 1.435505\n",
      "Step 4100: Loss = 1.297445\n",
      "Step 4200: Loss = 1.184612\n",
      "Step 4300: Loss = 1.092341\n",
      "Step 4400: Loss = 1.016600\n",
      "Step 4500: Loss = 0.954140\n",
      "Step 4600: Loss = 0.902385\n",
      "Step 4700: Loss = 0.859295\n",
      "Step 4800: Loss = 0.823257\n",
      "Step 4900: Loss = 0.792991\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 2/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 183.087952\n",
      "Step 100: Loss = 18.325417\n",
      "Step 200: Loss = 964.245483\n",
      "Step 300: Loss = 94.081627\n",
      "Step 400: Loss = 233.190994\n",
      "Step 500: Loss = 12.943258\n",
      "Step 600: Loss = 257.286346\n",
      "Step 700: Loss = 5.020760\n",
      "Step 800: Loss = 529.758728\n",
      "Step 900: Loss = 46.344063\n",
      "Step 1000: Loss = 134.704651\n",
      "Step 1100: Loss = 4.809282\n",
      "Step 1200: Loss = 303.830597\n",
      "Step 1300: Loss = 7.723454\n",
      "Step 1400: Loss = 238.414307\n",
      "Step 1500: Loss = 8.346104\n",
      "Step 1600: Loss = 494.628113\n",
      "Step 1700: Loss = 12.745450\n",
      "Step 1800: Loss = 309.427795\n",
      "Step 1900: Loss = 4.789178\n",
      "Step 2000: Loss = 309.731628\n",
      "Step 2100: Loss = 11.768120\n",
      "Step 2200: Loss = 371.166473\n",
      "Step 2300: Loss = 19.698170\n",
      "Step 2400: Loss = 790.788330\n",
      "Step 2500: Loss = 3.751593\n",
      "Step 2600: Loss = 231.349228\n",
      "Step 2700: Loss = 2.137242\n",
      "Step 2800: Loss = 489.948090\n",
      "Step 2900: Loss = 4.534575\n",
      "Step 3000: Loss = 304.990570\n",
      "Step 3100: Loss = 4.122050\n",
      "Step 3200: Loss = 281.052704\n",
      "Step 3300: Loss = 6.714078\n",
      "Step 3400: Loss = 222.743027\n",
      "Step 3500: Loss = 3.585441\n",
      "Step 3600: Loss = 362.827728\n",
      "Step 3700: Loss = 4.116019\n",
      "Step 3800: Loss = 404.611023\n",
      "Step 3900: Loss = 2.987560\n",
      "Step 4000: Loss = 2.759353\n",
      "Step 4100: Loss = 2.576005\n",
      "Step 4200: Loss = 2.419833\n",
      "Step 4300: Loss = 2.289344\n",
      "Step 4400: Loss = 2.181302\n",
      "Step 4500: Loss = 2.091830\n",
      "Step 4600: Loss = 2.017179\n",
      "Step 4700: Loss = 1.954102\n",
      "Step 4800: Loss = 1.899975\n",
      "Step 4900: Loss = 1.852752\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 3/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 235.064041\n",
      "Step 100: Loss = 22.279848\n",
      "Step 200: Loss = 318.828766\n",
      "Step 300: Loss = 8.097223\n",
      "Step 400: Loss = 104.578667\n",
      "Step 500: Loss = 3.901569\n",
      "Step 600: Loss = 758.686035\n",
      "Step 700: Loss = 7.297652\n",
      "Step 800: Loss = 314.029968\n",
      "Step 900: Loss = 5.979165\n",
      "Step 1000: Loss = 442.450470\n",
      "Step 1100: Loss = 24.572071\n",
      "Step 1200: Loss = 403.375061\n",
      "Step 1300: Loss = 3.508155\n",
      "Step 1400: Loss = 190.844086\n",
      "Step 1500: Loss = 3.017918\n",
      "Step 1600: Loss = 919.175964\n",
      "Step 1700: Loss = 20.661711\n",
      "Step 1800: Loss = 739.177490\n",
      "Step 1900: Loss = 26.906317\n",
      "Step 2000: Loss = 121.184166\n",
      "Step 2100: Loss = 42.048504\n",
      "Step 2200: Loss = 217.035522\n",
      "Step 2300: Loss = 18.246168\n",
      "Step 2400: Loss = 388.874664\n",
      "Step 2500: Loss = 9.255589\n",
      "Step 2600: Loss = 459.295929\n",
      "Step 2700: Loss = 27.845039\n",
      "Step 2800: Loss = 257.182709\n",
      "Step 2900: Loss = 37.420639\n",
      "Step 3000: Loss = 254.653839\n",
      "Step 3100: Loss = 16.857765\n",
      "Step 3200: Loss = 120.279625\n",
      "Step 3300: Loss = 17.933857\n",
      "Step 3400: Loss = 525.744507\n",
      "Step 3500: Loss = 36.938854\n",
      "Step 3600: Loss = 135.749664\n",
      "Step 3700: Loss = 5.946438\n",
      "Step 3800: Loss = 965.090576\n",
      "Step 3900: Loss = 31.562126\n",
      "Step 4000: Loss = 15.874562\n",
      "Step 4100: Loss = 6.730658\n",
      "Step 4200: Loss = 4.024205\n",
      "Step 4300: Loss = 3.363624\n",
      "Step 4400: Loss = 3.018093\n",
      "Step 4500: Loss = 2.746089\n",
      "Step 4600: Loss = 2.522543\n",
      "Step 4700: Loss = 2.335304\n",
      "Step 4800: Loss = 2.176593\n",
      "Step 4900: Loss = 2.040975\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 4/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 291.307922\n",
      "Step 100: Loss = 14.416032\n",
      "Step 200: Loss = 228.238861\n",
      "Step 300: Loss = 1.247177\n",
      "Step 400: Loss = 384.442780\n",
      "Step 500: Loss = 0.956898\n",
      "Step 600: Loss = 447.685944\n",
      "Step 700: Loss = 13.121165\n",
      "Step 800: Loss = 538.444458\n",
      "Step 900: Loss = 2.333462\n",
      "Step 1000: Loss = 120.554810\n",
      "Step 1100: Loss = 1.392330\n",
      "Step 1200: Loss = 645.095581\n",
      "Step 1300: Loss = 67.350075\n",
      "Step 1400: Loss = 482.191772\n",
      "Step 1500: Loss = 15.302957\n",
      "Step 1600: Loss = 1191.465454\n",
      "Step 1700: Loss = 131.422226\n",
      "Step 1800: Loss = 249.428711\n",
      "Step 1900: Loss = 38.520275\n",
      "Step 2000: Loss = 195.361481\n",
      "Step 2100: Loss = 9.704829\n",
      "Step 2200: Loss = 302.446655\n",
      "Step 2300: Loss = 34.030056\n",
      "Step 2400: Loss = 425.676270\n",
      "Step 2500: Loss = 33.863525\n",
      "Step 2600: Loss = 160.332138\n",
      "Step 2700: Loss = 42.566177\n",
      "Step 2800: Loss = 266.543732\n",
      "Step 2900: Loss = 4.402233\n",
      "Step 3000: Loss = 219.078766\n",
      "Step 3100: Loss = 2.450850\n",
      "Step 3200: Loss = 238.978302\n",
      "Step 3300: Loss = 37.388168\n",
      "Step 3400: Loss = 26.479956\n",
      "Step 3500: Loss = 4.841258\n",
      "Step 3600: Loss = 556.733337\n",
      "Step 3700: Loss = 20.305250\n",
      "Step 3800: Loss = 561.909607\n",
      "Step 3900: Loss = 5.909520\n",
      "Step 4000: Loss = 2.280835\n",
      "Step 4100: Loss = 2.036042\n",
      "Step 4200: Loss = 1.932475\n",
      "Step 4300: Loss = 1.844448\n",
      "Step 4400: Loss = 1.767516\n",
      "Step 4500: Loss = 1.699572\n",
      "Step 4600: Loss = 1.639037\n",
      "Step 4700: Loss = 1.584668\n",
      "Step 4800: Loss = 1.535482\n",
      "Step 4900: Loss = 1.490678\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 5/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 137.869843\n",
      "Step 100: Loss = 10.177416\n",
      "Step 200: Loss = 186.325424\n",
      "Step 300: Loss = 26.166477\n",
      "Step 400: Loss = 254.179916\n",
      "Step 500: Loss = 21.207127\n",
      "Step 600: Loss = 69.668419\n",
      "Step 700: Loss = 1.465678\n",
      "Step 800: Loss = 129.556000\n",
      "Step 900: Loss = 9.644187\n",
      "Step 1000: Loss = 297.506439\n",
      "Step 1100: Loss = 23.838461\n",
      "Step 1200: Loss = 99.726418\n",
      "Step 1300: Loss = 0.896275\n",
      "Step 1400: Loss = 154.718002\n",
      "Step 1500: Loss = 2.641421\n",
      "Step 1600: Loss = 361.390747\n",
      "Step 1700: Loss = 31.228397\n",
      "Step 1800: Loss = 383.401733\n",
      "Step 1900: Loss = 2.109637\n",
      "Step 2000: Loss = 301.219360\n",
      "Step 2100: Loss = 2.532876\n",
      "Step 2200: Loss = 205.887024\n",
      "Step 2300: Loss = 1.636772\n",
      "Step 2400: Loss = 190.846039\n",
      "Step 2500: Loss = 2.556303\n",
      "Step 2600: Loss = 833.135620\n",
      "Step 2700: Loss = 1.717447\n",
      "Step 2800: Loss = 295.509857\n",
      "Step 2900: Loss = 1.015255\n",
      "Step 3000: Loss = 195.894577\n",
      "Step 3100: Loss = 0.889807\n",
      "Step 3200: Loss = 380.418640\n",
      "Step 3300: Loss = 1.220684\n",
      "Step 3400: Loss = 538.054321\n",
      "Step 3500: Loss = 0.920556\n",
      "Step 3600: Loss = 251.834869\n",
      "Step 3700: Loss = 1.171542\n",
      "Step 3800: Loss = 146.372452\n",
      "Step 3900: Loss = 1.386806\n",
      "Step 4000: Loss = 0.924525\n",
      "Step 4100: Loss = 0.803982\n",
      "Step 4200: Loss = 0.744358\n",
      "Step 4300: Loss = 0.709066\n",
      "Step 4400: Loss = 0.685959\n",
      "Step 4500: Loss = 0.669743\n",
      "Step 4600: Loss = 0.657659\n",
      "Step 4700: Loss = 0.648161\n",
      "Step 4800: Loss = 0.640359\n",
      "Step 4900: Loss = 0.633728\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "  Resampling ratio η = 0.9\n",
      "    Repeat 1/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 206.338943\n",
      "Step 100: Loss = 2.381785\n",
      "Step 200: Loss = 397.729034\n",
      "Step 300: Loss = 6.554962\n",
      "Step 400: Loss = 445.502686\n",
      "Step 500: Loss = 39.027386\n",
      "Step 600: Loss = 263.187073\n",
      "Step 700: Loss = 43.588924\n",
      "Step 800: Loss = 2460.494873\n",
      "Step 900: Loss = 80.594688\n",
      "Step 1000: Loss = 193.658600\n",
      "Step 1100: Loss = 121.697289\n",
      "Step 1200: Loss = 683.526184\n",
      "Step 1300: Loss = 183.828979\n",
      "Step 1400: Loss = 266.061981\n",
      "Step 1500: Loss = 63.858368\n",
      "Step 1600: Loss = 295.399200\n",
      "Step 1700: Loss = 11.268622\n",
      "Step 1800: Loss = 429.870605\n",
      "Step 1900: Loss = 31.918312\n",
      "Step 2000: Loss = 785.885132\n",
      "Step 2100: Loss = 44.394077\n",
      "Step 2200: Loss = 374.746307\n",
      "Step 2300: Loss = 12.414253\n",
      "Step 2400: Loss = 212.479568\n",
      "Step 2500: Loss = 38.172810\n",
      "Step 2600: Loss = 579.495361\n",
      "Step 2700: Loss = 16.465450\n",
      "Step 2800: Loss = 298.972137\n",
      "Step 2900: Loss = 1.979025\n",
      "Step 3000: Loss = 657.662170\n",
      "Step 3100: Loss = 84.339478\n",
      "Step 3200: Loss = 403.028351\n",
      "Step 3300: Loss = 88.598045\n",
      "Step 3400: Loss = 400.027496\n",
      "Step 3500: Loss = 119.623993\n",
      "Step 3600: Loss = 386.801361\n",
      "Step 3700: Loss = 15.346247\n",
      "Step 3800: Loss = 341.840881\n",
      "Step 3900: Loss = 38.419205\n",
      "Step 4000: Loss = 19.691841\n",
      "Step 4100: Loss = 3.297721\n",
      "Step 4200: Loss = 2.956784\n",
      "Step 4300: Loss = 2.837973\n",
      "Step 4400: Loss = 2.743687\n",
      "Step 4500: Loss = 2.658945\n",
      "Step 4600: Loss = 2.581914\n",
      "Step 4700: Loss = 2.511770\n",
      "Step 4800: Loss = 2.447808\n",
      "Step 4900: Loss = 2.389364\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 2/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 271.109863\n",
      "Step 100: Loss = 22.453861\n",
      "Step 200: Loss = 776.202271\n",
      "Step 300: Loss = 59.621178\n",
      "Step 400: Loss = 606.775574\n",
      "Step 500: Loss = 10.107601\n",
      "Step 600: Loss = 739.232056\n",
      "Step 700: Loss = 54.560600\n",
      "Step 800: Loss = 267.689056\n",
      "Step 900: Loss = 26.973705\n",
      "Step 1000: Loss = 888.469604\n",
      "Step 1100: Loss = 21.656345\n",
      "Step 1200: Loss = 579.521545\n",
      "Step 1300: Loss = 7.099363\n",
      "Step 1400: Loss = 1136.366211\n",
      "Step 1500: Loss = 50.718231\n",
      "Step 1600: Loss = 793.418518\n",
      "Step 1700: Loss = 12.699298\n",
      "Step 1800: Loss = 453.428436\n",
      "Step 1900: Loss = 14.996408\n",
      "Step 2000: Loss = 275.837189\n",
      "Step 2100: Loss = 17.422123\n",
      "Step 2200: Loss = 240.571960\n",
      "Step 2300: Loss = 10.293572\n",
      "Step 2400: Loss = 590.304749\n",
      "Step 2500: Loss = 25.503603\n",
      "Step 2600: Loss = 928.013062\n",
      "Step 2700: Loss = 19.933964\n",
      "Step 2800: Loss = 558.890625\n",
      "Step 2900: Loss = 36.680378\n",
      "Step 3000: Loss = 356.344513\n",
      "Step 3100: Loss = 51.561867\n",
      "Step 3200: Loss = 263.583405\n",
      "Step 3300: Loss = 17.949608\n",
      "Step 3400: Loss = 406.032562\n",
      "Step 3500: Loss = 12.101768\n",
      "Step 3600: Loss = 418.525574\n",
      "Step 3700: Loss = 20.711636\n",
      "Step 3800: Loss = 183.805466\n",
      "Step 3900: Loss = 57.847599\n",
      "Step 4000: Loss = 6.600672\n",
      "Step 4100: Loss = 5.490255\n",
      "Step 4200: Loss = 5.187876\n",
      "Step 4300: Loss = 4.988324\n",
      "Step 4400: Loss = 4.819428\n",
      "Step 4500: Loss = 4.668612\n",
      "Step 4600: Loss = 4.532359\n",
      "Step 4700: Loss = 4.408357\n",
      "Step 4800: Loss = 4.294712\n",
      "Step 4900: Loss = 4.189907\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 3/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 134.263672\n",
      "Step 100: Loss = 10.561577\n",
      "Step 200: Loss = 246.269989\n",
      "Step 300: Loss = 2.623294\n",
      "Step 400: Loss = 346.669830\n",
      "Step 500: Loss = 3.597618\n",
      "Step 600: Loss = 367.187408\n",
      "Step 700: Loss = 1.151993\n",
      "Step 800: Loss = 363.781921\n",
      "Step 900: Loss = 5.883881\n",
      "Step 1000: Loss = 1125.309082\n",
      "Step 1100: Loss = 147.937653\n",
      "Step 1200: Loss = 219.004196\n",
      "Step 1300: Loss = 13.219549\n",
      "Step 1400: Loss = 160.771194\n",
      "Step 1500: Loss = 6.991156\n",
      "Step 1600: Loss = 529.898499\n",
      "Step 1700: Loss = 3.181526\n",
      "Step 1800: Loss = 503.218719\n",
      "Step 1900: Loss = 9.533422\n",
      "Step 2000: Loss = 762.574524\n",
      "Step 2100: Loss = 2.557858\n",
      "Step 2200: Loss = 752.964600\n",
      "Step 2300: Loss = 43.134548\n",
      "Step 2400: Loss = 487.903687\n",
      "Step 2500: Loss = 10.795362\n",
      "Step 2600: Loss = 342.908783\n",
      "Step 2700: Loss = 11.416661\n",
      "Step 2800: Loss = 286.397858\n",
      "Step 2900: Loss = 2.757191\n",
      "Step 3000: Loss = 1058.348877\n",
      "Step 3100: Loss = 46.865746\n",
      "Step 3200: Loss = 629.760315\n",
      "Step 3300: Loss = 13.579279\n",
      "Step 3400: Loss = 971.129578\n",
      "Step 3500: Loss = 45.945545\n",
      "Step 3600: Loss = 311.558350\n",
      "Step 3700: Loss = 5.433596\n",
      "Step 3800: Loss = 1009.190002\n",
      "Step 3900: Loss = 36.512901\n",
      "Step 4000: Loss = 15.845100\n",
      "Step 4100: Loss = 1.904469\n",
      "Step 4200: Loss = 0.971648\n",
      "Step 4300: Loss = 0.889204\n",
      "Step 4400: Loss = 0.852969\n",
      "Step 4500: Loss = 0.826768\n",
      "Step 4600: Loss = 0.804271\n",
      "Step 4700: Loss = 0.784363\n",
      "Step 4800: Loss = 0.766651\n",
      "Step 4900: Loss = 0.750855\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 4/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 184.876541\n",
      "Step 100: Loss = 7.293082\n",
      "Step 200: Loss = 285.442230\n",
      "Step 300: Loss = 11.171349\n",
      "Step 400: Loss = 492.742889\n",
      "Step 500: Loss = 8.834375\n",
      "Step 600: Loss = 836.067444\n",
      "Step 700: Loss = 93.791588\n",
      "Step 800: Loss = 374.670685\n",
      "Step 900: Loss = 31.391296\n",
      "Step 1000: Loss = 746.929993\n",
      "Step 1100: Loss = 35.250862\n",
      "Step 1200: Loss = 178.835892\n",
      "Step 1300: Loss = 19.934916\n",
      "Step 1400: Loss = 1224.512817\n",
      "Step 1500: Loss = 126.307899\n",
      "Step 1600: Loss = 190.066010\n",
      "Step 1700: Loss = 62.349949\n",
      "Step 1800: Loss = 269.068939\n",
      "Step 1900: Loss = 106.807030\n",
      "Step 2000: Loss = 441.962189\n",
      "Step 2100: Loss = 34.651783\n",
      "Step 2200: Loss = 580.316589\n",
      "Step 2300: Loss = 45.128841\n",
      "Step 2400: Loss = 898.925476\n",
      "Step 2500: Loss = 72.160797\n",
      "Step 2600: Loss = 187.048325\n",
      "Step 2700: Loss = 142.447281\n",
      "Step 2800: Loss = 226.279419\n",
      "Step 2900: Loss = 80.956726\n",
      "Step 3000: Loss = 346.856506\n",
      "Step 3100: Loss = 97.046074\n",
      "Step 3200: Loss = 633.981750\n",
      "Step 3300: Loss = 15.065668\n",
      "Step 3400: Loss = 1109.286743\n",
      "Step 3500: Loss = 47.499313\n",
      "Step 3600: Loss = 345.466492\n",
      "Step 3700: Loss = 18.848389\n",
      "Step 3800: Loss = 471.343201\n",
      "Step 3900: Loss = 96.512848\n",
      "Step 4000: Loss = 30.813606\n",
      "Step 4100: Loss = 2.893331\n",
      "Step 4200: Loss = 2.207138\n",
      "Step 4300: Loss = 1.815103\n",
      "Step 4400: Loss = 1.552265\n",
      "Step 4500: Loss = 1.359907\n",
      "Step 4600: Loss = 1.212677\n",
      "Step 4700: Loss = 1.098104\n",
      "Step 4800: Loss = 1.008635\n",
      "Step 4900: Loss = 0.938797\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 5/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 276.678436\n",
      "Step 100: Loss = 8.336534\n",
      "Step 200: Loss = 624.068665\n",
      "Step 300: Loss = 64.899223\n",
      "Step 400: Loss = 264.074341\n",
      "Step 500: Loss = 4.802734\n",
      "Step 600: Loss = 581.210876\n",
      "Step 700: Loss = 1.447559\n",
      "Step 800: Loss = 424.705566\n",
      "Step 900: Loss = 14.170969\n",
      "Step 1000: Loss = 280.163269\n",
      "Step 1100: Loss = 19.355740\n",
      "Step 1200: Loss = 885.567078\n",
      "Step 1300: Loss = 71.833260\n",
      "Step 1400: Loss = 720.942871\n",
      "Step 1500: Loss = 4.675591\n",
      "Step 1600: Loss = 433.465881\n",
      "Step 1700: Loss = 5.293416\n",
      "Step 1800: Loss = 598.563843\n",
      "Step 1900: Loss = 2.366558\n",
      "Step 2000: Loss = 231.125931\n",
      "Step 2100: Loss = 1.755138\n",
      "Step 2200: Loss = 247.879822\n",
      "Step 2300: Loss = 3.215821\n",
      "Step 2400: Loss = 221.277908\n",
      "Step 2500: Loss = 6.335711\n",
      "Step 2600: Loss = 811.084961\n",
      "Step 2700: Loss = 41.285896\n",
      "Step 2800: Loss = 837.006775\n",
      "Step 2900: Loss = 2.044191\n",
      "Step 3000: Loss = 151.558960\n",
      "Step 3100: Loss = 1.383817\n",
      "Step 3200: Loss = 315.023651\n",
      "Step 3300: Loss = 25.942842\n",
      "Step 3400: Loss = 340.735901\n",
      "Step 3500: Loss = 1.560000\n",
      "Step 3600: Loss = 364.968536\n",
      "Step 3700: Loss = 3.679372\n",
      "Step 3800: Loss = 773.139893\n",
      "Step 3900: Loss = 6.316957\n",
      "Step 4000: Loss = 4.384535\n",
      "Step 4100: Loss = 3.662460\n",
      "Step 4200: Loss = 3.201268\n",
      "Step 4300: Loss = 2.883048\n",
      "Step 4400: Loss = 2.666395\n",
      "Step 4500: Loss = 2.522581\n",
      "Step 4600: Loss = 2.427114\n",
      "Step 4700: Loss = 2.361340\n",
      "Step 4800: Loss = 2.312620\n",
      "Step 4900: Loss = 2.273197\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "\n",
      "Running experiments with 30 residual points\n",
      "  Resampling ratio η = 0.0\n",
      "    Repeat 1/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 182.741440\n",
      "Step 100: Loss = 11.520515\n",
      "Step 200: Loss = 2.523748\n",
      "Step 300: Loss = 1.785032\n",
      "Step 400: Loss = 1.630801\n",
      "Step 500: Loss = 1.551502\n",
      "Step 600: Loss = 1.392540\n",
      "Step 700: Loss = 1.375389\n",
      "Step 800: Loss = 1.250111\n",
      "Step 900: Loss = 1.410868\n",
      "Step 1000: Loss = 1.178252\n",
      "Step 1100: Loss = 1.108758\n",
      "Step 1200: Loss = 1.160813\n",
      "Step 1300: Loss = 1.080552\n",
      "Step 1400: Loss = 1.032557\n",
      "Step 1500: Loss = 1.272029\n",
      "Step 1600: Loss = 1.011754\n",
      "Step 1700: Loss = 0.976117\n",
      "Step 1800: Loss = 0.946114\n",
      "Step 1900: Loss = 0.970916\n",
      "Step 2000: Loss = 0.936428\n",
      "Step 2100: Loss = 0.912326\n",
      "Step 2200: Loss = 0.891212\n",
      "Step 2300: Loss = 0.885834\n",
      "Step 2400: Loss = 0.866912\n",
      "Step 2500: Loss = 0.875204\n",
      "Step 2600: Loss = 0.855047\n",
      "Step 2700: Loss = 0.840320\n",
      "Step 2800: Loss = 0.826898\n",
      "Step 2900: Loss = 0.828091\n",
      "Step 3000: Loss = 0.814283\n",
      "Step 3100: Loss = 0.803151\n",
      "Step 3200: Loss = 0.831303\n",
      "Step 3300: Loss = 0.789525\n",
      "Step 3400: Loss = 0.780192\n",
      "Step 3500: Loss = 0.783356\n",
      "Step 3600: Loss = 0.770760\n",
      "Step 3700: Loss = 0.763035\n",
      "Step 3800: Loss = 0.755650\n",
      "Step 3900: Loss = 0.754267\n",
      "Step 4000: Loss = 0.746620\n",
      "Step 4100: Loss = 0.740332\n",
      "Step 4200: Loss = 0.754831\n",
      "Step 4300: Loss = 0.730574\n",
      "Step 4400: Loss = 0.790634\n",
      "Step 4500: Loss = 0.722038\n",
      "Step 4600: Loss = 0.717078\n",
      "Step 4700: Loss = 0.714334\n",
      "Step 4800: Loss = 0.709871\n",
      "Step 4900: Loss = 0.705546\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 2/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 168.302490\n",
      "Step 100: Loss = 32.471043\n",
      "Step 200: Loss = 10.812481\n",
      "Step 300: Loss = 6.480822\n",
      "Step 400: Loss = 5.772590\n",
      "Step 500: Loss = 5.280275\n",
      "Step 600: Loss = 4.932918\n",
      "Step 700: Loss = 4.665330\n",
      "Step 800: Loss = 4.422996\n",
      "Step 900: Loss = 4.173664\n",
      "Step 1000: Loss = 6.158118\n",
      "Step 1100: Loss = 3.791930\n",
      "Step 1200: Loss = 3.701406\n",
      "Step 1300: Loss = 4.192158\n",
      "Step 1400: Loss = 3.331579\n",
      "Step 1500: Loss = 3.343470\n",
      "Step 1600: Loss = 3.125222\n",
      "Step 1700: Loss = 3.381781\n",
      "Step 1800: Loss = 3.039245\n",
      "Step 1900: Loss = 2.891690\n",
      "Step 2000: Loss = 3.058681\n",
      "Step 2100: Loss = 2.838876\n",
      "Step 2200: Loss = 2.724563\n",
      "Step 2300: Loss = 5.991449\n",
      "Step 2400: Loss = 2.660034\n",
      "Step 2500: Loss = 2.571092\n",
      "Step 2600: Loss = 2.636013\n",
      "Step 2700: Loss = 2.500504\n",
      "Step 2800: Loss = 2.453266\n",
      "Step 2900: Loss = 2.438195\n",
      "Step 3000: Loss = 2.374260\n",
      "Step 3100: Loss = 2.420984\n",
      "Step 3200: Loss = 2.331927\n",
      "Step 3300: Loss = 3.058575\n",
      "Step 3400: Loss = 2.296752\n",
      "Step 3500: Loss = 2.249568\n",
      "Step 3600: Loss = 2.206505\n",
      "Step 3700: Loss = 2.213810\n",
      "Step 3800: Loss = 2.173084\n",
      "Step 3900: Loss = 2.137017\n",
      "Step 4000: Loss = 2.143125\n",
      "Step 4100: Loss = 2.110533\n",
      "Step 4200: Loss = 2.081128\n",
      "Step 4300: Loss = 2.080946\n",
      "Step 4400: Loss = 2.054529\n",
      "Step 4500: Loss = 2.290164\n",
      "Step 4600: Loss = 2.018823\n",
      "Step 4700: Loss = 2.946824\n",
      "Step 4800: Loss = 1.999218\n",
      "Step 4900: Loss = 1.980310\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 3/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 208.997620\n",
      "Step 100: Loss = 33.239681\n",
      "Step 200: Loss = 20.664764\n",
      "Step 300: Loss = 10.861401\n",
      "Step 400: Loss = 8.767475\n",
      "Step 500: Loss = 7.553219\n",
      "Step 600: Loss = 13.601227\n",
      "Step 700: Loss = 7.243563\n",
      "Step 800: Loss = 5.296508\n",
      "Step 900: Loss = 5.189251\n",
      "Step 1000: Loss = 13.321489\n",
      "Step 1100: Loss = 4.537905\n",
      "Step 1200: Loss = 4.893826\n",
      "Step 1300: Loss = 4.178234\n",
      "Step 1400: Loss = 3.851579\n",
      "Step 1500: Loss = 4.245503\n",
      "Step 1600: Loss = 3.891348\n",
      "Step 1700: Loss = 3.640307\n",
      "Step 1800: Loss = 4.046811\n",
      "Step 1900: Loss = 3.399797\n",
      "Step 2000: Loss = 5.254864\n",
      "Step 2100: Loss = 3.327793\n",
      "Step 2200: Loss = 3.187885\n",
      "Step 2300: Loss = 3.073749\n",
      "Step 2400: Loss = 3.062303\n",
      "Step 2500: Loss = 2.959982\n",
      "Step 2600: Loss = 2.945549\n",
      "Step 2700: Loss = 2.866710\n",
      "Step 2800: Loss = 2.857460\n",
      "Step 2900: Loss = 2.772395\n",
      "Step 3000: Loss = 2.750087\n",
      "Step 3100: Loss = 2.700486\n",
      "Step 3200: Loss = 2.665180\n",
      "Step 3300: Loss = 2.643444\n",
      "Step 3400: Loss = 2.597660\n",
      "Step 3500: Loss = 2.587149\n",
      "Step 3600: Loss = 3.500865\n",
      "Step 3700: Loss = 2.521360\n",
      "Step 3800: Loss = 2.488025\n",
      "Step 3900: Loss = 2.470371\n",
      "Step 4000: Loss = 2.435867\n",
      "Step 4100: Loss = 2.421698\n",
      "Step 4200: Loss = 2.395099\n",
      "Step 4300: Loss = 2.391945\n",
      "Step 4400: Loss = 2.355339\n",
      "Step 4500: Loss = 2.343714\n",
      "Step 4600: Loss = 2.322181\n",
      "Step 4700: Loss = 2.642269\n",
      "Step 4800: Loss = 2.286758\n",
      "Step 4900: Loss = 2.274522\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 4/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 241.999756\n",
      "Step 100: Loss = 32.753880\n",
      "Step 200: Loss = 5.962348\n",
      "Step 300: Loss = 2.518922\n",
      "Step 400: Loss = 1.147127\n",
      "Step 500: Loss = 0.825719\n",
      "Step 600: Loss = 0.760687\n",
      "Step 700: Loss = 0.725067\n",
      "Step 800: Loss = 0.713303\n",
      "Step 900: Loss = 0.677830\n",
      "Step 1000: Loss = 0.685622\n",
      "Step 1100: Loss = 0.644433\n",
      "Step 1200: Loss = 0.633835\n",
      "Step 1300: Loss = 0.618716\n",
      "Step 1400: Loss = 0.608890\n",
      "Step 1500: Loss = 0.599373\n",
      "Step 1600: Loss = 0.591598\n",
      "Step 1700: Loss = 0.584785\n",
      "Step 1800: Loss = 0.578742\n",
      "Step 1900: Loss = 0.581253\n",
      "Step 2000: Loss = 0.568340\n",
      "Step 2100: Loss = 0.564040\n",
      "Step 2200: Loss = 0.560133\n",
      "Step 2300: Loss = 0.556553\n",
      "Step 2400: Loss = 0.553250\n",
      "Step 2500: Loss = 0.640404\n",
      "Step 2600: Loss = 0.547245\n",
      "Step 2700: Loss = 0.544568\n",
      "Step 2800: Loss = 0.542033\n",
      "Step 2900: Loss = 0.539616\n",
      "Step 3000: Loss = 0.537328\n",
      "Step 3100: Loss = 0.535021\n",
      "Step 3200: Loss = 0.532918\n",
      "Step 3300: Loss = 0.530893\n",
      "Step 3400: Loss = 0.528927\n",
      "Step 3500: Loss = 0.527013\n",
      "Step 3600: Loss = 0.525144\n",
      "Step 3700: Loss = 0.523317\n",
      "Step 3800: Loss = 0.521529\n",
      "Step 3900: Loss = 0.519784\n",
      "Step 4000: Loss = 0.518088\n",
      "Step 4100: Loss = 0.516392\n",
      "Step 4200: Loss = 0.514766\n",
      "Step 4300: Loss = 0.513165\n",
      "Step 4400: Loss = 0.511589\n",
      "Step 4500: Loss = 0.510460\n",
      "Step 4600: Loss = 0.508819\n",
      "Step 4700: Loss = 0.507041\n",
      "Step 4800: Loss = 0.505596\n",
      "Step 4900: Loss = 0.504167\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 5/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 185.714615\n",
      "Step 100: Loss = 56.351799\n",
      "Step 200: Loss = 22.282909\n",
      "Step 300: Loss = 5.881941\n",
      "Step 400: Loss = 2.525427\n",
      "Step 500: Loss = 2.042637\n",
      "Step 600: Loss = 1.817053\n",
      "Step 700: Loss = 1.684049\n",
      "Step 800: Loss = 2.014519\n",
      "Step 900: Loss = 1.456525\n",
      "Step 1000: Loss = 1.342331\n",
      "Step 1100: Loss = 1.267745\n",
      "Step 1200: Loss = 3.011482\n",
      "Step 1300: Loss = 1.160495\n",
      "Step 1400: Loss = 1.733977\n",
      "Step 1500: Loss = 1.102329\n",
      "Step 1600: Loss = 1.071903\n",
      "Step 1700: Loss = 1.067403\n",
      "Step 1800: Loss = 1.038731\n",
      "Step 1900: Loss = 1.166428\n",
      "Step 2000: Loss = 1.011369\n",
      "Step 2100: Loss = 0.996088\n",
      "Step 2200: Loss = 0.993051\n",
      "Step 2300: Loss = 0.978277\n",
      "Step 2400: Loss = 0.966655\n",
      "Step 2500: Loss = 1.069907\n",
      "Step 2600: Loss = 0.951221\n",
      "Step 2700: Loss = 0.942121\n",
      "Step 2800: Loss = 0.953251\n",
      "Step 2900: Loss = 0.927822\n",
      "Step 3000: Loss = 0.926149\n",
      "Step 3100: Loss = 0.918377\n",
      "Step 3200: Loss = 0.912088\n",
      "Step 3300: Loss = 0.906236\n",
      "Step 3400: Loss = 0.900721\n",
      "Step 3500: Loss = 0.945974\n",
      "Step 3600: Loss = 0.892081\n",
      "Step 3700: Loss = 0.887373\n",
      "Step 3800: Loss = 0.882858\n",
      "Step 3900: Loss = 0.894005\n",
      "Step 4000: Loss = 0.875317\n",
      "Step 4100: Loss = 0.871320\n",
      "Step 4200: Loss = 0.867427\n",
      "Step 4300: Loss = 0.909011\n",
      "Step 4400: Loss = 0.860490\n",
      "Step 4500: Loss = 0.856913\n",
      "Step 4600: Loss = 0.853383\n",
      "Step 4700: Loss = 0.850906\n",
      "Step 4800: Loss = 0.847145\n",
      "Step 4900: Loss = 0.843880\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "  Resampling ratio η = 0.3\n",
      "    Repeat 1/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 245.617233\n",
      "Step 100: Loss = 50.754780\n",
      "Step 200: Loss = 120.597595\n",
      "Step 300: Loss = 5.453589\n",
      "Step 400: Loss = 111.277191\n",
      "Step 500: Loss = 1.329667\n",
      "Step 600: Loss = 163.982758\n",
      "Step 700: Loss = 11.020319\n",
      "Step 800: Loss = 153.510498\n",
      "Step 900: Loss = 6.430721\n",
      "Step 1000: Loss = 143.568527\n",
      "Step 1100: Loss = 0.730407\n",
      "Step 1200: Loss = 184.940155\n",
      "Step 1300: Loss = 0.882338\n",
      "Step 1400: Loss = 217.701645\n",
      "Step 1500: Loss = 0.711743\n",
      "Step 1600: Loss = 128.336090\n",
      "Step 1700: Loss = 0.763172\n",
      "Step 1800: Loss = 108.216911\n",
      "Step 1900: Loss = 0.675833\n",
      "Step 2000: Loss = 124.740501\n",
      "Step 2100: Loss = 0.630390\n",
      "Step 2200: Loss = 95.191147\n",
      "Step 2300: Loss = 0.740271\n",
      "Step 2400: Loss = 88.433586\n",
      "Step 2500: Loss = 0.986930\n",
      "Step 2600: Loss = 96.680000\n",
      "Step 2700: Loss = 1.613909\n",
      "Step 2800: Loss = 265.590637\n",
      "Step 2900: Loss = 28.406902\n",
      "Step 3000: Loss = 79.466278\n",
      "Step 3100: Loss = 1.384183\n",
      "Step 3200: Loss = 115.847427\n",
      "Step 3300: Loss = 2.943688\n",
      "Step 3400: Loss = 279.719025\n",
      "Step 3500: Loss = 3.122931\n",
      "Step 3600: Loss = 195.208145\n",
      "Step 3700: Loss = 6.431435\n",
      "Step 3800: Loss = 191.061859\n",
      "Step 3900: Loss = 2.557214\n",
      "Step 4000: Loss = 1.147375\n",
      "Step 4100: Loss = 0.704652\n",
      "Step 4200: Loss = 0.634844\n",
      "Step 4300: Loss = 0.617905\n",
      "Step 4400: Loss = 0.611058\n",
      "Step 4500: Loss = 0.607564\n",
      "Step 4600: Loss = 0.605280\n",
      "Step 4700: Loss = 0.603449\n",
      "Step 4800: Loss = 0.601802\n",
      "Step 4900: Loss = 0.600242\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 2/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 257.301666\n",
      "Step 100: Loss = 26.601789\n",
      "Step 200: Loss = 110.514603\n",
      "Step 300: Loss = 4.664838\n",
      "Step 400: Loss = 100.681763\n",
      "Step 500: Loss = 4.049910\n",
      "Step 600: Loss = 70.909988\n",
      "Step 700: Loss = 4.448446\n",
      "Step 800: Loss = 390.992950\n",
      "Step 900: Loss = 17.296247\n",
      "Step 1000: Loss = 144.439407\n",
      "Step 1100: Loss = 10.095607\n",
      "Step 1200: Loss = 152.165039\n",
      "Step 1300: Loss = 5.196182\n",
      "Step 1400: Loss = 120.082298\n",
      "Step 1500: Loss = 3.988121\n",
      "Step 1600: Loss = 246.949692\n",
      "Step 1700: Loss = 3.881531\n",
      "Step 1800: Loss = 176.321045\n",
      "Step 1900: Loss = 2.717512\n",
      "Step 2000: Loss = 92.350807\n",
      "Step 2100: Loss = 2.428659\n",
      "Step 2200: Loss = 354.480682\n",
      "Step 2300: Loss = 3.373635\n",
      "Step 2400: Loss = 142.970520\n",
      "Step 2500: Loss = 3.954440\n",
      "Step 2600: Loss = 416.324829\n",
      "Step 2700: Loss = 10.794428\n",
      "Step 2800: Loss = 174.302139\n",
      "Step 2900: Loss = 7.121810\n",
      "Step 3000: Loss = 188.675354\n",
      "Step 3100: Loss = 7.949980\n",
      "Step 3200: Loss = 147.501678\n",
      "Step 3300: Loss = 8.123053\n",
      "Step 3400: Loss = 117.430801\n",
      "Step 3500: Loss = 9.696564\n",
      "Step 3600: Loss = 143.974625\n",
      "Step 3700: Loss = 3.845923\n",
      "Step 3800: Loss = 225.951813\n",
      "Step 3900: Loss = 4.990136\n",
      "Step 4000: Loss = 4.453588\n",
      "Step 4100: Loss = 4.162782\n",
      "Step 4200: Loss = 3.941917\n",
      "Step 4300: Loss = 3.761575\n",
      "Step 4400: Loss = 3.608572\n",
      "Step 4500: Loss = 3.475322\n",
      "Step 4600: Loss = 3.356739\n",
      "Step 4700: Loss = 3.249107\n",
      "Step 4800: Loss = 3.149557\n",
      "Step 4900: Loss = 3.055763\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 3/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 115.845139\n",
      "Step 100: Loss = 30.336290\n",
      "Step 200: Loss = 536.558167\n",
      "Step 300: Loss = 31.903961\n",
      "Step 400: Loss = 94.580727\n",
      "Step 500: Loss = 2.469087\n",
      "Step 600: Loss = 194.733292\n",
      "Step 700: Loss = 1.285760\n",
      "Step 800: Loss = 89.532906\n",
      "Step 900: Loss = 0.974150\n",
      "Step 1000: Loss = 115.201370\n",
      "Step 1100: Loss = 0.857685\n",
      "Step 1200: Loss = 200.054916\n",
      "Step 1300: Loss = 4.185562\n",
      "Step 1400: Loss = 76.288315\n",
      "Step 1500: Loss = 0.727313\n",
      "Step 1600: Loss = 234.320953\n",
      "Step 1700: Loss = 0.664348\n",
      "Step 1800: Loss = 130.623276\n",
      "Step 1900: Loss = 0.907990\n",
      "Step 2000: Loss = 177.141220\n",
      "Step 2100: Loss = 0.989299\n",
      "Step 2200: Loss = 127.803696\n",
      "Step 2300: Loss = 1.152680\n",
      "Step 2400: Loss = 139.656982\n",
      "Step 2500: Loss = 0.863406\n",
      "Step 2600: Loss = 63.866112\n",
      "Step 2700: Loss = 1.234458\n",
      "Step 2800: Loss = 160.189789\n",
      "Step 2900: Loss = 4.830973\n",
      "Step 3000: Loss = 318.732422\n",
      "Step 3100: Loss = 1.548360\n",
      "Step 3200: Loss = 240.829529\n",
      "Step 3300: Loss = 0.807689\n",
      "Step 3400: Loss = 285.937714\n",
      "Step 3500: Loss = 1.459821\n",
      "Step 3600: Loss = 490.425140\n",
      "Step 3700: Loss = 0.947697\n",
      "Step 3800: Loss = 94.281769\n",
      "Step 3900: Loss = 4.446171\n",
      "Step 4000: Loss = 0.748988\n",
      "Step 4100: Loss = 0.725780\n",
      "Step 4200: Loss = 0.712871\n",
      "Step 4300: Loss = 0.701590\n",
      "Step 4400: Loss = 0.691246\n",
      "Step 4500: Loss = 0.681735\n",
      "Step 4600: Loss = 0.672997\n",
      "Step 4700: Loss = 0.664976\n",
      "Step 4800: Loss = 0.657615\n",
      "Step 4900: Loss = 0.650860\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 4/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 188.196350\n",
      "Step 100: Loss = 59.760284\n",
      "Step 200: Loss = 155.950333\n",
      "Step 300: Loss = 4.327607\n",
      "Step 400: Loss = 211.972488\n",
      "Step 500: Loss = 1.659879\n",
      "Step 600: Loss = 179.485916\n",
      "Step 700: Loss = 3.185657\n",
      "Step 800: Loss = 301.837769\n",
      "Step 900: Loss = 5.854264\n",
      "Step 1000: Loss = 127.119545\n",
      "Step 1100: Loss = 2.263747\n",
      "Step 1200: Loss = 95.217598\n",
      "Step 1300: Loss = 1.728036\n",
      "Step 1400: Loss = 154.835052\n",
      "Step 1500: Loss = 1.340967\n",
      "Step 1600: Loss = 208.816574\n",
      "Step 1700: Loss = 0.942490\n",
      "Step 1800: Loss = 98.682640\n",
      "Step 1900: Loss = 1.216334\n",
      "Step 2000: Loss = 141.205490\n",
      "Step 2100: Loss = 0.874628\n",
      "Step 2200: Loss = 57.858082\n",
      "Step 2300: Loss = 1.352772\n",
      "Step 2400: Loss = 102.496956\n",
      "Step 2500: Loss = 1.350695\n",
      "Step 2600: Loss = 122.964828\n",
      "Step 2700: Loss = 0.950676\n",
      "Step 2800: Loss = 73.654503\n",
      "Step 2900: Loss = 0.854277\n",
      "Step 3000: Loss = 43.906574\n",
      "Step 3100: Loss = 0.710760\n",
      "Step 3200: Loss = 30.625286\n",
      "Step 3300: Loss = 0.772677\n",
      "Step 3400: Loss = 98.646103\n",
      "Step 3500: Loss = 1.026699\n",
      "Step 3600: Loss = 103.496841\n",
      "Step 3700: Loss = 3.290495\n",
      "Step 3800: Loss = 122.906593\n",
      "Step 3900: Loss = 9.833761\n",
      "Step 4000: Loss = 1.217399\n",
      "Step 4100: Loss = 0.657286\n",
      "Step 4200: Loss = 0.611226\n",
      "Step 4300: Loss = 0.592010\n",
      "Step 4400: Loss = 0.580887\n",
      "Step 4500: Loss = 0.572976\n",
      "Step 4600: Loss = 0.566641\n",
      "Step 4700: Loss = 0.561234\n",
      "Step 4800: Loss = 0.556462\n",
      "Step 4900: Loss = 0.552167\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 5/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 144.887848\n",
      "Step 100: Loss = 28.411795\n",
      "Step 200: Loss = 76.652229\n",
      "Step 300: Loss = 2.785724\n",
      "Step 400: Loss = 169.822021\n",
      "Step 500: Loss = 1.003854\n",
      "Step 600: Loss = 287.956543\n",
      "Step 700: Loss = 2.061316\n",
      "Step 800: Loss = 238.425812\n",
      "Step 900: Loss = 3.689328\n",
      "Step 1000: Loss = 297.039001\n",
      "Step 1100: Loss = 1.069674\n",
      "Step 1200: Loss = 163.165543\n",
      "Step 1300: Loss = 3.333148\n",
      "Step 1400: Loss = 325.439514\n",
      "Step 1500: Loss = 1.299078\n",
      "Step 1600: Loss = 133.322205\n",
      "Step 1700: Loss = 1.014472\n",
      "Step 1800: Loss = 92.868210\n",
      "Step 1900: Loss = 0.971713\n",
      "Step 2000: Loss = 118.555817\n",
      "Step 2100: Loss = 5.045565\n",
      "Step 2200: Loss = 166.043365\n",
      "Step 2300: Loss = 5.004744\n",
      "Step 2400: Loss = 43.096474\n",
      "Step 2500: Loss = 1.629986\n",
      "Step 2600: Loss = 165.011124\n",
      "Step 2700: Loss = 0.823800\n",
      "Step 2800: Loss = 200.304855\n",
      "Step 2900: Loss = 0.824618\n",
      "Step 3000: Loss = 146.982208\n",
      "Step 3100: Loss = 0.798241\n",
      "Step 3200: Loss = 344.851166\n",
      "Step 3300: Loss = 11.255261\n",
      "Step 3400: Loss = 494.837097\n",
      "Step 3500: Loss = 9.113701\n",
      "Step 3600: Loss = 77.256813\n",
      "Step 3700: Loss = 4.399427\n",
      "Step 3800: Loss = 133.618698\n",
      "Step 3900: Loss = 1.155980\n",
      "Step 4000: Loss = 0.798386\n",
      "Step 4100: Loss = 0.716776\n",
      "Step 4200: Loss = 0.672561\n",
      "Step 4300: Loss = 0.646195\n",
      "Step 4400: Loss = 0.629817\n",
      "Step 4500: Loss = 0.619234\n",
      "Step 4600: Loss = 0.612074\n",
      "Step 4700: Loss = 0.606964\n",
      "Step 4800: Loss = 0.603103\n",
      "Step 4900: Loss = 0.600018\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "  Resampling ratio η = 0.6\n",
      "    Repeat 1/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 215.298920\n",
      "Step 100: Loss = 111.311981\n",
      "Step 200: Loss = 261.893066\n",
      "Step 300: Loss = 67.264816\n",
      "Step 400: Loss = 867.093140\n",
      "Step 500: Loss = 37.757683\n",
      "Step 600: Loss = 250.458527\n",
      "Step 700: Loss = 9.164956\n",
      "Step 800: Loss = 350.759827\n",
      "Step 900: Loss = 21.455231\n",
      "Step 1000: Loss = 311.974609\n",
      "Step 1100: Loss = 38.437134\n",
      "Step 1200: Loss = 318.300995\n",
      "Step 1300: Loss = 1.791814\n",
      "Step 1400: Loss = 312.472870\n",
      "Step 1500: Loss = 34.532269\n",
      "Step 1600: Loss = 311.286987\n",
      "Step 1700: Loss = 3.454299\n",
      "Step 1800: Loss = 550.083801\n",
      "Step 1900: Loss = 28.451260\n",
      "Step 2000: Loss = 301.267487\n",
      "Step 2100: Loss = 21.457188\n",
      "Step 2200: Loss = 324.392029\n",
      "Step 2300: Loss = 4.056553\n",
      "Step 2400: Loss = 125.383736\n",
      "Step 2500: Loss = 7.587665\n",
      "Step 2600: Loss = 287.643250\n",
      "Step 2700: Loss = 14.506617\n",
      "Step 2800: Loss = 491.259979\n",
      "Step 2900: Loss = 38.178280\n",
      "Step 3000: Loss = 410.996643\n",
      "Step 3100: Loss = 4.389620\n",
      "Step 3200: Loss = 317.892639\n",
      "Step 3300: Loss = 23.295284\n",
      "Step 3400: Loss = 404.653412\n",
      "Step 3500: Loss = 6.010167\n",
      "Step 3600: Loss = 252.121262\n",
      "Step 3700: Loss = 71.837074\n",
      "Step 3800: Loss = 728.039429\n",
      "Step 3900: Loss = 25.880329\n",
      "Step 4000: Loss = 11.575379\n",
      "Step 4100: Loss = 9.337149\n",
      "Step 4200: Loss = 8.534527\n",
      "Step 4300: Loss = 8.077753\n",
      "Step 4400: Loss = 7.747839\n",
      "Step 4500: Loss = 7.467596\n",
      "Step 4600: Loss = 7.215404\n",
      "Step 4700: Loss = 6.982173\n",
      "Step 4800: Loss = 6.762463\n",
      "Step 4900: Loss = 6.553280\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 2/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 214.539001\n",
      "Step 100: Loss = 33.330563\n",
      "Step 200: Loss = 302.670441\n",
      "Step 300: Loss = 12.666092\n",
      "Step 400: Loss = 1240.271606\n",
      "Step 500: Loss = 159.498871\n",
      "Step 600: Loss = 105.925880\n",
      "Step 700: Loss = 11.015841\n",
      "Step 800: Loss = 860.935791\n",
      "Step 900: Loss = 8.202226\n",
      "Step 1000: Loss = 202.883209\n",
      "Step 1100: Loss = 8.841608\n",
      "Step 1200: Loss = 300.054047\n",
      "Step 1300: Loss = 5.727321\n",
      "Step 1400: Loss = 267.651093\n",
      "Step 1500: Loss = 6.986611\n",
      "Step 1600: Loss = 443.590027\n",
      "Step 1700: Loss = 1.523185\n",
      "Step 1800: Loss = 516.055908\n",
      "Step 1900: Loss = 3.851875\n",
      "Step 2000: Loss = 766.330383\n",
      "Step 2100: Loss = 9.656976\n",
      "Step 2200: Loss = 574.744263\n",
      "Step 2300: Loss = 12.660591\n",
      "Step 2400: Loss = 302.562714\n",
      "Step 2500: Loss = 1.598139\n",
      "Step 2600: Loss = 340.598480\n",
      "Step 2700: Loss = 6.135219\n",
      "Step 2800: Loss = 723.945251\n",
      "Step 2900: Loss = 20.098339\n",
      "Step 3000: Loss = 234.736206\n",
      "Step 3100: Loss = 1.307628\n",
      "Step 3200: Loss = 256.122467\n",
      "Step 3300: Loss = 2.343635\n",
      "Step 3400: Loss = 284.237183\n",
      "Step 3500: Loss = 12.056605\n",
      "Step 3600: Loss = 317.950470\n",
      "Step 3700: Loss = 9.456703\n",
      "Step 3800: Loss = 312.611450\n",
      "Step 3900: Loss = 14.848000\n",
      "Step 4000: Loss = 5.713530\n",
      "Step 4100: Loss = 0.957979\n",
      "Step 4200: Loss = 0.692345\n",
      "Step 4300: Loss = 0.675920\n",
      "Step 4400: Loss = 0.667800\n",
      "Step 4500: Loss = 0.660829\n",
      "Step 4600: Loss = 0.654564\n",
      "Step 4700: Loss = 0.648853\n",
      "Step 4800: Loss = 0.643590\n",
      "Step 4900: Loss = 0.638699\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 3/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 207.125549\n",
      "Step 100: Loss = 10.104910\n",
      "Step 200: Loss = 924.622009\n",
      "Step 300: Loss = 134.213882\n",
      "Step 400: Loss = 630.453430\n",
      "Step 500: Loss = 80.108086\n",
      "Step 600: Loss = 283.990814\n",
      "Step 700: Loss = 24.015316\n",
      "Step 800: Loss = 1252.398071\n",
      "Step 900: Loss = 63.898113\n",
      "Step 1000: Loss = 209.315689\n",
      "Step 1100: Loss = 79.792976\n",
      "Step 1200: Loss = 374.501099\n",
      "Step 1300: Loss = 39.210358\n",
      "Step 1400: Loss = 314.512604\n",
      "Step 1500: Loss = 6.615138\n",
      "Step 1600: Loss = 245.048340\n",
      "Step 1700: Loss = 10.992342\n",
      "Step 1800: Loss = 270.147827\n",
      "Step 1900: Loss = 6.160748\n",
      "Step 2000: Loss = 304.577057\n",
      "Step 2100: Loss = 4.006497\n",
      "Step 2200: Loss = 266.569519\n",
      "Step 2300: Loss = 18.526533\n",
      "Step 2400: Loss = 420.151611\n",
      "Step 2500: Loss = 14.843048\n",
      "Step 2600: Loss = 217.362381\n",
      "Step 2700: Loss = 20.325756\n",
      "Step 2800: Loss = 306.570618\n",
      "Step 2900: Loss = 23.893665\n",
      "Step 3000: Loss = 150.570892\n",
      "Step 3100: Loss = 4.311053\n",
      "Step 3200: Loss = 302.700653\n",
      "Step 3300: Loss = 7.777742\n",
      "Step 3400: Loss = 219.689667\n",
      "Step 3500: Loss = 26.639633\n",
      "Step 3600: Loss = 652.049316\n",
      "Step 3700: Loss = 5.774708\n",
      "Step 3800: Loss = 289.051392\n",
      "Step 3900: Loss = 4.363641\n",
      "Step 4000: Loss = 1.562247\n",
      "Step 4100: Loss = 0.991265\n",
      "Step 4200: Loss = 0.914875\n",
      "Step 4300: Loss = 0.882502\n",
      "Step 4400: Loss = 0.858099\n",
      "Step 4500: Loss = 0.837499\n",
      "Step 4600: Loss = 0.819132\n",
      "Step 4700: Loss = 0.802242\n",
      "Step 4800: Loss = 0.786442\n",
      "Step 4900: Loss = 0.771526\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 4/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 187.682556\n",
      "Step 100: Loss = 32.348598\n",
      "Step 200: Loss = 296.634460\n",
      "Step 300: Loss = 12.884241\n",
      "Step 400: Loss = 344.031403\n",
      "Step 500: Loss = 27.268919\n",
      "Step 600: Loss = 278.976593\n",
      "Step 700: Loss = 8.344480\n",
      "Step 800: Loss = 244.568771\n",
      "Step 900: Loss = 5.097426\n",
      "Step 1000: Loss = 817.311646\n",
      "Step 1100: Loss = 27.851456\n",
      "Step 1200: Loss = 430.369843\n",
      "Step 1300: Loss = 41.266548\n",
      "Step 1400: Loss = 676.965027\n",
      "Step 1500: Loss = 12.051735\n",
      "Step 1600: Loss = 269.213287\n",
      "Step 1700: Loss = 11.980180\n",
      "Step 1800: Loss = 453.768829\n",
      "Step 1900: Loss = 18.969967\n",
      "Step 2000: Loss = 232.889160\n",
      "Step 2100: Loss = 14.663681\n",
      "Step 2200: Loss = 122.174004\n",
      "Step 2300: Loss = 6.179336\n",
      "Step 2400: Loss = 462.152496\n",
      "Step 2500: Loss = 12.605070\n",
      "Step 2600: Loss = 337.506897\n",
      "Step 2700: Loss = 25.648842\n",
      "Step 2800: Loss = 238.901535\n",
      "Step 2900: Loss = 8.004032\n",
      "Step 3000: Loss = 634.090942\n",
      "Step 3100: Loss = 19.014217\n",
      "Step 3200: Loss = 409.347412\n",
      "Step 3300: Loss = 18.498251\n",
      "Step 3400: Loss = 230.482025\n",
      "Step 3500: Loss = 7.265347\n",
      "Step 3600: Loss = 298.743713\n",
      "Step 3700: Loss = 10.830433\n",
      "Step 3800: Loss = 238.600983\n",
      "Step 3900: Loss = 12.304224\n",
      "Step 4000: Loss = 7.622063\n",
      "Step 4100: Loss = 6.615507\n",
      "Step 4200: Loss = 6.156069\n",
      "Step 4300: Loss = 5.839155\n",
      "Step 4400: Loss = 5.582539\n",
      "Step 4500: Loss = 5.363449\n",
      "Step 4600: Loss = 5.171578\n",
      "Step 4700: Loss = 5.000913\n",
      "Step 4800: Loss = 4.847443\n",
      "Step 4900: Loss = 4.708260\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 5/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 170.310532\n",
      "Step 100: Loss = 24.411018\n",
      "Step 200: Loss = 424.777466\n",
      "Step 300: Loss = 12.769386\n",
      "Step 400: Loss = 282.131012\n",
      "Step 500: Loss = 2.657245\n",
      "Step 600: Loss = 237.142563\n",
      "Step 700: Loss = 2.318816\n",
      "Step 800: Loss = 414.998749\n",
      "Step 900: Loss = 5.105321\n",
      "Step 1000: Loss = 191.451630\n",
      "Step 1100: Loss = 0.878912\n",
      "Step 1200: Loss = 237.972565\n",
      "Step 1300: Loss = 1.673897\n",
      "Step 1400: Loss = 460.298401\n",
      "Step 1500: Loss = 44.936012\n",
      "Step 1600: Loss = 248.960907\n",
      "Step 1700: Loss = 13.475016\n",
      "Step 1800: Loss = 509.058960\n",
      "Step 1900: Loss = 52.014008\n",
      "Step 2000: Loss = 214.385925\n",
      "Step 2100: Loss = 10.578484\n",
      "Step 2200: Loss = 232.541000\n",
      "Step 2300: Loss = 13.246724\n",
      "Step 2400: Loss = 663.421509\n",
      "Step 2500: Loss = 33.827866\n",
      "Step 2600: Loss = 697.065369\n",
      "Step 2700: Loss = 22.565802\n",
      "Step 2800: Loss = 369.302551\n",
      "Step 2900: Loss = 14.946299\n",
      "Step 3000: Loss = 663.384766\n",
      "Step 3100: Loss = 21.442879\n",
      "Step 3200: Loss = 319.324738\n",
      "Step 3300: Loss = 2.714489\n",
      "Step 3400: Loss = 883.467163\n",
      "Step 3500: Loss = 12.299688\n",
      "Step 3600: Loss = 177.858154\n",
      "Step 3700: Loss = 9.217520\n",
      "Step 3800: Loss = 183.792328\n",
      "Step 3900: Loss = 10.732353\n",
      "Step 4000: Loss = 8.198715\n",
      "Step 4100: Loss = 6.254859\n",
      "Step 4200: Loss = 4.554700\n",
      "Step 4300: Loss = 2.992500\n",
      "Step 4400: Loss = 1.768275\n",
      "Step 4500: Loss = 1.119983\n",
      "Step 4600: Loss = 0.923700\n",
      "Step 4700: Loss = 0.878986\n",
      "Step 4800: Loss = 0.860905\n",
      "Step 4900: Loss = 0.847178\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "  Resampling ratio η = 0.9\n",
      "    Repeat 1/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 281.764862\n",
      "Step 100: Loss = 21.114719\n",
      "Step 200: Loss = 576.747498\n",
      "Step 300: Loss = 71.310623\n",
      "Step 400: Loss = 669.260986\n",
      "Step 500: Loss = 8.877845\n",
      "Step 600: Loss = 587.599243\n",
      "Step 700: Loss = 84.348946\n",
      "Step 800: Loss = 697.220947\n",
      "Step 900: Loss = 64.947876\n",
      "Step 1000: Loss = 559.510742\n",
      "Step 1100: Loss = 39.051361\n",
      "Step 1200: Loss = 1074.110474\n",
      "Step 1300: Loss = 63.651527\n",
      "Step 1400: Loss = 260.104767\n",
      "Step 1500: Loss = 11.842411\n",
      "Step 1600: Loss = 346.924774\n",
      "Step 1700: Loss = 40.311100\n",
      "Step 1800: Loss = 343.910492\n",
      "Step 1900: Loss = 16.457422\n",
      "Step 2000: Loss = 576.786194\n",
      "Step 2100: Loss = 30.965490\n",
      "Step 2200: Loss = 1032.469238\n",
      "Step 2300: Loss = 77.324509\n",
      "Step 2400: Loss = 491.727844\n",
      "Step 2500: Loss = 14.683064\n",
      "Step 2600: Loss = 299.816284\n",
      "Step 2700: Loss = 25.369480\n",
      "Step 2800: Loss = 434.839630\n",
      "Step 2900: Loss = 11.588511\n",
      "Step 3000: Loss = 780.273865\n",
      "Step 3100: Loss = 18.731422\n",
      "Step 3200: Loss = 646.455933\n",
      "Step 3300: Loss = 32.979816\n",
      "Step 3400: Loss = 782.541931\n",
      "Step 3500: Loss = 17.458252\n",
      "Step 3600: Loss = 1042.889526\n",
      "Step 3700: Loss = 23.455114\n",
      "Step 3800: Loss = 732.195923\n",
      "Step 3900: Loss = 59.851463\n",
      "Step 4000: Loss = 19.729420\n",
      "Step 4100: Loss = 12.625453\n",
      "Step 4200: Loss = 11.228398\n",
      "Step 4300: Loss = 10.514156\n",
      "Step 4400: Loss = 9.969316\n",
      "Step 4500: Loss = 9.477156\n",
      "Step 4600: Loss = 9.007929\n",
      "Step 4700: Loss = 8.555892\n",
      "Step 4800: Loss = 8.123949\n",
      "Step 4900: Loss = 7.717475\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 2/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 180.713867\n",
      "Step 100: Loss = 18.004475\n",
      "Step 200: Loss = 600.641052\n",
      "Step 300: Loss = 15.341309\n",
      "Step 400: Loss = 334.168549\n",
      "Step 500: Loss = 32.310867\n",
      "Step 600: Loss = 408.993317\n",
      "Step 700: Loss = 6.876513\n",
      "Step 800: Loss = 929.651672\n",
      "Step 900: Loss = 51.942749\n",
      "Step 1000: Loss = 422.324829\n",
      "Step 1100: Loss = 8.491699\n",
      "Step 1200: Loss = 1118.928833\n",
      "Step 1300: Loss = 24.743969\n",
      "Step 1400: Loss = 285.098053\n",
      "Step 1500: Loss = 44.368065\n",
      "Step 1600: Loss = 371.547913\n",
      "Step 1700: Loss = 13.346642\n",
      "Step 1800: Loss = 339.602081\n",
      "Step 1900: Loss = 7.984552\n",
      "Step 2000: Loss = 362.245026\n",
      "Step 2100: Loss = 2.033501\n",
      "Step 2200: Loss = 397.773346\n",
      "Step 2300: Loss = 9.009348\n",
      "Step 2400: Loss = 372.344757\n",
      "Step 2500: Loss = 5.578846\n",
      "Step 2600: Loss = 662.008850\n",
      "Step 2700: Loss = 34.660583\n",
      "Step 2800: Loss = 168.392899\n",
      "Step 2900: Loss = 24.816313\n",
      "Step 3000: Loss = 376.753815\n",
      "Step 3100: Loss = 18.999863\n",
      "Step 3200: Loss = 295.207001\n",
      "Step 3300: Loss = 12.172737\n",
      "Step 3400: Loss = 459.632019\n",
      "Step 3500: Loss = 17.105438\n",
      "Step 3600: Loss = 342.753693\n",
      "Step 3700: Loss = 27.811043\n",
      "Step 3800: Loss = 173.668411\n",
      "Step 3900: Loss = 39.459026\n",
      "Step 4000: Loss = 6.981507\n",
      "Step 4100: Loss = 5.018676\n",
      "Step 4200: Loss = 4.488263\n",
      "Step 4300: Loss = 4.181932\n",
      "Step 4400: Loss = 3.947446\n",
      "Step 4500: Loss = 3.747617\n",
      "Step 4600: Loss = 3.571911\n",
      "Step 4700: Loss = 3.415645\n",
      "Step 4800: Loss = 3.275751\n",
      "Step 4900: Loss = 3.149876\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 3/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 225.200439\n",
      "Step 100: Loss = 34.059765\n",
      "Step 200: Loss = 414.134460\n",
      "Step 300: Loss = 2.534713\n",
      "Step 400: Loss = 601.126343\n",
      "Step 500: Loss = 34.753548\n",
      "Step 600: Loss = 750.946289\n",
      "Step 700: Loss = 27.338223\n",
      "Step 800: Loss = 548.295898\n",
      "Step 900: Loss = 66.523972\n",
      "Step 1000: Loss = 585.957703\n",
      "Step 1100: Loss = 30.959530\n",
      "Step 1200: Loss = 1037.612915\n",
      "Step 1300: Loss = 74.544456\n",
      "Step 1400: Loss = 426.716370\n",
      "Step 1500: Loss = 33.990166\n",
      "Step 1600: Loss = 598.112244\n",
      "Step 1700: Loss = 37.690975\n",
      "Step 1800: Loss = 625.824646\n",
      "Step 1900: Loss = 50.655743\n",
      "Step 2000: Loss = 550.643250\n",
      "Step 2100: Loss = 90.579971\n",
      "Step 2200: Loss = 344.446625\n",
      "Step 2300: Loss = 51.601368\n",
      "Step 2400: Loss = 293.625641\n",
      "Step 2500: Loss = 37.259075\n",
      "Step 2600: Loss = 433.502808\n",
      "Step 2700: Loss = 27.517769\n",
      "Step 2800: Loss = 514.868347\n",
      "Step 2900: Loss = 43.163647\n",
      "Step 3000: Loss = 360.494476\n",
      "Step 3100: Loss = 17.486944\n",
      "Step 3200: Loss = 823.923157\n",
      "Step 3300: Loss = 5.204158\n",
      "Step 3400: Loss = 383.865448\n",
      "Step 3500: Loss = 14.487714\n",
      "Step 3600: Loss = 819.888977\n",
      "Step 3700: Loss = 88.363457\n",
      "Step 3800: Loss = 576.366455\n",
      "Step 3900: Loss = 69.257324\n",
      "Step 4000: Loss = 41.548679\n",
      "Step 4100: Loss = 34.170940\n",
      "Step 4200: Loss = 15.876017\n",
      "Step 4300: Loss = 1.481274\n",
      "Step 4400: Loss = 1.338778\n",
      "Step 4500: Loss = 1.249614\n",
      "Step 4600: Loss = 1.173170\n",
      "Step 4700: Loss = 1.107364\n",
      "Step 4800: Loss = 1.050654\n",
      "Step 4900: Loss = 1.001704\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 4/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 197.944870\n",
      "Step 100: Loss = 11.109437\n",
      "Step 200: Loss = 547.400391\n",
      "Step 300: Loss = 88.170837\n",
      "Step 400: Loss = 475.515686\n",
      "Step 500: Loss = 19.957455\n",
      "Step 600: Loss = 863.536255\n",
      "Step 700: Loss = 50.983620\n",
      "Step 800: Loss = 205.259277\n",
      "Step 900: Loss = 92.465454\n",
      "Step 1000: Loss = 496.846344\n",
      "Step 1100: Loss = 77.616234\n",
      "Step 1200: Loss = 466.795929\n",
      "Step 1300: Loss = 5.139577\n",
      "Step 1400: Loss = 356.307526\n",
      "Step 1500: Loss = 25.480700\n",
      "Step 1600: Loss = 246.765976\n",
      "Step 1700: Loss = 26.570442\n",
      "Step 1800: Loss = 197.308304\n",
      "Step 1900: Loss = 4.217022\n",
      "Step 2000: Loss = 398.183716\n",
      "Step 2100: Loss = 2.597774\n",
      "Step 2200: Loss = 534.180664\n",
      "Step 2300: Loss = 59.182213\n",
      "Step 2400: Loss = 447.350555\n",
      "Step 2500: Loss = 14.344463\n",
      "Step 2600: Loss = 179.605225\n",
      "Step 2700: Loss = 9.252753\n",
      "Step 2800: Loss = 354.883942\n",
      "Step 2900: Loss = 19.701786\n",
      "Step 3000: Loss = 204.513138\n",
      "Step 3100: Loss = 43.745682\n",
      "Step 3200: Loss = 298.529480\n",
      "Step 3300: Loss = 14.442854\n",
      "Step 3400: Loss = 352.804840\n",
      "Step 3500: Loss = 1.912445\n",
      "Step 3600: Loss = 272.972412\n",
      "Step 3700: Loss = 23.926311\n",
      "Step 3800: Loss = 382.227692\n",
      "Step 3900: Loss = 94.736794\n",
      "Step 4000: Loss = 37.988777\n",
      "Step 4100: Loss = 2.247032\n",
      "Step 4200: Loss = 1.421206\n",
      "Step 4300: Loss = 1.164236\n",
      "Step 4400: Loss = 1.013335\n",
      "Step 4500: Loss = 0.917718\n",
      "Step 4600: Loss = 0.853702\n",
      "Step 4700: Loss = 0.808944\n",
      "Step 4800: Loss = 0.776484\n",
      "Step 4900: Loss = 0.752158\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 5/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 162.329117\n",
      "Step 100: Loss = 13.215961\n",
      "Step 200: Loss = 812.284607\n",
      "Step 300: Loss = 66.569725\n",
      "Step 400: Loss = 537.177124\n",
      "Step 500: Loss = 76.316154\n",
      "Step 600: Loss = 536.121399\n",
      "Step 700: Loss = 36.069496\n",
      "Step 800: Loss = 517.598755\n",
      "Step 900: Loss = 36.670609\n",
      "Step 1000: Loss = 1528.708740\n",
      "Step 1100: Loss = 41.467354\n",
      "Step 1200: Loss = 742.932739\n",
      "Step 1300: Loss = 40.558941\n",
      "Step 1400: Loss = 965.461121\n",
      "Step 1500: Loss = 65.374008\n",
      "Step 1600: Loss = 878.323547\n",
      "Step 1700: Loss = 40.872383\n",
      "Step 1800: Loss = 356.851837\n",
      "Step 1900: Loss = 5.807858\n",
      "Step 2000: Loss = 294.176819\n",
      "Step 2100: Loss = 9.368736\n",
      "Step 2200: Loss = 771.641479\n",
      "Step 2300: Loss = 15.016127\n",
      "Step 2400: Loss = 393.383484\n",
      "Step 2500: Loss = 4.561862\n",
      "Step 2600: Loss = 1028.953125\n",
      "Step 2700: Loss = 33.082878\n",
      "Step 2800: Loss = 377.270447\n",
      "Step 2900: Loss = 21.617390\n",
      "Step 3000: Loss = 320.432587\n",
      "Step 3100: Loss = 30.802511\n",
      "Step 3200: Loss = 915.180786\n",
      "Step 3300: Loss = 40.228497\n",
      "Step 3400: Loss = 856.367371\n",
      "Step 3500: Loss = 22.171761\n",
      "Step 3600: Loss = 429.562653\n",
      "Step 3700: Loss = 2.974078\n",
      "Step 3800: Loss = 420.061493\n",
      "Step 3900: Loss = 37.888947\n",
      "Step 4000: Loss = 14.699726\n",
      "Step 4100: Loss = 9.978025\n",
      "Step 4200: Loss = 6.431058\n",
      "Step 4300: Loss = 3.456370\n",
      "Step 4400: Loss = 1.882097\n",
      "Step 4500: Loss = 1.542531\n",
      "Step 4600: Loss = 1.444798\n",
      "Step 4700: Loss = 1.389482\n",
      "Step 4800: Loss = 1.350129\n",
      "Step 4900: Loss = 1.319328\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "\n",
      "Running experiments with 40 residual points\n",
      "  Resampling ratio η = 0.0\n",
      "    Repeat 1/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 174.746002\n",
      "Step 100: Loss = 62.838806\n",
      "Step 200: Loss = 11.165241\n",
      "Step 300: Loss = 3.975047\n",
      "Step 400: Loss = 1.907532\n",
      "Step 500: Loss = 1.701795\n",
      "Step 600: Loss = 1.583584\n",
      "Step 700: Loss = 1.460424\n",
      "Step 800: Loss = 2.090977\n",
      "Step 900: Loss = 1.202635\n",
      "Step 1000: Loss = 1.754670\n",
      "Step 1100: Loss = 0.998543\n",
      "Step 1200: Loss = 0.946173\n",
      "Step 1300: Loss = 0.920325\n",
      "Step 1400: Loss = 0.890530\n",
      "Step 1500: Loss = 1.122451\n",
      "Step 1600: Loss = 0.853829\n",
      "Step 1700: Loss = 1.304446\n",
      "Step 1800: Loss = 0.826309\n",
      "Step 1900: Loss = 0.813328\n",
      "Step 2000: Loss = 0.807490\n",
      "Step 2100: Loss = 0.794396\n",
      "Step 2200: Loss = 0.784332\n",
      "Step 2300: Loss = 0.777553\n",
      "Step 2400: Loss = 0.767880\n",
      "Step 2500: Loss = 0.780730\n",
      "Step 2600: Loss = 0.753387\n",
      "Step 2700: Loss = 0.745994\n",
      "Step 2800: Loss = 0.745609\n",
      "Step 2900: Loss = 0.734823\n",
      "Step 3000: Loss = 0.728516\n",
      "Step 3100: Loss = 0.722494\n",
      "Step 3200: Loss = 0.812397\n",
      "Step 3300: Loss = 0.712414\n",
      "Step 3400: Loss = 0.707111\n",
      "Step 3500: Loss = 0.838356\n",
      "Step 3600: Loss = 0.698023\n",
      "Step 3700: Loss = 0.693433\n",
      "Step 3800: Loss = 0.695711\n",
      "Step 3900: Loss = 0.685267\n",
      "Step 4000: Loss = 0.681120\n",
      "Step 4100: Loss = 0.678840\n",
      "Step 4200: Loss = 0.673729\n",
      "Step 4300: Loss = 0.669968\n",
      "Step 4400: Loss = 0.667122\n",
      "Step 4500: Loss = 0.663467\n",
      "Step 4600: Loss = 0.660084\n",
      "Step 4700: Loss = 0.658768\n",
      "Step 4800: Loss = 0.654149\n",
      "Step 4900: Loss = 0.651050\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 2/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 165.832123\n",
      "Step 100: Loss = 78.830536\n",
      "Step 200: Loss = 8.697302\n",
      "Step 300: Loss = 2.003696\n",
      "Step 400: Loss = 1.467964\n",
      "Step 500: Loss = 1.035156\n",
      "Step 600: Loss = 0.851251\n",
      "Step 700: Loss = 0.785039\n",
      "Step 800: Loss = 1.677538\n",
      "Step 900: Loss = 0.693323\n",
      "Step 1000: Loss = 0.673476\n",
      "Step 1100: Loss = 0.670544\n",
      "Step 1200: Loss = 0.663111\n",
      "Step 1300: Loss = 0.641773\n",
      "Step 1400: Loss = 0.621838\n",
      "Step 1500: Loss = 0.616192\n",
      "Step 1600: Loss = 0.603177\n",
      "Step 1700: Loss = 0.600152\n",
      "Step 1800: Loss = 0.587403\n",
      "Step 1900: Loss = 0.585545\n",
      "Step 2000: Loss = 0.574503\n",
      "Step 2100: Loss = 0.629804\n",
      "Step 2200: Loss = 0.563116\n",
      "Step 2300: Loss = 0.557636\n",
      "Step 2400: Loss = 0.552546\n",
      "Step 2500: Loss = 0.548417\n",
      "Step 2600: Loss = 0.543663\n",
      "Step 2700: Loss = 0.684908\n",
      "Step 2800: Loss = 0.535916\n",
      "Step 2900: Loss = 1.009646\n",
      "Step 3000: Loss = 0.529110\n",
      "Step 3100: Loss = 0.525754\n",
      "Step 3200: Loss = 0.522561\n",
      "Step 3300: Loss = 0.519581\n",
      "Step 3400: Loss = 0.516986\n",
      "Step 3500: Loss = 0.629380\n",
      "Step 3600: Loss = 0.511580\n",
      "Step 3700: Loss = 0.509600\n",
      "Step 3800: Loss = 0.508790\n",
      "Step 3900: Loss = 0.504683\n",
      "Step 4000: Loss = 0.631040\n",
      "Step 4100: Loss = 0.500474\n",
      "Step 4200: Loss = 0.498528\n",
      "Step 4300: Loss = 0.496753\n",
      "Step 4400: Loss = 0.494859\n",
      "Step 4500: Loss = 0.493089\n",
      "Step 4600: Loss = 0.504384\n",
      "Step 4700: Loss = 0.489867\n",
      "Step 4800: Loss = 0.488299\n",
      "Step 4900: Loss = 0.490334\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 3/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 192.307083\n",
      "Step 100: Loss = 19.554182\n",
      "Step 200: Loss = 2.481860\n",
      "Step 300: Loss = 1.895712\n",
      "Step 400: Loss = 1.817554\n",
      "Step 500: Loss = 1.417745\n",
      "Step 600: Loss = 1.263069\n",
      "Step 700: Loss = 1.168845\n",
      "Step 800: Loss = 1.029559\n",
      "Step 900: Loss = 0.982271\n",
      "Step 1000: Loss = 0.906013\n",
      "Step 1100: Loss = 0.867592\n",
      "Step 1200: Loss = 0.826019\n",
      "Step 1300: Loss = 0.826238\n",
      "Step 1400: Loss = 0.791771\n",
      "Step 1500: Loss = 0.771205\n",
      "Step 1600: Loss = 0.754075\n",
      "Step 1700: Loss = 0.741059\n",
      "Step 1800: Loss = 0.729474\n",
      "Step 1900: Loss = 0.717372\n",
      "Step 2000: Loss = 0.717012\n",
      "Step 2100: Loss = 0.702384\n",
      "Step 2200: Loss = 0.692775\n",
      "Step 2300: Loss = 0.683991\n",
      "Step 2400: Loss = 1.154473\n",
      "Step 2500: Loss = 0.670294\n",
      "Step 2600: Loss = 0.663078\n",
      "Step 2700: Loss = 0.661798\n",
      "Step 2800: Loss = 0.653941\n",
      "Step 2900: Loss = 0.647974\n",
      "Step 3000: Loss = 0.642375\n",
      "Step 3100: Loss = 0.637071\n",
      "Step 3200: Loss = 0.638242\n",
      "Step 3300: Loss = 0.628167\n",
      "Step 3400: Loss = 0.668124\n",
      "Step 3500: Loss = 0.620794\n",
      "Step 3600: Loss = 0.616629\n",
      "Step 3700: Loss = 0.612664\n",
      "Step 3800: Loss = 0.610359\n",
      "Step 3900: Loss = 0.606630\n",
      "Step 4000: Loss = 0.603179\n",
      "Step 4100: Loss = 0.599862\n",
      "Step 4200: Loss = 0.597873\n",
      "Step 4300: Loss = 0.594648\n",
      "Step 4400: Loss = 0.591740\n",
      "Step 4500: Loss = 0.588921\n",
      "Step 4600: Loss = 0.587568\n",
      "Step 4700: Loss = 0.584592\n",
      "Step 4800: Loss = 0.582131\n",
      "Step 4900: Loss = 0.579737\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 4/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 211.069260\n",
      "Step 100: Loss = 120.800552\n",
      "Step 200: Loss = 10.204397\n",
      "Step 300: Loss = 2.696286\n",
      "Step 400: Loss = 1.304103\n",
      "Step 500: Loss = 1.076348\n",
      "Step 600: Loss = 0.789117\n",
      "Step 700: Loss = 0.746796\n",
      "Step 800: Loss = 2.532959\n",
      "Step 900: Loss = 0.703356\n",
      "Step 1000: Loss = 0.684341\n",
      "Step 1100: Loss = 1.526736\n",
      "Step 1200: Loss = 0.664524\n",
      "Step 1300: Loss = 0.652594\n",
      "Step 1400: Loss = 0.643580\n",
      "Step 1500: Loss = 0.802309\n",
      "Step 1600: Loss = 0.628407\n",
      "Step 1700: Loss = 0.623363\n",
      "Step 1800: Loss = 0.615580\n",
      "Step 1900: Loss = 0.609798\n",
      "Step 2000: Loss = 0.605588\n",
      "Step 2100: Loss = 0.599334\n",
      "Step 2200: Loss = 0.594661\n",
      "Step 2300: Loss = 0.853425\n",
      "Step 2400: Loss = 0.585958\n",
      "Step 2500: Loss = 0.582053\n",
      "Step 2600: Loss = 0.632465\n",
      "Step 2700: Loss = 0.574847\n",
      "Step 2800: Loss = 0.571566\n",
      "Step 2900: Loss = 0.568425\n",
      "Step 3000: Loss = 0.565411\n",
      "Step 3100: Loss = 0.562775\n",
      "Step 3200: Loss = 0.560156\n",
      "Step 3300: Loss = 0.557066\n",
      "Step 3400: Loss = 0.554522\n",
      "Step 3500: Loss = 0.552067\n",
      "Step 3600: Loss = 0.679269\n",
      "Step 3700: Loss = 0.547365\n",
      "Step 3800: Loss = 0.545162\n",
      "Step 3900: Loss = 0.543016\n",
      "Step 4000: Loss = 0.553709\n",
      "Step 4100: Loss = 0.538913\n",
      "Step 4200: Loss = 0.536969\n",
      "Step 4300: Loss = 0.535068\n",
      "Step 4400: Loss = 0.537992\n",
      "Step 4500: Loss = 0.531448\n",
      "Step 4600: Loss = 0.529700\n",
      "Step 4700: Loss = 0.527998\n",
      "Step 4800: Loss = 0.526702\n",
      "Step 4900: Loss = 0.524774\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 5/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 250.607651\n",
      "Step 100: Loss = 68.721703\n",
      "Step 200: Loss = 16.108948\n",
      "Step 300: Loss = 3.942132\n",
      "Step 400: Loss = 2.955519\n",
      "Step 500: Loss = 5.575708\n",
      "Step 600: Loss = 1.800178\n",
      "Step 700: Loss = 1.689686\n",
      "Step 800: Loss = 1.564147\n",
      "Step 900: Loss = 1.687286\n",
      "Step 1000: Loss = 1.421460\n",
      "Step 1100: Loss = 1.358385\n",
      "Step 1200: Loss = 1.470023\n",
      "Step 1300: Loss = 1.243117\n",
      "Step 1400: Loss = 1.204688\n",
      "Step 1500: Loss = 1.154390\n",
      "Step 1600: Loss = 1.917287\n",
      "Step 1700: Loss = 1.091936\n",
      "Step 1800: Loss = 1.066242\n",
      "Step 1900: Loss = 1.043864\n",
      "Step 2000: Loss = 1.032656\n",
      "Step 2100: Loss = 1.010291\n",
      "Step 2200: Loss = 1.007008\n",
      "Step 2300: Loss = 0.982064\n",
      "Step 2400: Loss = 2.334172\n",
      "Step 2500: Loss = 0.960087\n",
      "Step 2600: Loss = 0.948692\n",
      "Step 2700: Loss = 0.937991\n",
      "Step 2800: Loss = 0.930906\n",
      "Step 2900: Loss = 0.921221\n",
      "Step 3000: Loss = 0.912097\n",
      "Step 3100: Loss = 0.925184\n",
      "Step 3200: Loss = 0.896498\n",
      "Step 3300: Loss = 0.984462\n",
      "Step 3400: Loss = 0.883062\n",
      "Step 3500: Loss = 0.875635\n",
      "Step 3600: Loss = 0.868402\n",
      "Step 3700: Loss = 0.871398\n",
      "Step 3800: Loss = 0.856146\n",
      "Step 3900: Loss = 0.849562\n",
      "Step 4000: Loss = 0.844980\n",
      "Step 4100: Loss = 0.838662\n",
      "Step 4200: Loss = 0.832656\n",
      "Step 4300: Loss = 0.922842\n",
      "Step 4400: Loss = 0.822306\n",
      "Step 4500: Loss = 0.816775\n",
      "Step 4600: Loss = 0.823045\n",
      "Step 4700: Loss = 0.806568\n",
      "Step 4800: Loss = 0.813561\n",
      "Step 4900: Loss = 0.796961\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "  Resampling ratio η = 0.3\n",
      "    Repeat 1/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 177.855988\n",
      "Step 100: Loss = 23.874269\n",
      "Step 200: Loss = 82.568542\n",
      "Step 300: Loss = 1.152356\n",
      "Step 400: Loss = 104.388710\n",
      "Step 500: Loss = 1.919109\n",
      "Step 600: Loss = 143.500549\n",
      "Step 700: Loss = 11.983522\n",
      "Step 800: Loss = 187.799423\n",
      "Step 900: Loss = 1.419062\n",
      "Step 1000: Loss = 106.703773\n",
      "Step 1100: Loss = 1.328611\n",
      "Step 1200: Loss = 79.357727\n",
      "Step 1300: Loss = 1.135906\n",
      "Step 1400: Loss = 227.002792\n",
      "Step 1500: Loss = 1.633003\n",
      "Step 1600: Loss = 165.572922\n",
      "Step 1700: Loss = 1.588778\n",
      "Step 1800: Loss = 144.417465\n",
      "Step 1900: Loss = 9.884932\n",
      "Step 2000: Loss = 303.800018\n",
      "Step 2100: Loss = 11.781997\n",
      "Step 2200: Loss = 237.909363\n",
      "Step 2300: Loss = 4.006358\n",
      "Step 2400: Loss = 188.737305\n",
      "Step 2500: Loss = 2.702240\n",
      "Step 2600: Loss = 98.049553\n",
      "Step 2700: Loss = 4.345465\n",
      "Step 2800: Loss = 292.690033\n",
      "Step 2900: Loss = 4.598831\n",
      "Step 3000: Loss = 174.561569\n",
      "Step 3100: Loss = 2.315958\n",
      "Step 3200: Loss = 116.882614\n",
      "Step 3300: Loss = 13.395789\n",
      "Step 3400: Loss = 77.469643\n",
      "Step 3500: Loss = 9.133254\n",
      "Step 3600: Loss = 287.020447\n",
      "Step 3700: Loss = 1.335115\n",
      "Step 3800: Loss = 52.978016\n",
      "Step 3900: Loss = 1.189034\n",
      "Step 4000: Loss = 0.766056\n",
      "Step 4100: Loss = 0.730238\n",
      "Step 4200: Loss = 0.703796\n",
      "Step 4300: Loss = 0.683372\n",
      "Step 4400: Loss = 0.667726\n",
      "Step 4500: Loss = 0.655702\n",
      "Step 4600: Loss = 0.646368\n",
      "Step 4700: Loss = 0.639006\n",
      "Step 4800: Loss = 0.633088\n",
      "Step 4900: Loss = 0.628228\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 2/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 175.645294\n",
      "Step 100: Loss = 62.815708\n",
      "Step 200: Loss = 173.618317\n",
      "Step 300: Loss = 23.727484\n",
      "Step 400: Loss = 742.293762\n",
      "Step 500: Loss = 64.379250\n",
      "Step 600: Loss = 106.067314\n",
      "Step 700: Loss = 23.271120\n",
      "Step 800: Loss = 64.839340\n",
      "Step 900: Loss = 11.598539\n",
      "Step 1000: Loss = 273.527771\n",
      "Step 1100: Loss = 11.261059\n",
      "Step 1200: Loss = 474.157196\n",
      "Step 1300: Loss = 18.140306\n",
      "Step 1400: Loss = 132.407104\n",
      "Step 1500: Loss = 3.206414\n",
      "Step 1600: Loss = 546.712219\n",
      "Step 1700: Loss = 17.318279\n",
      "Step 1800: Loss = 98.068199\n",
      "Step 1900: Loss = 1.378433\n",
      "Step 2000: Loss = 180.368256\n",
      "Step 2100: Loss = 1.041563\n",
      "Step 2200: Loss = 102.111259\n",
      "Step 2300: Loss = 3.377693\n",
      "Step 2400: Loss = 189.009048\n",
      "Step 2500: Loss = 10.738853\n",
      "Step 2600: Loss = 85.447998\n",
      "Step 2700: Loss = 5.604418\n",
      "Step 2800: Loss = 171.490646\n",
      "Step 2900: Loss = 1.235314\n",
      "Step 3000: Loss = 165.947617\n",
      "Step 3100: Loss = 2.196334\n",
      "Step 3200: Loss = 205.524460\n",
      "Step 3300: Loss = 2.945262\n",
      "Step 3400: Loss = 162.013626\n",
      "Step 3500: Loss = 5.815567\n",
      "Step 3600: Loss = 126.976997\n",
      "Step 3700: Loss = 2.123479\n",
      "Step 3800: Loss = 115.244263\n",
      "Step 3900: Loss = 2.507032\n",
      "Step 4000: Loss = 1.110777\n",
      "Step 4100: Loss = 0.857412\n",
      "Step 4200: Loss = 0.813647\n",
      "Step 4300: Loss = 0.790807\n",
      "Step 4400: Loss = 0.771647\n",
      "Step 4500: Loss = 0.754871\n",
      "Step 4600: Loss = 0.740065\n",
      "Step 4700: Loss = 0.726924\n",
      "Step 4800: Loss = 0.715203\n",
      "Step 4900: Loss = 0.704697\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 3/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 185.843140\n",
      "Step 100: Loss = 24.485559\n",
      "Step 200: Loss = 99.198334\n",
      "Step 300: Loss = 1.398135\n",
      "Step 400: Loss = 155.773758\n",
      "Step 500: Loss = 1.718041\n",
      "Step 600: Loss = 208.712891\n",
      "Step 700: Loss = 17.136938\n",
      "Step 800: Loss = 177.679199\n",
      "Step 900: Loss = 0.964521\n",
      "Step 1000: Loss = 171.581375\n",
      "Step 1100: Loss = 0.973718\n",
      "Step 1200: Loss = 484.750885\n",
      "Step 1300: Loss = 2.209874\n",
      "Step 1400: Loss = 238.874863\n",
      "Step 1500: Loss = 9.135946\n",
      "Step 1600: Loss = 171.510101\n",
      "Step 1700: Loss = 1.202414\n",
      "Step 1800: Loss = 156.559586\n",
      "Step 1900: Loss = 2.233135\n",
      "Step 2000: Loss = 169.671188\n",
      "Step 2100: Loss = 19.472219\n",
      "Step 2200: Loss = 201.064133\n",
      "Step 2300: Loss = 14.547396\n",
      "Step 2400: Loss = 334.072144\n",
      "Step 2500: Loss = 3.480882\n",
      "Step 2600: Loss = 117.304779\n",
      "Step 2700: Loss = 9.344660\n",
      "Step 2800: Loss = 154.150803\n",
      "Step 2900: Loss = 4.800441\n",
      "Step 3000: Loss = 278.946320\n",
      "Step 3100: Loss = 10.222591\n",
      "Step 3200: Loss = 159.833038\n",
      "Step 3300: Loss = 4.192715\n",
      "Step 3400: Loss = 71.338371\n",
      "Step 3500: Loss = 2.202258\n",
      "Step 3600: Loss = 169.535858\n",
      "Step 3700: Loss = 3.185705\n",
      "Step 3800: Loss = 161.553787\n",
      "Step 3900: Loss = 11.078068\n",
      "Step 4000: Loss = 9.593796\n",
      "Step 4100: Loss = 7.855916\n",
      "Step 4200: Loss = 3.373572\n",
      "Step 4300: Loss = 1.716990\n",
      "Step 4400: Loss = 1.627133\n",
      "Step 4500: Loss = 1.555525\n",
      "Step 4600: Loss = 1.495967\n",
      "Step 4700: Loss = 1.445361\n",
      "Step 4800: Loss = 1.401621\n",
      "Step 4900: Loss = 1.367394\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 4/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 196.525192\n",
      "Step 100: Loss = 36.854160\n",
      "Step 200: Loss = 264.084381\n",
      "Step 300: Loss = 12.443213\n",
      "Step 400: Loss = 155.168808\n",
      "Step 500: Loss = 1.051992\n",
      "Step 600: Loss = 167.974335\n",
      "Step 700: Loss = 1.319023\n",
      "Step 800: Loss = 108.645889\n",
      "Step 900: Loss = 4.146780\n",
      "Step 1000: Loss = 78.604324\n",
      "Step 1100: Loss = 1.712606\n",
      "Step 1200: Loss = 158.267242\n",
      "Step 1300: Loss = 0.672904\n",
      "Step 1400: Loss = 81.895866\n",
      "Step 1500: Loss = 0.658620\n",
      "Step 1600: Loss = 92.151970\n",
      "Step 1700: Loss = 0.925069\n",
      "Step 1800: Loss = 218.403458\n",
      "Step 1900: Loss = 0.994008\n",
      "Step 2000: Loss = 106.869011\n",
      "Step 2100: Loss = 0.759702\n",
      "Step 2200: Loss = 192.774963\n",
      "Step 2300: Loss = 1.261996\n",
      "Step 2400: Loss = 202.270248\n",
      "Step 2500: Loss = 0.947506\n",
      "Step 2600: Loss = 87.778847\n",
      "Step 2700: Loss = 1.294151\n",
      "Step 2800: Loss = 131.186951\n",
      "Step 2900: Loss = 0.925946\n",
      "Step 3000: Loss = 134.131027\n",
      "Step 3100: Loss = 0.765142\n",
      "Step 3200: Loss = 195.309692\n",
      "Step 3300: Loss = 1.173250\n",
      "Step 3400: Loss = 118.687653\n",
      "Step 3500: Loss = 0.738542\n",
      "Step 3600: Loss = 123.116333\n",
      "Step 3700: Loss = 0.918820\n",
      "Step 3800: Loss = 141.603638\n",
      "Step 3900: Loss = 0.848711\n",
      "Step 4000: Loss = 0.711113\n",
      "Step 4100: Loss = 0.686372\n",
      "Step 4200: Loss = 0.668518\n",
      "Step 4300: Loss = 0.654476\n",
      "Step 4400: Loss = 0.643112\n",
      "Step 4500: Loss = 0.633735\n",
      "Step 4600: Loss = 0.625883\n",
      "Step 4700: Loss = 0.619228\n",
      "Step 4800: Loss = 0.613527\n",
      "Step 4900: Loss = 0.608596\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 5/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 174.694550\n",
      "Step 100: Loss = 50.273540\n",
      "Step 200: Loss = 74.755241\n",
      "Step 300: Loss = 2.287087\n",
      "Step 400: Loss = 314.366425\n",
      "Step 500: Loss = 20.266108\n",
      "Step 600: Loss = 172.940735\n",
      "Step 700: Loss = 2.258540\n",
      "Step 800: Loss = 222.395416\n",
      "Step 900: Loss = 4.460944\n",
      "Step 1000: Loss = 357.342804\n",
      "Step 1100: Loss = 12.119673\n",
      "Step 1200: Loss = 141.272720\n",
      "Step 1300: Loss = 2.784575\n",
      "Step 1400: Loss = 99.554451\n",
      "Step 1500: Loss = 1.299572\n",
      "Step 1600: Loss = 369.934662\n",
      "Step 1700: Loss = 1.356207\n",
      "Step 1800: Loss = 165.397446\n",
      "Step 1900: Loss = 1.431205\n",
      "Step 2000: Loss = 84.765617\n",
      "Step 2100: Loss = 2.746039\n",
      "Step 2200: Loss = 129.738159\n",
      "Step 2300: Loss = 2.060176\n",
      "Step 2400: Loss = 50.052765\n",
      "Step 2500: Loss = 1.501597\n",
      "Step 2600: Loss = 145.903854\n",
      "Step 2700: Loss = 1.589752\n",
      "Step 2800: Loss = 160.872314\n",
      "Step 2900: Loss = 1.564600\n",
      "Step 3000: Loss = 148.985077\n",
      "Step 3100: Loss = 12.290296\n",
      "Step 3200: Loss = 112.156883\n",
      "Step 3300: Loss = 1.371167\n",
      "Step 3400: Loss = 176.619308\n",
      "Step 3500: Loss = 1.385854\n",
      "Step 3600: Loss = 278.014557\n",
      "Step 3700: Loss = 2.900320\n",
      "Step 3800: Loss = 110.912437\n",
      "Step 3900: Loss = 0.940523\n",
      "Step 4000: Loss = 0.652396\n",
      "Step 4100: Loss = 0.599345\n",
      "Step 4200: Loss = 0.585576\n",
      "Step 4300: Loss = 0.577943\n",
      "Step 4400: Loss = 0.571873\n",
      "Step 4500: Loss = 0.566694\n",
      "Step 4600: Loss = 0.562201\n",
      "Step 4700: Loss = 0.558268\n",
      "Step 4800: Loss = 0.554798\n",
      "Step 4900: Loss = 0.551708\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "  Resampling ratio η = 0.6\n",
      "    Repeat 1/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 203.047745\n",
      "Step 100: Loss = 25.058626\n",
      "Step 200: Loss = 372.476166\n",
      "Step 300: Loss = 11.881050\n",
      "Step 400: Loss = 279.721771\n",
      "Step 500: Loss = 2.928189\n",
      "Step 600: Loss = 372.610413\n",
      "Step 700: Loss = 52.211891\n",
      "Step 800: Loss = 278.424103\n",
      "Step 900: Loss = 20.638607\n",
      "Step 1000: Loss = 302.094940\n",
      "Step 1100: Loss = 19.228580\n",
      "Step 1200: Loss = 372.302094\n",
      "Step 1300: Loss = 9.817234\n",
      "Step 1400: Loss = 274.847168\n",
      "Step 1500: Loss = 10.519687\n",
      "Step 1600: Loss = 435.169769\n",
      "Step 1700: Loss = 16.852720\n",
      "Step 1800: Loss = 954.295715\n",
      "Step 1900: Loss = 88.714546\n",
      "Step 2000: Loss = 632.035522\n",
      "Step 2100: Loss = 90.847954\n",
      "Step 2200: Loss = 336.273773\n",
      "Step 2300: Loss = 81.895866\n",
      "Step 2400: Loss = 400.584869\n",
      "Step 2500: Loss = 57.874626\n",
      "Step 2600: Loss = 253.569199\n",
      "Step 2700: Loss = 42.520554\n",
      "Step 2800: Loss = 282.705536\n",
      "Step 2900: Loss = 23.148352\n",
      "Step 3000: Loss = 499.648010\n",
      "Step 3100: Loss = 33.011574\n",
      "Step 3200: Loss = 272.746552\n",
      "Step 3300: Loss = 34.388519\n",
      "Step 3400: Loss = 287.138702\n",
      "Step 3500: Loss = 22.736206\n",
      "Step 3600: Loss = 330.900848\n",
      "Step 3700: Loss = 12.845686\n",
      "Step 3800: Loss = 659.990845\n",
      "Step 3900: Loss = 16.094954\n",
      "Step 4000: Loss = 8.045411\n",
      "Step 4100: Loss = 6.860633\n",
      "Step 4200: Loss = 6.384048\n",
      "Step 4300: Loss = 6.072175\n",
      "Step 4400: Loss = 5.823049\n",
      "Step 4500: Loss = 5.606383\n",
      "Step 4600: Loss = 5.411203\n",
      "Step 4700: Loss = 5.232553\n",
      "Step 4800: Loss = 5.067675\n",
      "Step 4900: Loss = 4.914868\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 2/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 186.912201\n",
      "Step 100: Loss = 45.347031\n",
      "Step 200: Loss = 327.980011\n",
      "Step 300: Loss = 6.199498\n",
      "Step 400: Loss = 282.278320\n",
      "Step 500: Loss = 3.786229\n",
      "Step 600: Loss = 380.099335\n",
      "Step 700: Loss = 5.192389\n",
      "Step 800: Loss = 400.189697\n",
      "Step 900: Loss = 2.964450\n",
      "Step 1000: Loss = 247.362427\n",
      "Step 1100: Loss = 3.070075\n",
      "Step 1200: Loss = 413.126129\n",
      "Step 1300: Loss = 10.292794\n",
      "Step 1400: Loss = 413.756439\n",
      "Step 1500: Loss = 1.769915\n",
      "Step 1600: Loss = 286.795807\n",
      "Step 1700: Loss = 2.103662\n",
      "Step 1800: Loss = 162.693039\n",
      "Step 1900: Loss = 1.704981\n",
      "Step 2000: Loss = 433.534668\n",
      "Step 2100: Loss = 4.690743\n",
      "Step 2200: Loss = 339.381195\n",
      "Step 2300: Loss = 4.259679\n",
      "Step 2400: Loss = 554.961487\n",
      "Step 2500: Loss = 4.185044\n",
      "Step 2600: Loss = 148.740814\n",
      "Step 2700: Loss = 5.397049\n",
      "Step 2800: Loss = 641.048096\n",
      "Step 2900: Loss = 7.050653\n",
      "Step 3000: Loss = 272.982758\n",
      "Step 3100: Loss = 12.073856\n",
      "Step 3200: Loss = 476.712433\n",
      "Step 3300: Loss = 0.813393\n",
      "Step 3400: Loss = 104.483643\n",
      "Step 3500: Loss = 2.274560\n",
      "Step 3600: Loss = 132.860474\n",
      "Step 3700: Loss = 2.926500\n",
      "Step 3800: Loss = 343.402008\n",
      "Step 3900: Loss = 6.138034\n",
      "Step 4000: Loss = 1.386842\n",
      "Step 4100: Loss = 0.860414\n",
      "Step 4200: Loss = 0.779153\n",
      "Step 4300: Loss = 0.742238\n",
      "Step 4400: Loss = 0.716531\n",
      "Step 4500: Loss = 0.696376\n",
      "Step 4600: Loss = 0.679904\n",
      "Step 4700: Loss = 0.666121\n",
      "Step 4800: Loss = 0.654388\n",
      "Step 4900: Loss = 0.644264\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 3/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 155.919968\n",
      "Step 100: Loss = 28.805939\n",
      "Step 200: Loss = 202.649887\n",
      "Step 300: Loss = 1.991320\n",
      "Step 400: Loss = 193.031189\n",
      "Step 500: Loss = 0.897936\n",
      "Step 600: Loss = 329.343658\n",
      "Step 700: Loss = 1.649928\n",
      "Step 800: Loss = 847.756287\n",
      "Step 900: Loss = 12.762946\n",
      "Step 1000: Loss = 349.317719\n",
      "Step 1100: Loss = 3.605139\n",
      "Step 1200: Loss = 219.083847\n",
      "Step 1300: Loss = 2.213792\n",
      "Step 1400: Loss = 323.654816\n",
      "Step 1500: Loss = 11.583152\n",
      "Step 1600: Loss = 834.003418\n",
      "Step 1700: Loss = 33.533070\n",
      "Step 1800: Loss = 444.149872\n",
      "Step 1900: Loss = 4.259197\n",
      "Step 2000: Loss = 321.177124\n",
      "Step 2100: Loss = 14.497465\n",
      "Step 2200: Loss = 242.988739\n",
      "Step 2300: Loss = 1.778429\n",
      "Step 2400: Loss = 384.043457\n",
      "Step 2500: Loss = 2.592560\n",
      "Step 2600: Loss = 125.946007\n",
      "Step 2700: Loss = 1.711873\n",
      "Step 2800: Loss = 557.970825\n",
      "Step 2900: Loss = 3.962084\n",
      "Step 3000: Loss = 268.799652\n",
      "Step 3100: Loss = 2.893391\n",
      "Step 3200: Loss = 109.608482\n",
      "Step 3300: Loss = 3.059618\n",
      "Step 3400: Loss = 244.849319\n",
      "Step 3500: Loss = 2.885405\n",
      "Step 3600: Loss = 178.504501\n",
      "Step 3700: Loss = 2.591905\n",
      "Step 3800: Loss = 257.867920\n",
      "Step 3900: Loss = 5.589067\n",
      "Step 4000: Loss = 0.932737\n",
      "Step 4100: Loss = 0.782181\n",
      "Step 4200: Loss = 0.769452\n",
      "Step 4300: Loss = 0.759593\n",
      "Step 4400: Loss = 0.750765\n",
      "Step 4500: Loss = 0.742550\n",
      "Step 4600: Loss = 0.734810\n",
      "Step 4700: Loss = 0.727478\n",
      "Step 4800: Loss = 0.720506\n",
      "Step 4900: Loss = 0.713858\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 4/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 205.075989\n",
      "Step 100: Loss = 70.448105\n",
      "Step 200: Loss = 309.897644\n",
      "Step 300: Loss = 20.909313\n",
      "Step 400: Loss = 270.906006\n",
      "Step 500: Loss = 34.486771\n",
      "Step 600: Loss = 249.284149\n",
      "Step 700: Loss = 16.037668\n",
      "Step 800: Loss = 286.627502\n",
      "Step 900: Loss = 20.344997\n",
      "Step 1000: Loss = 509.964996\n",
      "Step 1100: Loss = 31.590656\n",
      "Step 1200: Loss = 247.294983\n",
      "Step 1300: Loss = 62.797359\n",
      "Step 1400: Loss = 3829.375000\n",
      "Step 1500: Loss = 136.511826\n",
      "Step 1600: Loss = 288.542175\n",
      "Step 1700: Loss = 55.570263\n",
      "Step 1800: Loss = 635.803223\n",
      "Step 1900: Loss = 43.469818\n",
      "Step 2000: Loss = 277.186157\n",
      "Step 2100: Loss = 49.163296\n",
      "Step 2200: Loss = 376.328522\n",
      "Step 2300: Loss = 31.600452\n",
      "Step 2400: Loss = 959.349976\n",
      "Step 2500: Loss = 30.787203\n",
      "Step 2600: Loss = 272.209229\n",
      "Step 2700: Loss = 27.905315\n",
      "Step 2800: Loss = 246.624298\n",
      "Step 2900: Loss = 8.585543\n",
      "Step 3000: Loss = 477.148560\n",
      "Step 3100: Loss = 10.725224\n",
      "Step 3200: Loss = 237.291763\n",
      "Step 3300: Loss = 8.634708\n",
      "Step 3400: Loss = 203.346191\n",
      "Step 3500: Loss = 8.466366\n",
      "Step 3600: Loss = 395.853088\n",
      "Step 3700: Loss = 10.973505\n",
      "Step 3800: Loss = 504.017059\n",
      "Step 3900: Loss = 5.701290\n",
      "Step 4000: Loss = 1.613534\n",
      "Step 4100: Loss = 1.102301\n",
      "Step 4200: Loss = 0.992249\n",
      "Step 4300: Loss = 0.933891\n",
      "Step 4400: Loss = 0.890434\n",
      "Step 4500: Loss = 0.855664\n",
      "Step 4600: Loss = 0.827085\n",
      "Step 4700: Loss = 0.803118\n",
      "Step 4800: Loss = 0.782658\n",
      "Step 4900: Loss = 0.764920\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 5/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 260.844177\n",
      "Step 100: Loss = 37.142574\n",
      "Step 200: Loss = 608.709717\n",
      "Step 300: Loss = 46.090179\n",
      "Step 400: Loss = 383.857574\n",
      "Step 500: Loss = 15.947148\n",
      "Step 600: Loss = 238.330017\n",
      "Step 700: Loss = 13.580471\n",
      "Step 800: Loss = 965.274963\n",
      "Step 900: Loss = 13.326706\n",
      "Step 1000: Loss = 463.479858\n",
      "Step 1100: Loss = 45.860275\n",
      "Step 1200: Loss = 226.752502\n",
      "Step 1300: Loss = 9.287043\n",
      "Step 1400: Loss = 413.012390\n",
      "Step 1500: Loss = 19.528103\n",
      "Step 1600: Loss = 420.181488\n",
      "Step 1700: Loss = 10.779160\n",
      "Step 1800: Loss = 121.796638\n",
      "Step 1900: Loss = 11.220158\n",
      "Step 2000: Loss = 558.198914\n",
      "Step 2100: Loss = 5.594831\n",
      "Step 2200: Loss = 226.475311\n",
      "Step 2300: Loss = 6.342405\n",
      "Step 2400: Loss = 191.407776\n",
      "Step 2500: Loss = 14.922489\n",
      "Step 2600: Loss = 430.252747\n",
      "Step 2700: Loss = 6.135293\n",
      "Step 2800: Loss = 292.905243\n",
      "Step 2900: Loss = 27.499594\n",
      "Step 3000: Loss = 345.115601\n",
      "Step 3100: Loss = 18.997286\n",
      "Step 3200: Loss = 513.469299\n",
      "Step 3300: Loss = 24.061039\n",
      "Step 3400: Loss = 303.638458\n",
      "Step 3500: Loss = 19.453697\n",
      "Step 3600: Loss = 271.954254\n",
      "Step 3700: Loss = 20.010185\n",
      "Step 3800: Loss = 433.426727\n",
      "Step 3900: Loss = 11.306328\n",
      "Step 4000: Loss = 8.964564\n",
      "Step 4100: Loss = 7.691884\n",
      "Step 4200: Loss = 6.764053\n",
      "Step 4300: Loss = 6.429840\n",
      "Step 4400: Loss = 6.155244\n",
      "Step 4500: Loss = 5.918435\n",
      "Step 4600: Loss = 5.709985\n",
      "Step 4700: Loss = 5.523676\n",
      "Step 4800: Loss = 5.355348\n",
      "Step 4900: Loss = 5.202055\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "  Resampling ratio η = 0.9\n",
      "    Repeat 1/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 199.968567\n",
      "Step 100: Loss = 80.108391\n",
      "Step 200: Loss = 557.132507\n",
      "Step 300: Loss = 36.502419\n",
      "Step 400: Loss = 470.227570\n",
      "Step 500: Loss = 47.315758\n",
      "Step 600: Loss = 445.117828\n",
      "Step 700: Loss = 5.546801\n",
      "Step 800: Loss = 320.793213\n",
      "Step 900: Loss = 16.713898\n",
      "Step 1000: Loss = 141.672714\n",
      "Step 1100: Loss = 4.078698\n",
      "Step 1200: Loss = 161.727036\n",
      "Step 1300: Loss = 1.263201\n",
      "Step 1400: Loss = 115.154243\n",
      "Step 1500: Loss = 2.348867\n",
      "Step 1600: Loss = 212.535461\n",
      "Step 1700: Loss = 1.646492\n",
      "Step 1800: Loss = 53.899143\n",
      "Step 1900: Loss = 1.097365\n",
      "Step 2000: Loss = 51.287777\n",
      "Step 2100: Loss = 0.930999\n",
      "Step 2200: Loss = 16.063681\n",
      "Step 2300: Loss = 0.797545\n",
      "Step 2400: Loss = 66.907578\n",
      "Step 2500: Loss = 0.905337\n",
      "Step 2600: Loss = 56.777981\n",
      "Step 2700: Loss = 0.488879\n",
      "Step 2800: Loss = 19.053270\n",
      "Step 2900: Loss = 0.548303\n",
      "Step 3000: Loss = 44.273449\n",
      "Step 3100: Loss = 0.432750\n",
      "Step 3200: Loss = 20.838623\n",
      "Step 3300: Loss = 0.243741\n",
      "Step 3400: Loss = 23.821630\n",
      "Step 3500: Loss = 0.441318\n",
      "Step 3600: Loss = 19.926109\n",
      "Step 3700: Loss = 0.157477\n",
      "Step 3800: Loss = 7.889893\n",
      "Step 3900: Loss = 0.330422\n",
      "Step 4000: Loss = 0.125900\n",
      "Step 4100: Loss = 0.086999\n",
      "Step 4200: Loss = 0.073602\n",
      "Step 4300: Loss = 0.067285\n",
      "Step 4400: Loss = 0.063542\n",
      "Step 4500: Loss = 0.060898\n",
      "Step 4600: Loss = 0.058801\n",
      "Step 4700: Loss = 0.057025\n",
      "Step 4800: Loss = 0.055464\n",
      "Step 4900: Loss = 0.054063\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 2/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 186.113632\n",
      "Step 100: Loss = 80.974274\n",
      "Step 200: Loss = 808.752319\n",
      "Step 300: Loss = 95.211746\n",
      "Step 400: Loss = 226.371796\n",
      "Step 500: Loss = 50.550613\n",
      "Step 600: Loss = 1113.956421\n",
      "Step 700: Loss = 92.270454\n",
      "Step 800: Loss = 522.281555\n",
      "Step 900: Loss = 33.974419\n",
      "Step 1000: Loss = 513.817627\n",
      "Step 1100: Loss = 4.769065\n",
      "Step 1200: Loss = 564.877930\n",
      "Step 1300: Loss = 25.735577\n",
      "Step 1400: Loss = 303.978119\n",
      "Step 1500: Loss = 4.773468\n",
      "Step 1600: Loss = 520.419800\n",
      "Step 1700: Loss = 6.939182\n",
      "Step 1800: Loss = 463.013672\n",
      "Step 1900: Loss = 19.414068\n",
      "Step 2000: Loss = 385.375885\n",
      "Step 2100: Loss = 6.022647\n",
      "Step 2200: Loss = 375.380524\n",
      "Step 2300: Loss = 4.054567\n",
      "Step 2400: Loss = 395.710632\n",
      "Step 2500: Loss = 1.960790\n",
      "Step 2600: Loss = 299.203491\n",
      "Step 2700: Loss = 4.764431\n",
      "Step 2800: Loss = 202.042206\n",
      "Step 2900: Loss = 5.565835\n",
      "Step 3000: Loss = 431.067688\n",
      "Step 3100: Loss = 24.548265\n",
      "Step 3200: Loss = 344.893982\n",
      "Step 3300: Loss = 18.495085\n",
      "Step 3400: Loss = 272.639526\n",
      "Step 3500: Loss = 16.012320\n",
      "Step 3600: Loss = 452.022827\n",
      "Step 3700: Loss = 4.002766\n",
      "Step 3800: Loss = 290.731506\n",
      "Step 3900: Loss = 5.076362\n",
      "Step 4000: Loss = 0.932980\n",
      "Step 4100: Loss = 0.776099\n",
      "Step 4200: Loss = 0.742522\n",
      "Step 4300: Loss = 0.725429\n",
      "Step 4400: Loss = 0.713185\n",
      "Step 4500: Loss = 0.703216\n",
      "Step 4600: Loss = 0.694554\n",
      "Step 4700: Loss = 0.686760\n",
      "Step 4800: Loss = 0.679611\n",
      "Step 4900: Loss = 0.672984\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 3/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 174.910355\n",
      "Step 100: Loss = 40.227074\n",
      "Step 200: Loss = 1089.132812\n",
      "Step 300: Loss = 81.597343\n",
      "Step 400: Loss = 312.664886\n",
      "Step 500: Loss = 40.222996\n",
      "Step 600: Loss = 627.201233\n",
      "Step 700: Loss = 56.671139\n",
      "Step 800: Loss = 465.970032\n",
      "Step 900: Loss = 7.510331\n",
      "Step 1000: Loss = 925.255737\n",
      "Step 1100: Loss = 82.943085\n",
      "Step 1200: Loss = 293.309052\n",
      "Step 1300: Loss = 4.728501\n",
      "Step 1400: Loss = 579.656677\n",
      "Step 1500: Loss = 10.073380\n",
      "Step 1600: Loss = 631.869324\n",
      "Step 1700: Loss = 59.377571\n",
      "Step 1800: Loss = 592.270264\n",
      "Step 1900: Loss = 29.739969\n",
      "Step 2000: Loss = 924.305969\n",
      "Step 2100: Loss = 73.813293\n",
      "Step 2200: Loss = 469.625763\n",
      "Step 2300: Loss = 11.418733\n",
      "Step 2400: Loss = 265.566925\n",
      "Step 2500: Loss = 10.976345\n",
      "Step 2600: Loss = 403.483734\n",
      "Step 2700: Loss = 16.600046\n",
      "Step 2800: Loss = 202.394928\n",
      "Step 2900: Loss = 27.633013\n",
      "Step 3000: Loss = 235.456802\n",
      "Step 3100: Loss = 16.569130\n",
      "Step 3200: Loss = 646.024292\n",
      "Step 3300: Loss = 14.285901\n",
      "Step 3400: Loss = 482.405792\n",
      "Step 3500: Loss = 8.708276\n",
      "Step 3600: Loss = 480.335052\n",
      "Step 3700: Loss = 7.582650\n",
      "Step 3800: Loss = 317.844360\n",
      "Step 3900: Loss = 4.394527\n",
      "Step 4000: Loss = 1.895117\n",
      "Step 4100: Loss = 1.239018\n",
      "Step 4200: Loss = 1.043171\n",
      "Step 4300: Loss = 0.970962\n",
      "Step 4400: Loss = 0.925847\n",
      "Step 4500: Loss = 0.889769\n",
      "Step 4600: Loss = 0.858936\n",
      "Step 4700: Loss = 0.832022\n",
      "Step 4800: Loss = 0.808391\n",
      "Step 4900: Loss = 0.787679\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 4/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 267.304169\n",
      "Step 100: Loss = 101.095200\n",
      "Step 200: Loss = 258.164093\n",
      "Step 300: Loss = 26.067127\n",
      "Step 400: Loss = 448.401947\n",
      "Step 500: Loss = 50.477642\n",
      "Step 600: Loss = 800.745483\n",
      "Step 700: Loss = 36.724461\n",
      "Step 800: Loss = 362.935242\n",
      "Step 900: Loss = 74.049103\n",
      "Step 1000: Loss = 689.586792\n",
      "Step 1100: Loss = 62.450626\n",
      "Step 1200: Loss = 397.935577\n",
      "Step 1300: Loss = 91.722229\n",
      "Step 1400: Loss = 375.979706\n",
      "Step 1500: Loss = 15.497956\n",
      "Step 1600: Loss = 412.103271\n",
      "Step 1700: Loss = 18.290808\n",
      "Step 1800: Loss = 1163.205566\n",
      "Step 1900: Loss = 40.099541\n",
      "Step 2000: Loss = 619.570129\n",
      "Step 2100: Loss = 32.064068\n",
      "Step 2200: Loss = 308.467194\n",
      "Step 2300: Loss = 54.889992\n",
      "Step 2400: Loss = 494.553650\n",
      "Step 2500: Loss = 26.097260\n",
      "Step 2600: Loss = 671.778259\n",
      "Step 2700: Loss = 18.871300\n",
      "Step 2800: Loss = 474.102203\n",
      "Step 2900: Loss = 21.153595\n",
      "Step 3000: Loss = 583.481628\n",
      "Step 3100: Loss = 8.573145\n",
      "Step 3200: Loss = 373.947510\n",
      "Step 3300: Loss = 24.298470\n",
      "Step 3400: Loss = 347.600433\n",
      "Step 3500: Loss = 25.074276\n",
      "Step 3600: Loss = 446.650513\n",
      "Step 3700: Loss = 14.289262\n",
      "Step 3800: Loss = 370.687408\n",
      "Step 3900: Loss = 17.554987\n",
      "Step 4000: Loss = 3.292440\n",
      "Step 4100: Loss = 2.248532\n",
      "Step 4200: Loss = 2.043182\n",
      "Step 4300: Loss = 1.920695\n",
      "Step 4400: Loss = 1.828303\n",
      "Step 4500: Loss = 1.753181\n",
      "Step 4600: Loss = 1.689313\n",
      "Step 4700: Loss = 1.633358\n",
      "Step 4800: Loss = 1.583411\n",
      "Step 4900: Loss = 1.538343\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 5/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 241.894302\n",
      "Step 100: Loss = 41.062187\n",
      "Step 200: Loss = 881.857178\n",
      "Step 300: Loss = 37.393131\n",
      "Step 400: Loss = 404.470032\n",
      "Step 500: Loss = 52.013012\n",
      "Step 600: Loss = 416.370026\n",
      "Step 700: Loss = 49.766396\n",
      "Step 800: Loss = 312.131409\n",
      "Step 900: Loss = 50.961678\n",
      "Step 1000: Loss = 510.531464\n",
      "Step 1100: Loss = 36.955105\n",
      "Step 1200: Loss = 816.593079\n",
      "Step 1300: Loss = 97.100510\n",
      "Step 1400: Loss = 466.227264\n",
      "Step 1500: Loss = 38.340271\n",
      "Step 1600: Loss = 354.257507\n",
      "Step 1700: Loss = 10.788551\n",
      "Step 1800: Loss = 389.723419\n",
      "Step 1900: Loss = 21.781380\n",
      "Step 2000: Loss = 557.589905\n",
      "Step 2100: Loss = 12.930422\n",
      "Step 2200: Loss = 337.217041\n",
      "Step 2300: Loss = 13.160405\n",
      "Step 2400: Loss = 663.036255\n",
      "Step 2500: Loss = 16.293541\n",
      "Step 2600: Loss = 2287.339355\n",
      "Step 2700: Loss = 107.201469\n",
      "Step 2800: Loss = 421.681396\n",
      "Step 2900: Loss = 25.579239\n",
      "Step 3000: Loss = 357.071411\n",
      "Step 3100: Loss = 9.974343\n",
      "Step 3200: Loss = 907.350647\n",
      "Step 3300: Loss = 80.842690\n",
      "Step 3400: Loss = 366.196533\n",
      "Step 3500: Loss = 15.013762\n",
      "Step 3600: Loss = 488.186218\n",
      "Step 3700: Loss = 10.011606\n",
      "Step 3800: Loss = 416.079498\n",
      "Step 3900: Loss = 19.853065\n",
      "Step 4000: Loss = 5.781565\n",
      "Step 4100: Loss = 2.509246\n",
      "Step 4200: Loss = 1.520862\n",
      "Step 4300: Loss = 1.256631\n",
      "Step 4400: Loss = 1.140976\n",
      "Step 4500: Loss = 1.063154\n",
      "Step 4600: Loss = 1.002019\n",
      "Step 4700: Loss = 0.951497\n",
      "Step 4800: Loss = 0.908924\n",
      "Step 4900: Loss = 0.872738\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "\n",
      "Running experiments with 50 residual points\n",
      "  Resampling ratio η = 0.0\n",
      "    Repeat 1/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 156.938232\n",
      "Step 100: Loss = 55.227715\n",
      "Step 200: Loss = 9.756886\n",
      "Step 300: Loss = 3.143152\n",
      "Step 400: Loss = 2.270320\n",
      "Step 500: Loss = 1.625364\n",
      "Step 600: Loss = 1.263377\n",
      "Step 700: Loss = 1.098909\n",
      "Step 800: Loss = 0.995156\n",
      "Step 900: Loss = 0.883296\n",
      "Step 1000: Loss = 0.877299\n",
      "Step 1100: Loss = 0.756464\n",
      "Step 1200: Loss = 0.715201\n",
      "Step 1300: Loss = 0.686345\n",
      "Step 1400: Loss = 0.661924\n",
      "Step 1500: Loss = 0.643441\n",
      "Step 1600: Loss = 0.676888\n",
      "Step 1700: Loss = 0.618180\n",
      "Step 1800: Loss = 0.608579\n",
      "Step 1900: Loss = 0.600572\n",
      "Step 2000: Loss = 0.605032\n",
      "Step 2100: Loss = 0.588403\n",
      "Step 2200: Loss = 0.583271\n",
      "Step 2300: Loss = 0.596060\n",
      "Step 2400: Loss = 0.575013\n",
      "Step 2500: Loss = 0.571287\n",
      "Step 2600: Loss = 0.567867\n",
      "Step 2700: Loss = 0.564697\n",
      "Step 2800: Loss = 0.662800\n",
      "Step 2900: Loss = 0.559210\n",
      "Step 3000: Loss = 0.556609\n",
      "Step 3100: Loss = 0.554150\n",
      "Step 3200: Loss = 0.551810\n",
      "Step 3300: Loss = 0.549576\n",
      "Step 3400: Loss = 0.547461\n",
      "Step 3500: Loss = 0.545846\n",
      "Step 3600: Loss = 0.543554\n",
      "Step 3700: Loss = 0.541692\n",
      "Step 3800: Loss = 0.539899\n",
      "Step 3900: Loss = 0.538297\n",
      "Step 4000: Loss = 0.536550\n",
      "Step 4100: Loss = 0.538670\n",
      "Step 4200: Loss = 0.533503\n",
      "Step 4300: Loss = 0.532049\n",
      "Step 4400: Loss = 0.552314\n",
      "Step 4500: Loss = 0.529366\n",
      "Step 4600: Loss = 0.528346\n",
      "Step 4700: Loss = 0.526795\n",
      "Step 4800: Loss = 0.526010\n",
      "Step 4900: Loss = 0.524533\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 2/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 160.073822\n",
      "Step 100: Loss = 70.902260\n",
      "Step 200: Loss = 8.360583\n",
      "Step 300: Loss = 3.491218\n",
      "Step 400: Loss = 1.230818\n",
      "Step 500: Loss = 0.813635\n",
      "Step 600: Loss = 0.996815\n",
      "Step 700: Loss = 0.847016\n",
      "Step 800: Loss = 0.862438\n",
      "Step 900: Loss = 0.767843\n",
      "Step 1000: Loss = 0.790909\n",
      "Step 1100: Loss = 0.645484\n",
      "Step 1200: Loss = 0.729219\n",
      "Step 1300: Loss = 0.633329\n",
      "Step 1400: Loss = 0.630931\n",
      "Step 1500: Loss = 0.622962\n",
      "Step 1600: Loss = 0.640830\n",
      "Step 1700: Loss = 1.247291\n",
      "Step 1800: Loss = 0.610836\n",
      "Step 1900: Loss = 0.606910\n",
      "Step 2000: Loss = 0.611025\n",
      "Step 2100: Loss = 0.600578\n",
      "Step 2200: Loss = 0.623731\n",
      "Step 2300: Loss = 0.969886\n",
      "Step 2400: Loss = 0.592634\n",
      "Step 2500: Loss = 0.590130\n",
      "Step 2600: Loss = 0.588451\n",
      "Step 2700: Loss = 0.586079\n",
      "Step 2800: Loss = 0.583988\n",
      "Step 2900: Loss = 0.582021\n",
      "Step 3000: Loss = 1.121382\n",
      "Step 3100: Loss = 0.578688\n",
      "Step 3200: Loss = 0.576952\n",
      "Step 3300: Loss = 0.575307\n",
      "Step 3400: Loss = 0.573720\n",
      "Step 3500: Loss = 0.674099\n",
      "Step 3600: Loss = 0.570876\n",
      "Step 3700: Loss = 0.569430\n",
      "Step 3800: Loss = 0.568025\n",
      "Step 3900: Loss = 0.566676\n",
      "Step 4000: Loss = 0.565500\n",
      "Step 4100: Loss = 0.564138\n",
      "Step 4200: Loss = 0.562856\n",
      "Step 4300: Loss = 0.561591\n",
      "Step 4400: Loss = 0.560517\n",
      "Step 4500: Loss = 0.559231\n",
      "Step 4600: Loss = 0.557968\n",
      "Step 4700: Loss = 0.556773\n",
      "Step 4800: Loss = 0.559748\n",
      "Step 4900: Loss = 0.554559\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 3/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 168.770416\n",
      "Step 100: Loss = 73.667961\n",
      "Step 200: Loss = 4.712274\n",
      "Step 300: Loss = 1.107515\n",
      "Step 400: Loss = 0.904679\n",
      "Step 500: Loss = 0.818022\n",
      "Step 600: Loss = 0.767104\n",
      "Step 700: Loss = 0.738074\n",
      "Step 800: Loss = 0.723265\n",
      "Step 900: Loss = 0.700475\n",
      "Step 1000: Loss = 0.686895\n",
      "Step 1100: Loss = 0.675157\n",
      "Step 1200: Loss = 0.666995\n",
      "Step 1300: Loss = 0.655526\n",
      "Step 1400: Loss = 0.647132\n",
      "Step 1500: Loss = 0.639459\n",
      "Step 1600: Loss = 0.749043\n",
      "Step 1700: Loss = 0.625976\n",
      "Step 1800: Loss = 0.620051\n",
      "Step 1900: Loss = 0.614535\n",
      "Step 2000: Loss = 0.609381\n",
      "Step 2100: Loss = 0.604956\n",
      "Step 2200: Loss = 0.600202\n",
      "Step 2300: Loss = 0.596751\n",
      "Step 2400: Loss = 0.592206\n",
      "Step 2500: Loss = 0.588450\n",
      "Step 2600: Loss = 0.586043\n",
      "Step 2700: Loss = 0.581846\n",
      "Step 2800: Loss = 0.578766\n",
      "Step 2900: Loss = 0.576047\n",
      "Step 3000: Loss = 0.573102\n",
      "Step 3100: Loss = 0.570463\n",
      "Step 3200: Loss = 0.568196\n",
      "Step 3300: Loss = 0.565641\n",
      "Step 3400: Loss = 0.587483\n",
      "Step 3500: Loss = 0.561229\n",
      "Step 3600: Loss = 0.559149\n",
      "Step 3700: Loss = 0.559237\n",
      "Step 3800: Loss = 0.555302\n",
      "Step 3900: Loss = 0.553661\n",
      "Step 4000: Loss = 0.551743\n",
      "Step 4100: Loss = 0.550482\n",
      "Step 4200: Loss = 0.548510\n",
      "Step 4300: Loss = 0.546946\n",
      "Step 4400: Loss = 0.545419\n",
      "Step 4500: Loss = 0.570096\n",
      "Step 4600: Loss = 0.542591\n",
      "Step 4700: Loss = 0.541456\n",
      "Step 4800: Loss = 0.539932\n",
      "Step 4900: Loss = 0.539934\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 4/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 143.910812\n",
      "Step 100: Loss = 40.725231\n",
      "Step 200: Loss = 12.254953\n",
      "Step 300: Loss = 1.311130\n",
      "Step 400: Loss = 0.917981\n",
      "Step 500: Loss = 0.803190\n",
      "Step 600: Loss = 0.753175\n",
      "Step 700: Loss = 0.718747\n",
      "Step 800: Loss = 0.690947\n",
      "Step 900: Loss = 0.667784\n",
      "Step 1000: Loss = 0.648092\n",
      "Step 1100: Loss = 0.631746\n",
      "Step 1200: Loss = 0.617860\n",
      "Step 1300: Loss = 0.612881\n",
      "Step 1400: Loss = 0.595807\n",
      "Step 1500: Loss = 0.587228\n",
      "Step 1600: Loss = 0.579805\n",
      "Step 1700: Loss = 0.573330\n",
      "Step 1800: Loss = 0.567630\n",
      "Step 1900: Loss = 0.562644\n",
      "Step 2000: Loss = 0.558197\n",
      "Step 2100: Loss = 0.553951\n",
      "Step 2200: Loss = 0.550275\n",
      "Step 2300: Loss = 0.546896\n",
      "Step 2400: Loss = 0.543801\n",
      "Step 2500: Loss = 0.540948\n",
      "Step 2600: Loss = 0.538220\n",
      "Step 2700: Loss = 0.535718\n",
      "Step 2800: Loss = 0.533355\n",
      "Step 2900: Loss = 0.531189\n",
      "Step 3000: Loss = 0.529102\n",
      "Step 3100: Loss = 0.527024\n",
      "Step 3200: Loss = 0.525129\n",
      "Step 3300: Loss = 0.523317\n",
      "Step 3400: Loss = 0.523013\n",
      "Step 3500: Loss = 0.551877\n",
      "Step 3600: Loss = 0.518359\n",
      "Step 3700: Loss = 0.516856\n",
      "Step 3800: Loss = 0.515409\n",
      "Step 3900: Loss = 0.514022\n",
      "Step 4000: Loss = 0.512733\n",
      "Step 4100: Loss = 0.511426\n",
      "Step 4200: Loss = 0.510200\n",
      "Step 4300: Loss = 0.509011\n",
      "Step 4400: Loss = 0.509570\n",
      "Step 4500: Loss = 0.506800\n",
      "Step 4600: Loss = 0.505752\n",
      "Step 4700: Loss = 0.504731\n",
      "Step 4800: Loss = 0.503736\n",
      "Step 4900: Loss = 0.503665\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 5/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 189.087189\n",
      "Step 100: Loss = 88.803757\n",
      "Step 200: Loss = 14.942370\n",
      "Step 300: Loss = 3.004259\n",
      "Step 400: Loss = 1.959190\n",
      "Step 500: Loss = 2.499436\n",
      "Step 600: Loss = 1.237806\n",
      "Step 700: Loss = 0.990890\n",
      "Step 800: Loss = 0.896654\n",
      "Step 900: Loss = 0.898419\n",
      "Step 1000: Loss = 1.661028\n",
      "Step 1100: Loss = 0.773887\n",
      "Step 1200: Loss = 0.852078\n",
      "Step 1300: Loss = 0.726754\n",
      "Step 1400: Loss = 0.709864\n",
      "Step 1500: Loss = 0.736620\n",
      "Step 1600: Loss = 0.793704\n",
      "Step 1700: Loss = 0.691371\n",
      "Step 1800: Loss = 0.658270\n",
      "Step 1900: Loss = 0.647357\n",
      "Step 2000: Loss = 0.635847\n",
      "Step 2100: Loss = 0.666764\n",
      "Step 2200: Loss = 0.807751\n",
      "Step 2300: Loss = 0.609893\n",
      "Step 2400: Loss = 0.602188\n",
      "Step 2500: Loss = 0.595779\n",
      "Step 2600: Loss = 0.596728\n",
      "Step 2700: Loss = 0.584960\n",
      "Step 2800: Loss = 0.578415\n",
      "Step 2900: Loss = 0.575827\n",
      "Step 3000: Loss = 0.569342\n",
      "Step 3100: Loss = 0.565269\n",
      "Step 3200: Loss = 0.562138\n",
      "Step 3300: Loss = 0.575093\n",
      "Step 3400: Loss = 0.557974\n",
      "Step 3500: Loss = 0.552360\n",
      "Step 3600: Loss = 0.548978\n",
      "Step 3700: Loss = 0.546786\n",
      "Step 3800: Loss = 0.544037\n",
      "Step 3900: Loss = 0.541759\n",
      "Step 4000: Loss = 0.587491\n",
      "Step 4100: Loss = 0.537757\n",
      "Step 4200: Loss = 0.535944\n",
      "Step 4300: Loss = 0.549707\n",
      "Step 4400: Loss = 0.593172\n",
      "Step 4500: Loss = 0.531103\n",
      "Step 4600: Loss = 0.529655\n",
      "Step 4700: Loss = 0.550638\n",
      "Step 4800: Loss = 0.527038\n",
      "Step 4900: Loss = 0.525813\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "  Resampling ratio η = 0.3\n",
      "    Repeat 1/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 158.673248\n",
      "Step 100: Loss = 45.298988\n",
      "Step 200: Loss = 175.418762\n",
      "Step 300: Loss = 6.813473\n",
      "Step 400: Loss = 275.521149\n",
      "Step 500: Loss = 4.672663\n",
      "Step 600: Loss = 170.851578\n",
      "Step 700: Loss = 1.331544\n",
      "Step 800: Loss = 145.526001\n",
      "Step 900: Loss = 2.373013\n",
      "Step 1000: Loss = 165.458481\n",
      "Step 1100: Loss = 10.231177\n",
      "Step 1200: Loss = 1190.966797\n",
      "Step 1300: Loss = 22.263779\n",
      "Step 1400: Loss = 108.257393\n",
      "Step 1500: Loss = 10.193687\n",
      "Step 1600: Loss = 380.992493\n",
      "Step 1700: Loss = 8.653388\n",
      "Step 1800: Loss = 166.762512\n",
      "Step 1900: Loss = 3.386143\n",
      "Step 2000: Loss = 113.593262\n",
      "Step 2100: Loss = 2.825352\n",
      "Step 2200: Loss = 120.253319\n",
      "Step 2300: Loss = 4.914794\n",
      "Step 2400: Loss = 101.419098\n",
      "Step 2500: Loss = 2.358916\n",
      "Step 2600: Loss = 86.218170\n",
      "Step 2700: Loss = 1.709225\n",
      "Step 2800: Loss = 130.687805\n",
      "Step 2900: Loss = 2.993955\n",
      "Step 3000: Loss = 285.281616\n",
      "Step 3100: Loss = 1.869400\n",
      "Step 3200: Loss = 172.615326\n",
      "Step 3300: Loss = 1.670718\n",
      "Step 3400: Loss = 87.985497\n",
      "Step 3500: Loss = 3.352899\n",
      "Step 3600: Loss = 166.966278\n",
      "Step 3700: Loss = 1.978981\n",
      "Step 3800: Loss = 82.667252\n",
      "Step 3900: Loss = 2.114341\n",
      "Step 4000: Loss = 1.234210\n",
      "Step 4100: Loss = 1.073962\n",
      "Step 4200: Loss = 1.020566\n",
      "Step 4300: Loss = 0.988969\n",
      "Step 4400: Loss = 0.964954\n",
      "Step 4500: Loss = 0.944654\n",
      "Step 4600: Loss = 0.926763\n",
      "Step 4700: Loss = 0.910685\n",
      "Step 4800: Loss = 0.896074\n",
      "Step 4900: Loss = 0.882690\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 2/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 181.266708\n",
      "Step 100: Loss = 78.258415\n",
      "Step 200: Loss = 164.978516\n",
      "Step 300: Loss = 1.695570\n",
      "Step 400: Loss = 188.670685\n",
      "Step 500: Loss = 6.603806\n",
      "Step 600: Loss = 132.788467\n",
      "Step 700: Loss = 1.736713\n",
      "Step 800: Loss = 191.209229\n",
      "Step 900: Loss = 1.679940\n",
      "Step 1000: Loss = 148.019379\n",
      "Step 1100: Loss = 1.995100\n",
      "Step 1200: Loss = 139.431641\n",
      "Step 1300: Loss = 1.503655\n",
      "Step 1400: Loss = 210.211166\n",
      "Step 1500: Loss = 5.773029\n",
      "Step 1600: Loss = 138.384583\n",
      "Step 1700: Loss = 1.730453\n",
      "Step 1800: Loss = 159.290176\n",
      "Step 1900: Loss = 1.376440\n",
      "Step 2000: Loss = 56.594875\n",
      "Step 2100: Loss = 2.530066\n",
      "Step 2200: Loss = 89.738182\n",
      "Step 2300: Loss = 1.502821\n",
      "Step 2400: Loss = 127.924835\n",
      "Step 2500: Loss = 1.156544\n",
      "Step 2600: Loss = 112.166626\n",
      "Step 2700: Loss = 4.718593\n",
      "Step 2800: Loss = 243.949310\n",
      "Step 2900: Loss = 1.651442\n",
      "Step 3000: Loss = 184.463028\n",
      "Step 3100: Loss = 1.312065\n",
      "Step 3200: Loss = 190.081848\n",
      "Step 3300: Loss = 2.752591\n",
      "Step 3400: Loss = 118.005783\n",
      "Step 3500: Loss = 3.220237\n",
      "Step 3600: Loss = 193.704956\n",
      "Step 3700: Loss = 3.789183\n",
      "Step 3800: Loss = 59.056801\n",
      "Step 3900: Loss = 1.024220\n",
      "Step 4000: Loss = 0.876621\n",
      "Step 4100: Loss = 0.820508\n",
      "Step 4200: Loss = 0.785437\n",
      "Step 4300: Loss = 0.758001\n",
      "Step 4400: Loss = 0.734443\n",
      "Step 4500: Loss = 0.713487\n",
      "Step 4600: Loss = 0.694532\n",
      "Step 4700: Loss = 0.677206\n",
      "Step 4800: Loss = 0.661246\n",
      "Step 4900: Loss = 0.646453\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 3/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 168.056335\n",
      "Step 100: Loss = 57.529537\n",
      "Step 200: Loss = 149.691528\n",
      "Step 300: Loss = 3.579535\n",
      "Step 400: Loss = 52.759594\n",
      "Step 500: Loss = 1.055145\n",
      "Step 600: Loss = 75.936951\n",
      "Step 700: Loss = 0.768509\n",
      "Step 800: Loss = 33.508064\n",
      "Step 900: Loss = 2.812244\n",
      "Step 1000: Loss = 77.253532\n",
      "Step 1100: Loss = 0.910484\n",
      "Step 1200: Loss = 36.008766\n",
      "Step 1300: Loss = 0.851229\n",
      "Step 1400: Loss = 72.253548\n",
      "Step 1500: Loss = 0.806224\n",
      "Step 1600: Loss = 81.340263\n",
      "Step 1700: Loss = 0.797293\n",
      "Step 1800: Loss = 117.816338\n",
      "Step 1900: Loss = 0.971273\n",
      "Step 2000: Loss = 26.784628\n",
      "Step 2100: Loss = 0.554977\n",
      "Step 2200: Loss = 45.170982\n",
      "Step 2300: Loss = 0.367544\n",
      "Step 2400: Loss = 43.913326\n",
      "Step 2500: Loss = 0.545697\n",
      "Step 2600: Loss = 25.639486\n",
      "Step 2700: Loss = 0.467986\n",
      "Step 2800: Loss = 19.204430\n",
      "Step 2900: Loss = 0.417986\n",
      "Step 3000: Loss = 22.197458\n",
      "Step 3100: Loss = 0.401046\n",
      "Step 3200: Loss = 20.718857\n",
      "Step 3300: Loss = 0.563517\n",
      "Step 3400: Loss = 51.018356\n",
      "Step 3500: Loss = 0.496359\n",
      "Step 3600: Loss = 38.807533\n",
      "Step 3700: Loss = 0.327836\n",
      "Step 3800: Loss = 18.788504\n",
      "Step 3900: Loss = 0.617995\n",
      "Step 4000: Loss = 0.452065\n",
      "Step 4100: Loss = 0.381540\n",
      "Step 4200: Loss = 0.340457\n",
      "Step 4300: Loss = 0.312652\n",
      "Step 4400: Loss = 0.291972\n",
      "Step 4500: Loss = 0.275614\n",
      "Step 4600: Loss = 0.262126\n",
      "Step 4700: Loss = 0.250676\n",
      "Step 4800: Loss = 0.240741\n",
      "Step 4900: Loss = 0.231973\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 4/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 218.545364\n",
      "Step 100: Loss = 87.738098\n",
      "Step 200: Loss = 217.500031\n",
      "Step 300: Loss = 6.935281\n",
      "Step 400: Loss = 1170.114380\n",
      "Step 500: Loss = 86.608322\n",
      "Step 600: Loss = 113.565094\n",
      "Step 700: Loss = 12.601879\n",
      "Step 800: Loss = 163.801559\n",
      "Step 900: Loss = 5.729466\n",
      "Step 1000: Loss = 197.848358\n",
      "Step 1100: Loss = 5.418116\n",
      "Step 1200: Loss = 137.097366\n",
      "Step 1300: Loss = 4.403883\n",
      "Step 1400: Loss = 448.115601\n",
      "Step 1500: Loss = 11.322864\n",
      "Step 1600: Loss = 90.108124\n",
      "Step 1700: Loss = 6.494232\n",
      "Step 1800: Loss = 143.635025\n",
      "Step 1900: Loss = 4.255050\n",
      "Step 2000: Loss = 153.854233\n",
      "Step 2100: Loss = 6.700050\n",
      "Step 2200: Loss = 93.255219\n",
      "Step 2300: Loss = 3.939751\n",
      "Step 2400: Loss = 145.250793\n",
      "Step 2500: Loss = 4.761045\n",
      "Step 2600: Loss = 255.802444\n",
      "Step 2700: Loss = 5.440513\n",
      "Step 2800: Loss = 153.113800\n",
      "Step 2900: Loss = 1.785160\n",
      "Step 3000: Loss = 234.279953\n",
      "Step 3100: Loss = 2.354895\n",
      "Step 3200: Loss = 326.219543\n",
      "Step 3300: Loss = 1.572132\n",
      "Step 3400: Loss = 108.605118\n",
      "Step 3500: Loss = 4.500072\n",
      "Step 3600: Loss = 166.463470\n",
      "Step 3700: Loss = 1.774876\n",
      "Step 3800: Loss = 175.964935\n",
      "Step 3900: Loss = 0.817197\n",
      "Step 4000: Loss = 0.721682\n",
      "Step 4100: Loss = 0.696730\n",
      "Step 4200: Loss = 0.681920\n",
      "Step 4300: Loss = 0.671820\n",
      "Step 4400: Loss = 0.664162\n",
      "Step 4500: Loss = 0.657863\n",
      "Step 4600: Loss = 0.652392\n",
      "Step 4700: Loss = 0.647487\n",
      "Step 4800: Loss = 0.643005\n",
      "Step 4900: Loss = 0.638865\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 5/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 210.024353\n",
      "Step 100: Loss = 65.368248\n",
      "Step 200: Loss = 134.324615\n",
      "Step 300: Loss = 17.838995\n",
      "Step 400: Loss = 1742.022583\n",
      "Step 500: Loss = 100.658707\n",
      "Step 600: Loss = 179.136688\n",
      "Step 700: Loss = 15.286510\n",
      "Step 800: Loss = 351.762115\n",
      "Step 900: Loss = 10.322091\n",
      "Step 1000: Loss = 270.313873\n",
      "Step 1100: Loss = 14.675810\n",
      "Step 1200: Loss = 153.252609\n",
      "Step 1300: Loss = 3.206480\n",
      "Step 1400: Loss = 159.642426\n",
      "Step 1500: Loss = 3.314050\n",
      "Step 1600: Loss = 322.330444\n",
      "Step 1700: Loss = 10.762120\n",
      "Step 1800: Loss = 326.283569\n",
      "Step 1900: Loss = 6.475987\n",
      "Step 2000: Loss = 293.469818\n",
      "Step 2100: Loss = 5.298139\n",
      "Step 2200: Loss = 285.994629\n",
      "Step 2300: Loss = 5.747486\n",
      "Step 2400: Loss = 197.422211\n",
      "Step 2500: Loss = 14.347181\n",
      "Step 2600: Loss = 131.533875\n",
      "Step 2700: Loss = 10.041568\n",
      "Step 2800: Loss = 120.528275\n",
      "Step 2900: Loss = 5.382455\n",
      "Step 3000: Loss = 81.091904\n",
      "Step 3100: Loss = 6.210924\n",
      "Step 3200: Loss = 119.680916\n",
      "Step 3300: Loss = 8.852070\n",
      "Step 3400: Loss = 213.656464\n",
      "Step 3500: Loss = 2.287488\n",
      "Step 3600: Loss = 136.162994\n",
      "Step 3700: Loss = 6.351335\n",
      "Step 3800: Loss = 114.093224\n",
      "Step 3900: Loss = 1.855339\n",
      "Step 4000: Loss = 1.613866\n",
      "Step 4100: Loss = 1.485230\n",
      "Step 4200: Loss = 1.395680\n",
      "Step 4300: Loss = 1.332021\n",
      "Step 4400: Loss = 1.285645\n",
      "Step 4500: Loss = 1.250561\n",
      "Step 4600: Loss = 1.222854\n",
      "Step 4700: Loss = 1.200068\n",
      "Step 4800: Loss = 1.180694\n",
      "Step 4900: Loss = 1.163798\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "  Resampling ratio η = 0.6\n",
      "    Repeat 1/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 244.202072\n",
      "Step 100: Loss = 98.316345\n",
      "Step 200: Loss = 1347.880981\n",
      "Step 300: Loss = 113.160835\n",
      "Step 400: Loss = 225.564957\n",
      "Step 500: Loss = 61.357807\n",
      "Step 600: Loss = 457.519928\n",
      "Step 700: Loss = 100.476715\n",
      "Step 800: Loss = 231.710663\n",
      "Step 900: Loss = 24.465151\n",
      "Step 1000: Loss = 203.461990\n",
      "Step 1100: Loss = 13.557216\n",
      "Step 1200: Loss = 310.410309\n",
      "Step 1300: Loss = 21.070042\n",
      "Step 1400: Loss = 216.084381\n",
      "Step 1500: Loss = 17.944832\n",
      "Step 1600: Loss = 604.360168\n",
      "Step 1700: Loss = 29.012321\n",
      "Step 1800: Loss = 266.428009\n",
      "Step 1900: Loss = 11.507737\n",
      "Step 2000: Loss = 547.222595\n",
      "Step 2100: Loss = 3.436353\n",
      "Step 2200: Loss = 270.558655\n",
      "Step 2300: Loss = 16.015282\n",
      "Step 2400: Loss = 283.596130\n",
      "Step 2500: Loss = 5.290893\n",
      "Step 2600: Loss = 252.069870\n",
      "Step 2700: Loss = 6.778892\n",
      "Step 2800: Loss = 181.154526\n",
      "Step 2900: Loss = 2.963183\n",
      "Step 3000: Loss = 238.206512\n",
      "Step 3100: Loss = 2.108704\n",
      "Step 3200: Loss = 164.646255\n",
      "Step 3300: Loss = 3.719900\n",
      "Step 3400: Loss = 213.911636\n",
      "Step 3500: Loss = 6.951290\n",
      "Step 3600: Loss = 251.169891\n",
      "Step 3700: Loss = 5.261802\n",
      "Step 3800: Loss = 305.850586\n",
      "Step 3900: Loss = 2.748322\n",
      "Step 4000: Loss = 1.445246\n",
      "Step 4100: Loss = 1.152575\n",
      "Step 4200: Loss = 1.020442\n",
      "Step 4300: Loss = 0.941589\n",
      "Step 4400: Loss = 0.887513\n",
      "Step 4500: Loss = 0.847427\n",
      "Step 4600: Loss = 0.816300\n",
      "Step 4700: Loss = 0.791371\n",
      "Step 4800: Loss = 0.770940\n",
      "Step 4900: Loss = 0.753875\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 2/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 150.819855\n",
      "Step 100: Loss = 55.600136\n",
      "Step 200: Loss = 420.045959\n",
      "Step 300: Loss = 73.753159\n",
      "Step 400: Loss = 215.614059\n",
      "Step 500: Loss = 29.238953\n",
      "Step 600: Loss = 321.065125\n",
      "Step 700: Loss = 21.870493\n",
      "Step 800: Loss = 286.674011\n",
      "Step 900: Loss = 38.039391\n",
      "Step 1000: Loss = 612.687439\n",
      "Step 1100: Loss = 135.110580\n",
      "Step 1200: Loss = 341.713745\n",
      "Step 1300: Loss = 91.675995\n",
      "Step 1400: Loss = 196.219543\n",
      "Step 1500: Loss = 38.160786\n",
      "Step 1600: Loss = 283.332489\n",
      "Step 1700: Loss = 16.684467\n",
      "Step 1800: Loss = 353.435333\n",
      "Step 1900: Loss = 31.019119\n",
      "Step 2000: Loss = 273.922485\n",
      "Step 2100: Loss = 8.166553\n",
      "Step 2200: Loss = 490.574249\n",
      "Step 2300: Loss = 35.506790\n",
      "Step 2400: Loss = 485.953735\n",
      "Step 2500: Loss = 48.483479\n",
      "Step 2600: Loss = 220.788010\n",
      "Step 2700: Loss = 14.248233\n",
      "Step 2800: Loss = 283.702881\n",
      "Step 2900: Loss = 11.996106\n",
      "Step 3000: Loss = 354.919098\n",
      "Step 3100: Loss = 11.236484\n",
      "Step 3200: Loss = 316.710388\n",
      "Step 3300: Loss = 3.321979\n",
      "Step 3400: Loss = 235.331573\n",
      "Step 3500: Loss = 5.395859\n",
      "Step 3600: Loss = 278.874298\n",
      "Step 3700: Loss = 8.740477\n",
      "Step 3800: Loss = 208.280838\n",
      "Step 3900: Loss = 7.385712\n",
      "Step 4000: Loss = 2.932199\n",
      "Step 4100: Loss = 1.541018\n",
      "Step 4200: Loss = 1.086503\n",
      "Step 4300: Loss = 0.925686\n",
      "Step 4400: Loss = 0.854701\n",
      "Step 4500: Loss = 0.814549\n",
      "Step 4600: Loss = 0.787086\n",
      "Step 4700: Loss = 0.765807\n",
      "Step 4800: Loss = 0.748073\n",
      "Step 4900: Loss = 0.732693\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 3/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 163.571228\n",
      "Step 100: Loss = 39.595047\n",
      "Step 200: Loss = 867.469482\n",
      "Step 300: Loss = 80.425354\n",
      "Step 400: Loss = 1025.067017\n",
      "Step 500: Loss = 158.484161\n",
      "Step 600: Loss = 215.629898\n",
      "Step 700: Loss = 61.182732\n",
      "Step 800: Loss = 181.187622\n",
      "Step 900: Loss = 45.119106\n",
      "Step 1000: Loss = 204.802704\n",
      "Step 1100: Loss = 32.249657\n",
      "Step 1200: Loss = 856.935242\n",
      "Step 1300: Loss = 42.701218\n",
      "Step 1400: Loss = 786.818420\n",
      "Step 1500: Loss = 13.091738\n",
      "Step 1600: Loss = 334.556732\n",
      "Step 1700: Loss = 5.769755\n",
      "Step 1800: Loss = 246.205841\n",
      "Step 1900: Loss = 4.977655\n",
      "Step 2000: Loss = 476.668884\n",
      "Step 2100: Loss = 11.704476\n",
      "Step 2200: Loss = 248.444153\n",
      "Step 2300: Loss = 1.337437\n",
      "Step 2400: Loss = 456.239624\n",
      "Step 2500: Loss = 4.652835\n",
      "Step 2600: Loss = 228.268921\n",
      "Step 2700: Loss = 9.237219\n",
      "Step 2800: Loss = 263.596252\n",
      "Step 2900: Loss = 4.712541\n",
      "Step 3000: Loss = 378.526459\n",
      "Step 3100: Loss = 8.335951\n",
      "Step 3200: Loss = 209.029266\n",
      "Step 3300: Loss = 23.724483\n",
      "Step 3400: Loss = 270.593384\n",
      "Step 3500: Loss = 20.794441\n",
      "Step 3600: Loss = 303.836334\n",
      "Step 3700: Loss = 4.519760\n",
      "Step 3800: Loss = 227.192078\n",
      "Step 3900: Loss = 4.776033\n",
      "Step 4000: Loss = 1.162336\n",
      "Step 4100: Loss = 0.879004\n",
      "Step 4200: Loss = 0.835548\n",
      "Step 4300: Loss = 0.806611\n",
      "Step 4400: Loss = 0.781921\n",
      "Step 4500: Loss = 0.760238\n",
      "Step 4600: Loss = 0.741046\n",
      "Step 4700: Loss = 0.723971\n",
      "Step 4800: Loss = 0.708709\n",
      "Step 4900: Loss = 0.695002\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 4/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 235.558670\n",
      "Step 100: Loss = 57.306553\n",
      "Step 200: Loss = 511.013977\n",
      "Step 300: Loss = 41.809593\n",
      "Step 400: Loss = 223.276550\n",
      "Step 500: Loss = 18.837330\n",
      "Step 600: Loss = 276.237701\n",
      "Step 700: Loss = 16.316124\n",
      "Step 800: Loss = 126.209732\n",
      "Step 900: Loss = 8.172483\n",
      "Step 1000: Loss = 372.820892\n",
      "Step 1100: Loss = 20.543026\n",
      "Step 1200: Loss = 404.851349\n",
      "Step 1300: Loss = 15.818260\n",
      "Step 1400: Loss = 462.262634\n",
      "Step 1500: Loss = 8.720535\n",
      "Step 1600: Loss = 514.421326\n",
      "Step 1700: Loss = 50.212555\n",
      "Step 1800: Loss = 347.949768\n",
      "Step 1900: Loss = 35.198402\n",
      "Step 2000: Loss = 180.622635\n",
      "Step 2100: Loss = 12.591032\n",
      "Step 2200: Loss = 209.608414\n",
      "Step 2300: Loss = 13.074627\n",
      "Step 2400: Loss = 208.194046\n",
      "Step 2500: Loss = 22.752335\n",
      "Step 2600: Loss = 188.136993\n",
      "Step 2700: Loss = 14.320000\n",
      "Step 2800: Loss = 279.114441\n",
      "Step 2900: Loss = 11.221779\n",
      "Step 3000: Loss = 298.336914\n",
      "Step 3100: Loss = 13.113625\n",
      "Step 3200: Loss = 222.859238\n",
      "Step 3300: Loss = 128.639069\n",
      "Step 3400: Loss = 311.606354\n",
      "Step 3500: Loss = 55.187511\n",
      "Step 3600: Loss = 184.169662\n",
      "Step 3700: Loss = 22.564009\n",
      "Step 3800: Loss = 288.845978\n",
      "Step 3900: Loss = 32.140297\n",
      "Step 4000: Loss = 19.581184\n",
      "Step 4100: Loss = 15.304449\n",
      "Step 4200: Loss = 13.289742\n",
      "Step 4300: Loss = 11.331964\n",
      "Step 4400: Loss = 9.211776\n",
      "Step 4500: Loss = 7.476025\n",
      "Step 4600: Loss = 6.713708\n",
      "Step 4700: Loss = 6.377277\n",
      "Step 4800: Loss = 6.113930\n",
      "Step 4900: Loss = 5.863356\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 5/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 237.427063\n",
      "Step 100: Loss = 69.215797\n",
      "Step 200: Loss = 349.761322\n",
      "Step 300: Loss = 24.448830\n",
      "Step 400: Loss = 257.892853\n",
      "Step 500: Loss = 15.409822\n",
      "Step 600: Loss = 469.067352\n",
      "Step 700: Loss = 54.422665\n",
      "Step 800: Loss = 417.910156\n",
      "Step 900: Loss = 24.407841\n",
      "Step 1000: Loss = 1270.502930\n",
      "Step 1100: Loss = 29.326429\n",
      "Step 1200: Loss = 327.919220\n",
      "Step 1300: Loss = 10.980554\n",
      "Step 1400: Loss = 453.559723\n",
      "Step 1500: Loss = 5.357099\n",
      "Step 1600: Loss = 577.083496\n",
      "Step 1700: Loss = 4.301023\n",
      "Step 1800: Loss = 140.128616\n",
      "Step 1900: Loss = 3.605718\n",
      "Step 2000: Loss = 276.933868\n",
      "Step 2100: Loss = 3.516384\n",
      "Step 2200: Loss = 411.486816\n",
      "Step 2300: Loss = 2.911831\n",
      "Step 2400: Loss = 252.433075\n",
      "Step 2500: Loss = 3.612343\n",
      "Step 2600: Loss = 298.949219\n",
      "Step 2700: Loss = 14.065279\n",
      "Step 2800: Loss = 286.662109\n",
      "Step 2900: Loss = 4.245635\n",
      "Step 3000: Loss = 619.142334\n",
      "Step 3100: Loss = 14.714651\n",
      "Step 3200: Loss = 359.555084\n",
      "Step 3300: Loss = 22.567329\n",
      "Step 3400: Loss = 250.990509\n",
      "Step 3500: Loss = 11.283593\n",
      "Step 3600: Loss = 281.114960\n",
      "Step 3700: Loss = 7.024359\n",
      "Step 3800: Loss = 214.354568\n",
      "Step 3900: Loss = 14.313488\n",
      "Step 4000: Loss = 7.334811\n",
      "Step 4100: Loss = 4.641610\n",
      "Step 4200: Loss = 2.808589\n",
      "Step 4300: Loss = 1.247641\n",
      "Step 4400: Loss = 0.802910\n",
      "Step 4500: Loss = 0.744294\n",
      "Step 4600: Loss = 0.724923\n",
      "Step 4700: Loss = 0.711901\n",
      "Step 4800: Loss = 0.701460\n",
      "Step 4900: Loss = 0.692597\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "  Resampling ratio η = 0.9\n",
      "    Repeat 1/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 244.902252\n",
      "Step 100: Loss = 37.094456\n",
      "Step 200: Loss = 973.961975\n",
      "Step 300: Loss = 57.341175\n",
      "Step 400: Loss = 1512.481079\n",
      "Step 500: Loss = 34.556591\n",
      "Step 600: Loss = 263.117065\n",
      "Step 700: Loss = 62.463676\n",
      "Step 800: Loss = 464.726837\n",
      "Step 900: Loss = 45.845261\n",
      "Step 1000: Loss = 497.840576\n",
      "Step 1100: Loss = 35.730389\n",
      "Step 1200: Loss = 621.512878\n",
      "Step 1300: Loss = 51.657433\n",
      "Step 1400: Loss = 600.791443\n",
      "Step 1500: Loss = 46.414307\n",
      "Step 1600: Loss = 383.726227\n",
      "Step 1700: Loss = 43.728485\n",
      "Step 1800: Loss = 658.197083\n",
      "Step 1900: Loss = 32.907112\n",
      "Step 2000: Loss = 893.640686\n",
      "Step 2100: Loss = 36.942066\n",
      "Step 2200: Loss = 485.382416\n",
      "Step 2300: Loss = 46.254108\n",
      "Step 2400: Loss = 403.881653\n",
      "Step 2500: Loss = 36.350185\n",
      "Step 2600: Loss = 371.620300\n",
      "Step 2700: Loss = 26.213818\n",
      "Step 2800: Loss = 574.069214\n",
      "Step 2900: Loss = 20.117037\n",
      "Step 3000: Loss = 336.283966\n",
      "Step 3100: Loss = 44.372684\n",
      "Step 3200: Loss = 916.303101\n",
      "Step 3300: Loss = 88.345581\n",
      "Step 3400: Loss = 336.306335\n",
      "Step 3500: Loss = 54.907700\n",
      "Step 3600: Loss = 365.473724\n",
      "Step 3700: Loss = 51.184471\n",
      "Step 3800: Loss = 425.039551\n",
      "Step 3900: Loss = 41.504448\n",
      "Step 4000: Loss = 27.311287\n",
      "Step 4100: Loss = 19.165699\n",
      "Step 4200: Loss = 14.589279\n",
      "Step 4300: Loss = 12.400637\n",
      "Step 4400: Loss = 11.188884\n",
      "Step 4500: Loss = 10.455458\n",
      "Step 4600: Loss = 9.980124\n",
      "Step 4700: Loss = 9.636204\n",
      "Step 4800: Loss = 9.362534\n",
      "Step 4900: Loss = 9.130107\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 2/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 205.339233\n",
      "Step 100: Loss = 61.398827\n",
      "Step 200: Loss = 469.249268\n",
      "Step 300: Loss = 52.437565\n",
      "Step 400: Loss = 464.033569\n",
      "Step 500: Loss = 64.646461\n",
      "Step 600: Loss = 786.980835\n",
      "Step 700: Loss = 63.013412\n",
      "Step 800: Loss = 306.684875\n",
      "Step 900: Loss = 68.515671\n",
      "Step 1000: Loss = 610.119080\n",
      "Step 1100: Loss = 165.845169\n",
      "Step 1200: Loss = 269.925140\n",
      "Step 1300: Loss = 118.293640\n",
      "Step 1400: Loss = 479.139465\n",
      "Step 1500: Loss = 63.813934\n",
      "Step 1600: Loss = 276.664703\n",
      "Step 1700: Loss = 113.807419\n",
      "Step 1800: Loss = 502.228607\n",
      "Step 1900: Loss = 123.206085\n",
      "Step 2000: Loss = 489.788696\n",
      "Step 2100: Loss = 67.989700\n",
      "Step 2200: Loss = 710.795288\n",
      "Step 2300: Loss = 81.638458\n",
      "Step 2400: Loss = 749.566895\n",
      "Step 2500: Loss = 88.506165\n",
      "Step 2600: Loss = 417.940979\n",
      "Step 2700: Loss = 53.939857\n",
      "Step 2800: Loss = 374.689453\n",
      "Step 2900: Loss = 36.689541\n",
      "Step 3000: Loss = 584.479370\n",
      "Step 3100: Loss = 85.451096\n",
      "Step 3200: Loss = 594.607666\n",
      "Step 3300: Loss = 106.871735\n",
      "Step 3400: Loss = 303.768097\n",
      "Step 3500: Loss = 76.490685\n",
      "Step 3600: Loss = 464.633453\n",
      "Step 3700: Loss = 103.233978\n",
      "Step 3800: Loss = 482.923431\n",
      "Step 3900: Loss = 131.335403\n",
      "Step 4000: Loss = 104.454086\n",
      "Step 4100: Loss = 89.027031\n",
      "Step 4200: Loss = 80.793922\n",
      "Step 4300: Loss = 72.079018\n",
      "Step 4400: Loss = 67.124817\n",
      "Step 4500: Loss = 63.624634\n",
      "Step 4600: Loss = 59.883881\n",
      "Step 4700: Loss = 55.045441\n",
      "Step 4800: Loss = 49.804211\n",
      "Step 4900: Loss = 44.316330\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 3/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 184.668533\n",
      "Step 100: Loss = 40.189644\n",
      "Step 200: Loss = 3369.018555\n",
      "Step 300: Loss = 139.963699\n",
      "Step 400: Loss = 393.033783\n",
      "Step 500: Loss = 75.696411\n",
      "Step 600: Loss = 516.271606\n",
      "Step 700: Loss = 73.700577\n",
      "Step 800: Loss = 885.430420\n",
      "Step 900: Loss = 157.102493\n",
      "Step 1000: Loss = 246.790222\n",
      "Step 1100: Loss = 59.731815\n",
      "Step 1200: Loss = 739.312866\n",
      "Step 1300: Loss = 114.076050\n",
      "Step 1400: Loss = 882.089111\n",
      "Step 1500: Loss = 84.693405\n",
      "Step 1600: Loss = 761.195618\n",
      "Step 1700: Loss = 60.403584\n",
      "Step 1800: Loss = 334.435242\n",
      "Step 1900: Loss = 35.677860\n",
      "Step 2000: Loss = 319.841949\n",
      "Step 2100: Loss = 21.732080\n",
      "Step 2200: Loss = 600.659851\n",
      "Step 2300: Loss = 17.412928\n",
      "Step 2400: Loss = 887.128357\n",
      "Step 2500: Loss = 39.745934\n",
      "Step 2600: Loss = 385.733948\n",
      "Step 2700: Loss = 43.349190\n",
      "Step 2800: Loss = 593.987671\n",
      "Step 2900: Loss = 49.236485\n",
      "Step 3000: Loss = 613.722595\n",
      "Step 3100: Loss = 46.350090\n",
      "Step 3200: Loss = 440.621307\n",
      "Step 3300: Loss = 10.065339\n",
      "Step 3400: Loss = 540.871216\n",
      "Step 3500: Loss = 7.869847\n",
      "Step 3600: Loss = 395.340576\n",
      "Step 3700: Loss = 19.842520\n",
      "Step 3800: Loss = 361.196838\n",
      "Step 3900: Loss = 22.831001\n",
      "Step 4000: Loss = 6.730134\n",
      "Step 4100: Loss = 4.134151\n",
      "Step 4200: Loss = 2.666143\n",
      "Step 4300: Loss = 1.591975\n",
      "Step 4400: Loss = 1.081187\n",
      "Step 4500: Loss = 0.927696\n",
      "Step 4600: Loss = 0.863505\n",
      "Step 4700: Loss = 0.822371\n",
      "Step 4800: Loss = 0.791838\n",
      "Step 4900: Loss = 0.767634\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 4/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 248.556107\n",
      "Step 100: Loss = 134.386673\n",
      "Step 200: Loss = 317.375092\n",
      "Step 300: Loss = 21.842915\n",
      "Step 400: Loss = 203.328629\n",
      "Step 500: Loss = 6.801619\n",
      "Step 600: Loss = 375.261078\n",
      "Step 700: Loss = 93.289757\n",
      "Step 800: Loss = 614.406128\n",
      "Step 900: Loss = 39.930416\n",
      "Step 1000: Loss = 178.052734\n",
      "Step 1100: Loss = 13.288796\n",
      "Step 1200: Loss = 92.056114\n",
      "Step 1300: Loss = 1.390367\n",
      "Step 1400: Loss = 37.960293\n",
      "Step 1500: Loss = 2.445341\n",
      "Step 1600: Loss = 50.927361\n",
      "Step 1700: Loss = 1.462263\n",
      "Step 1800: Loss = 33.753056\n",
      "Step 1900: Loss = 1.202896\n",
      "Step 2000: Loss = 31.323122\n",
      "Step 2100: Loss = 0.942248\n",
      "Step 2200: Loss = 50.973194\n",
      "Step 2300: Loss = 0.971242\n",
      "Step 2400: Loss = 71.934616\n",
      "Step 2500: Loss = 1.030891\n",
      "Step 2600: Loss = 9.211376\n",
      "Step 2700: Loss = 0.625621\n",
      "Step 2800: Loss = 19.300175\n",
      "Step 2900: Loss = 0.933734\n",
      "Step 3000: Loss = 40.464455\n",
      "Step 3100: Loss = 1.077753\n",
      "Step 3200: Loss = 46.958385\n",
      "Step 3300: Loss = 0.690275\n",
      "Step 3400: Loss = 24.432325\n",
      "Step 3500: Loss = 0.546278\n",
      "Step 3600: Loss = 12.766363\n",
      "Step 3700: Loss = 1.785826\n",
      "Step 3800: Loss = 8.688930\n",
      "Step 3900: Loss = 1.134527\n",
      "Step 4000: Loss = 0.648729\n",
      "Step 4100: Loss = 0.412748\n",
      "Step 4200: Loss = 0.291237\n",
      "Step 4300: Loss = 0.225695\n",
      "Step 4400: Loss = 0.186994\n",
      "Step 4500: Loss = 0.162080\n",
      "Step 4600: Loss = 0.144818\n",
      "Step 4700: Loss = 0.132111\n",
      "Step 4800: Loss = 0.122286\n",
      "Step 4900: Loss = 0.114382\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n",
      "    Repeat 5/5\n",
      "Starting Adam optimization phase...\n",
      "Step 0: Loss = 183.042877\n",
      "Step 100: Loss = 77.632637\n",
      "Step 200: Loss = 1037.230713\n",
      "Step 300: Loss = 169.650604\n",
      "Step 400: Loss = 360.475952\n",
      "Step 500: Loss = 47.644890\n",
      "Step 600: Loss = 244.302582\n",
      "Step 700: Loss = 15.156296\n",
      "Step 800: Loss = 321.619080\n",
      "Step 900: Loss = 12.293967\n",
      "Step 1000: Loss = 922.420776\n",
      "Step 1100: Loss = 29.318367\n",
      "Step 1200: Loss = 266.664673\n",
      "Step 1300: Loss = 10.266972\n",
      "Step 1400: Loss = 324.657166\n",
      "Step 1500: Loss = 22.341854\n",
      "Step 1600: Loss = 357.868225\n",
      "Step 1700: Loss = 16.271748\n",
      "Step 1800: Loss = 533.913879\n",
      "Step 1900: Loss = 18.922825\n",
      "Step 2000: Loss = 792.848755\n",
      "Step 2100: Loss = 50.003868\n",
      "Step 2200: Loss = 627.639343\n",
      "Step 2300: Loss = 9.343599\n",
      "Step 2400: Loss = 387.922363\n",
      "Step 2500: Loss = 19.282026\n",
      "Step 2600: Loss = 392.888062\n",
      "Step 2700: Loss = 39.893620\n",
      "Step 2800: Loss = 541.259094\n",
      "Step 2900: Loss = 10.673110\n",
      "Step 3000: Loss = 407.440247\n",
      "Step 3100: Loss = 15.950397\n",
      "Step 3200: Loss = 502.417145\n",
      "Step 3300: Loss = 38.398003\n",
      "Step 3400: Loss = 552.701965\n",
      "Step 3500: Loss = 17.428549\n",
      "Step 3600: Loss = 809.532532\n",
      "Step 3700: Loss = 44.036934\n",
      "Step 3800: Loss = 627.111084\n",
      "Step 3900: Loss = 27.062410\n",
      "Step 4000: Loss = 7.593464\n",
      "Step 4100: Loss = 3.869878\n",
      "Step 4200: Loss = 2.278519\n",
      "Step 4300: Loss = 1.559535\n",
      "Step 4400: Loss = 1.254953\n",
      "Step 4500: Loss = 1.108707\n",
      "Step 4600: Loss = 1.016378\n",
      "Step 4700: Loss = 0.945489\n",
      "Step 4800: Loss = 0.886536\n",
      "Step 4900: Loss = 0.836298\n",
      "Starting L-BFGS optimization phase...\n",
      "Training completed!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_all_experiments()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_win39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
